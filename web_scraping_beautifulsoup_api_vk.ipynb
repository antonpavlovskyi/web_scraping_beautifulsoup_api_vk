{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "homeless-madison",
   "metadata": {},
   "source": [
    "# Web scraping using BeautifulSoup and vkontakte.ru API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "silver-journey",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-optics",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Let's parse a page from a popular it website Habr. URL: habr.com/ru/all/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pointed-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keywords we will be looking for in parsed data\n",
    "KEYWORDS = ['python', 'парсинг']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "international-court",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Response [200]>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting the page itself\n",
    "habr = requests.get('https://habr.com/ru/all/')\n",
    "habr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "written-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parsing the page using BeautifulSoup library\n",
    "soup = BeautifulSoup(habr.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "super-guess",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<div class=\"tm-article-snippet\"><div class=\"tm-article-snippet__meta-container\"><div class=\"tm-article-snippet__meta\"><span class=\"tm-user-info tm-article-snippet__author\"><a class=\"tm-user-info__userpic\" href=\"/ru/users/KaosEngineer/\" title=\"KaosEngineer\"><div class=\"tm-entity-image\"><img alt=\"\" class=\"tm-entity-image__pic\" height=\"24\" loading=\"lazy\" src=\"//habrastorage.org/r/w32/getpro/habr/avatars/959/913/2f9/9599132f99c786ebb87b9026d10ae93c.jpg\" width=\"24\"/></div></a><span class=\"tm-user-info__user\"><a class=\"tm-user-info__username\" href=\"/ru/users/KaosEngineer/\">\n       KaosEngineer\n     </a></span></span><span class=\"tm-article-snippet__datetime-published\"><time datetime=\"2021-07-22T11:03:04.000Z\" title=\"2021-07-22, 14:03\">сегодня в 14:03</time></span></div><!-- --></div><h2 class=\"tm-article-snippet__title tm-article-snippet__title_h2\"><a class=\"tm-article-snippet__title-link\" data-article-link=\"\" href=\"/ru/company/yandex/blog/568672/\"><span>Яндекс открывает датасеты Беспилотных автомобилей, Погоды и Переводчика, чтобы помочь решить проблему сдвига данных в ML</span></a></h2><div class=\"tm-article-snippet__hubs\"><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/company/yandex/blog/\"><span>Блог компании Яндекс</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/open_source/\"><span>Open source</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/machine_learning/\"><span>Машинное обучение</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/research/\"><span>Исследования и прогнозы в IT</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/artificial_intelligence/\"><span>Искусственный интеллект</span></a></span></div><div class=\"tm-article-snippet__labels\"><!-- --></div><!-- --><div class=\"tm-article-body tm-article-snippet__lead\">\n <!-- --><div class=\"article-formatted-body article-formatted-body_version-1\"><img src=\"https://habrastorage.org/webt/tw/yw/qk/twywqkfrfbhix8vhgacsxrqzpss.png\"/><br/>\n <br/>\n В рамках конкурса <a href=\"https://research.yandex.com/shifts\" rel=\"nofollow noopener noreferrer\">Shifts Challenge</a> мы выкладываем в открытый доступ крупнейший в мире датасет для обучения беспилотных автомобилей, а также данные Яндекс.Переводчика и Погоды. Приглашаем исследователей в области машинного обучения присоединиться к поиску решения проблемы сдвига распределения данных в реальном мире по отношению к тому, с чем моделям приходится иметь дело при обучении.<br/>\n <br/>\n Меня зовут Андрей Малинин, я старший исследователь в Yandex Research. Сегодня я расскажу о проблеме, о наших датасетах, а также о конкурсе, который мы проводим в рамках международной конференции NeurIPS 2021 совместно с учеными из Оксфордского и Кембриджского университетов.<br/>\n <br/></div><a class=\"tm-article-snippet__readmore\" href=\"/ru/company/yandex/blog/568672/\"><span>Читать дальше →</span></a></div></div>,\n <div class=\"tm-article-snippet\"><div class=\"tm-article-snippet__meta-container\"><div class=\"tm-article-snippet__meta\"><span class=\"tm-user-info tm-article-snippet__author\"><a class=\"tm-user-info__userpic\" href=\"/ru/users/andrey_bursian_r7/\" title=\"andrey_bursian_r7\"><div class=\"tm-entity-image\"><img alt=\"\" class=\"tm-entity-image__pic\" height=\"24\" loading=\"lazy\" src=\"//habrastorage.org/r/w32/getpro/habr/avatars/21c/aa7/299/21caa72991a1bffc1d4e842028cc2728.jpg\" width=\"24\"/></div></a><span class=\"tm-user-info__user\"><a class=\"tm-user-info__username\" href=\"/ru/users/andrey_bursian_r7/\">\n       andrey_bursian_r7\n     </a></span></span><span class=\"tm-article-snippet__datetime-published\"><time datetime=\"2021-07-22T11:01:53.000Z\" title=\"2021-07-22, 14:01\">сегодня в 14:01</time></span></div><!-- --></div><h2 class=\"tm-article-snippet__title tm-article-snippet__title_h2\"><a class=\"tm-article-snippet__title-link\" data-article-link=\"\" href=\"/ru/company/r7-office/blog/568942/\"><span>О небольших, но бесяще важных различиях текстовых редакторов</span></a></h2><div class=\"tm-article-snippet__hubs\"><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/company/r7-office/blog/\"><span>Блог компании Р7-Офис</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/typography/\"><span>Типографика</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/pm/\"><span>Управление проектами</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/soft/\"><span>Софт</span></a></span></div><div class=\"tm-article-snippet__labels\"><!-- --></div><!-- --><div class=\"tm-article-body tm-article-snippet__lead\">\n <div class=\"tm-article-snippet__cover tm-article-snippet__cover_cover\"><img class=\"tm-article-snippet__lead-image\" src=\"https://habrastorage.org/getpro/habr/upload_files/315/d25/804/315d258044baa8f04eb7d3a92be4c874.jpg\" style=\"object-position:0% 0%;\"/></div><div class=\"article-formatted-body article-formatted-body_version-2\"><p>Привет! Я из команды «Р7-Офис», и я немного гик текстового редактирования. Например, в MS Word, равно как и в нашем редакторе Р7, есть двойное зачёркивание, а вот в других русских офисах такой фичи «из коробки» нет. На самом деле это очень русская фишка: у нас есть госкомпании, которые по своим стандартам должны использовать именно двойное зачёркивание в ряде ситуаций. И если эту функцию не поддерживать, то где-то далеко в Сибири заплачет ещё один инженер. </p><p>У нас нет разницы между ядрами онлайн-редактора и десктопной версией. У большинства офисных пакетов эволюция прослеживается с тех времён, когда никакого онлайна толком не было, и внутри монолита можно было построить Римскую империю. Поэтому при переходе к онлайну они обычно не трогали старую кодовую базу, а просто выписывали её основные свойства в бэклог и повторяли в новой версии. Как метод рефакторинга подход замечательный, если не считать того, что полностью скопировать исходный функционал не всегда получается (и не всегда экономически оправданно, видимо), и в итоге приходится поддерживать два разных продукта, которые нередко для пользователя выглядят как один. </p><p>Но я бы хотел рассказать немного о буднях того, что происходит в «исконно славянском труЪ офисе». Начнём с подхода к открытию и сохранению docx и плавно перейдём к тому, каких функций вам не хватает. </p></div><a class=\"tm-article-snippet__readmore\" href=\"/ru/company/r7-office/blog/568942/\"><span>Читать далее</span></a></div></div>,\n <div class=\"tm-article-snippet\"><div class=\"tm-article-snippet__meta-container\"><div class=\"tm-article-snippet__meta\"><span class=\"tm-user-info tm-article-snippet__author\"><a class=\"tm-user-info__userpic\" href=\"/ru/users/MaxRokatansky/\" title=\"MaxRokatansky\"><div class=\"tm-entity-image\"><img alt=\"\" class=\"tm-entity-image__pic\" height=\"24\" loading=\"lazy\" src=\"//habrastorage.org/r/w32/getpro/habr/avatars/b9f/baf/5f9/b9fbaf5f96ae52973706a0716bd9216e.jpg\" width=\"24\"/></div></a><span class=\"tm-user-info__user\"><a class=\"tm-user-info__username\" href=\"/ru/users/MaxRokatansky/\">\n       MaxRokatansky\n     </a></span></span><span class=\"tm-article-snippet__datetime-published\"><time datetime=\"2021-07-22T11:01:36.000Z\" title=\"2021-07-22, 14:01\">сегодня в 14:01</time></span></div><!-- --></div><h2 class=\"tm-article-snippet__title tm-article-snippet__title_h2\"><a class=\"tm-article-snippet__title-link\" data-article-link=\"\" href=\"/ru/company/otus/blog/569094/\"><span>Горячая перезагрузка .NET: новая возможность для редактирования кода во время выполнения приложений</span></a></h2><div class=\"tm-article-snippet__hubs\"><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/company/otus/blog/\"><span>Блог компании OTUS</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/net/\"><span>.NET</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/csharp/\"><span>C#</span></a></span></div><div class=\"tm-article-snippet__labels\"><div class=\"tm-article-snippet__label\"><span>\n           Перевод\n         </span></div></div><!-- --><div class=\"tm-article-body tm-article-snippet__lead\">\n <div class=\"tm-article-snippet__cover tm-article-snippet__cover_cover\"><img class=\"tm-article-snippet__lead-image\" src=\"https://habrastorage.org/getpro/habr/upload_files/13d/b1e/760/13db1e760259bb01b5f75b25df54b4a9.png\" style=\"object-position:0% 0%;\"/></div><div class=\"article-formatted-body article-formatted-body_version-2\"><p>Сегодня мы с радостью представляем возможность «горячей перезагрузки» для .NET, которая уже доступна в Visual Studio 2019 16.11 (предварительная версия 1), а также в .NET 6 (предварительная версия 4) через инструмент командной строки <em>dotnet watch</em>. В этой статье мы расскажем, что такое горячая перезагрузка .NET, как приступить к ее использованию, как мы планируем развивать эту возможность в будущем, а также какие виды правок кода и языки поддерживаются на данный момент.</p></div><a class=\"tm-article-snippet__readmore\" href=\"/ru/company/otus/blog/569094/\"><span>Читать далее</span></a></div></div>,\n <div class=\"tm-article-snippet\"><div class=\"tm-article-snippet__meta-container\"><div class=\"tm-article-snippet__meta\"><span class=\"tm-user-info tm-article-snippet__author\"><a class=\"tm-user-info__userpic\" href=\"/ru/users/tgeshka/\" title=\"tgeshka\"><div class=\"tm-entity-image\"><img alt=\"\" class=\"tm-entity-image__pic\" height=\"24\" loading=\"lazy\" src=\"//habrastorage.org/r/w32/getpro/habr/avatars/6b7/af8/6df/6b7af86df0fe81a540fa8a51d0d94831.jpg\" width=\"24\"/></div></a><span class=\"tm-user-info__user\"><a class=\"tm-user-info__username\" href=\"/ru/users/tgeshka/\">\n       tgeshka\n     </a></span></span><span class=\"tm-article-snippet__datetime-published\"><time datetime=\"2021-07-22T10:59:35.000Z\" title=\"2021-07-22, 13:59\">сегодня в 13:59</time></span></div><!-- --></div><h2 class=\"tm-article-snippet__title tm-article-snippet__title_h2\"><a class=\"tm-article-snippet__title-link\" data-article-link=\"\" href=\"/ru/company/selectel/blog/568724/\"><span>Radxa ROCK 3A: конкурент Raspberry Pi со слотом M.2 для NVMe SSD</span></a></h2><div class=\"tm-article-snippet__hubs\"><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/company/selectel/blog/\"><span>Блог компании Selectel</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/electronics/\"><span>Производство и разработка электроники</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/gadgets/\"><span>Гаджеты</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/hardware/\"><span>Компьютерное железо</span></a></span></div><div class=\"tm-article-snippet__labels\"><div class=\"tm-article-snippet__label\"><span>\n           Перевод\n         </span></div></div><!-- --><div class=\"tm-article-body tm-article-snippet__lead\">\n <!-- --><div class=\"article-formatted-body article-formatted-body_version-1\"><div style=\"text-align:center;\"><img src=\"https://habrastorage.org/webt/ju/mf/uo/jumfuoie095_c6-bre3-bzgakrc.jpeg\"/></div> <br/>\n На днях полку одноплатников прибыло: китайская компания выпустила модель Radxa ROCK 3A размером с кредитку. Основа одноплатника — четырехъядерный Rockchip RK3568 ARM Cortex-A55 c частотой работы ядра 2 ГГц. Графика здесь — Mali-G52. Кроме того, есть слот для накопителей M.2 для NVMe SSD.<br/>\n <br/>\n По словам разработчиков, девайс может на равных конкурировать с последними версиями Raspberry Pi. Подробности о нем — под катом.<br/></div><a class=\"tm-article-snippet__readmore\" href=\"/ru/company/selectel/blog/568724/\"><span>Читать дальше →</span></a></div></div>,\n <div class=\"tm-article-snippet\"><div class=\"tm-article-snippet__meta-container\"><div class=\"tm-article-snippet__meta\"><span class=\"tm-user-info tm-article-snippet__author\"><a class=\"tm-user-info__userpic\" href=\"/ru/users/ImangazalievM/\" title=\"ImangazalievM\"><div class=\"tm-entity-image\"><img alt=\"\" class=\"tm-entity-image__pic\" height=\"24\" loading=\"lazy\" src=\"//habrastorage.org/r/w32/getpro/habr/avatars/00f/735/f8e/00f735f8e8901eb06fa51ceff1292656.png\" width=\"24\"/></div></a><span class=\"tm-user-info__user\"><a class=\"tm-user-info__username\" href=\"/ru/users/ImangazalievM/\">\n       ImangazalievM\n     </a></span></span><span class=\"tm-article-snippet__datetime-published\"><time datetime=\"2021-07-22T10:59:31.000Z\" title=\"2021-07-22, 13:59\">сегодня в 13:59</time></span></div><!-- --></div><h2 class=\"tm-article-snippet__title tm-article-snippet__title_h2\"><a class=\"tm-article-snippet__title-link\" data-article-link=\"\" href=\"/ru/post/569092/\"><span>Памятка по жизненному циклу Android — часть I. Отдельные Activity</span></a></h2><div class=\"tm-article-snippet__hubs\"><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/android_dev/\"><span>Разработка под Android</span></a></span></div><div class=\"tm-article-snippet__labels\"><div class=\"tm-article-snippet__label\"><span>\n           Перевод\n         </span></div></div><!-- --><div class=\"tm-article-body tm-article-snippet__lead\">\n <!-- --><div class=\"article-formatted-body article-formatted-body_version-2\"><p>Android спроектирован так, чтобы использование приложения пользователем было максимально интуитивным. Чтобы обеспечить такое взаимодействие с пользователем, вы должны знать, как управлять жизненными циклами (ЖЦ) компонентов. Компонентом может быть Activity, Fragment, Service, класс Application и даже сам процесс приложения. Компонент имеет жизненный цикл, в течение которого он проходит через различные состояния. Всякий раз, когда происходит переход, система уведомляет вас об этом при помощи методов жизненного цикла. В этой статье мы разберем то, как устроен ЖЦ Activity на основе нескольких распространенных сценариев использования.</p></div><a class=\"tm-article-snippet__readmore\" href=\"/ru/post/569092/\"><span>Читать далее</span></a></div></div>,\n <div class=\"tm-article-snippet\"><div class=\"tm-article-snippet__meta-container\"><div class=\"tm-article-snippet__meta\"><span class=\"tm-user-info tm-article-snippet__author\"><a class=\"tm-user-info__userpic\" href=\"/ru/users/RPAconsultant/\" title=\"RPAconsultant\"><div class=\"tm-entity-image\"><img alt=\"\" class=\"tm-entity-image__pic\" height=\"24\" loading=\"lazy\" src=\"//habrastorage.org/r/w32/getpro/habr/avatars/f96/bff/1bb/f96bff1bbc9c14d4f36be4b28d3f08d3.png\" width=\"24\"/></div></a><span class=\"tm-user-info__user\"><a class=\"tm-user-info__username\" href=\"/ru/users/RPAconsultant/\">\n       RPAconsultant\n     </a></span></span><span class=\"tm-article-snippet__datetime-published\"><time datetime=\"2021-07-22T10:53:18.000Z\" title=\"2021-07-22, 13:53\">сегодня в 13:53</time></span></div><!-- --></div><h2 class=\"tm-article-snippet__title tm-article-snippet__title_h2\"><a class=\"tm-article-snippet__title-link\" data-article-link=\"\" href=\"/ru/company/uipath/blog/569090/\"><span>Гайд: как создавать собственные активности для RPA-платформ</span></a></h2><div class=\"tm-article-snippet__hubs\"><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/company/uipath/blog/\"><span>Блог компании UiPath</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/it-infrastructure/\"><span>IT-инфраструктура</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/research/\"><span>Исследования и прогнозы в IT</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/itcompanies/\"><span>IT-компании</span></a></span></div><div class=\"tm-article-snippet__labels\"><!-- --></div><!-- --><div class=\"tm-article-body tm-article-snippet__lead\">\n <div class=\"tm-article-snippet__cover tm-article-snippet__cover_cover\"><img class=\"tm-article-snippet__lead-image\" src=\"https://habrastorage.org/getpro/habr/upload_files/0d7/28c/4ad/0d728c4ad630eaed9305f04276b8b4ed.png\" style=\"object-position:0% 0%;\"/></div><div class=\"article-formatted-body article-formatted-body_version-2\"><p>Платформы для роботизированной автоматизации имеют широкий спектр возможностей и позволяют использовать множество готовых действий без программирования. Однако, часто у бизнес-пользователей возникает потребность в создании новых активностей без привлечения разработчиков на постоянной основе. В этом посте рассказываем, как программисты могут помогать citizen developers решать эту задачу. </p><p><strong><em>Статья написана при поддержке технического эксперта UiPath: Валентина Драздова.</em></strong></p></div><a class=\"tm-article-snippet__readmore\" href=\"/ru/company/uipath/blog/569090/\"><span>Читать далее</span></a></div></div>,\n <div class=\"tm-article-snippet\"><div class=\"tm-article-snippet__meta-container\"><div class=\"tm-article-snippet__meta\"><span class=\"tm-user-info tm-article-snippet__author\"><a class=\"tm-user-info__userpic\" href=\"/ru/users/redhatrussia/\" title=\"redhatrussia\"><div class=\"tm-entity-image\"><img alt=\"\" class=\"tm-entity-image__pic\" height=\"24\" loading=\"lazy\" src=\"//habrastorage.org/r/w32/getpro/habr/avatars/592/816/4a9/5928164a9c2b4d0e7693ebb16e0f792a.png\" width=\"24\"/></div></a><span class=\"tm-user-info__user\"><a class=\"tm-user-info__username\" href=\"/ru/users/redhatrussia/\">\n       redhatrussia\n     </a></span></span><span class=\"tm-article-snippet__datetime-published\"><time datetime=\"2021-07-22T10:50:27.000Z\" title=\"2021-07-22, 13:50\">сегодня в 13:50</time></span></div><!-- --></div><h2 class=\"tm-article-snippet__title tm-article-snippet__title_h2\"><a class=\"tm-article-snippet__title-link\" data-article-link=\"\" href=\"/ru/company/redhatrussia/blog/569084/\"><span>Преобразуем CentOS в RHEL с помощью Convert2RHEL и Red Hat Satellite</span></a></h2><div class=\"tm-article-snippet__hubs\"><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/company/redhatrussia/blog/\"><span>Блог компании Red Hat</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/linux/\"><span>Настройка Linux</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/open_source/\"><span>Open source</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/it-infrastructure/\"><span>IT-инфраструктура</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/s_admin/\"><span>Серверное администрирование</span></a></span></div><div class=\"tm-article-snippet__labels\"><!-- --></div><!-- --><div class=\"tm-article-body tm-article-snippet__lead\">\n <div class=\"tm-article-snippet__cover tm-article-snippet__cover_cover\"><img class=\"tm-article-snippet__lead-image\" src=\"https://habrastorage.org/getpro/habr/upload_files/154/23f/c4f/15423fc4f25d70cc8fbd202e2fc20af9.png\" style=\"object-position:0% 0%;\"/></div><div class=\"article-formatted-body article-formatted-body_version-2\"><p>Сегодня мы расскажем, как провести миграцию с помощью утилиты Convert2RHEL, с апреля 2021 ставшей <a href=\"https://www.redhat.com/en/blog/introduction-convert2rhel-now-officially-supported-convert-rhel-systems-rhel\">официально поддерживаемым</a> компонентом Red Hat Enterprise Linux (RHEL), и средства системного управления <a href=\"https://www.redhat.com/en/technologies/management/satellite\">Red Hat Satellite</a>, которое входит в add-on к Red Hat Enterprise Linux <a href=\"https://www.redhat.com/en/technologies/management/smart-management\">Red Hat Smart Management</a>.</p></div><a class=\"tm-article-snippet__readmore\" href=\"/ru/company/redhatrussia/blog/569084/\"><span>Читать дальше: Преобразуем CentOS в RHEL..</span></a></div></div>,\n <div class=\"tm-article-snippet\"><div class=\"tm-article-snippet__meta-container\"><div class=\"tm-article-snippet__meta\"><span class=\"tm-user-info tm-article-snippet__author\"><a class=\"tm-user-info__userpic\" href=\"/ru/users/nkarpov/\" title=\"nkarpov\"><div class=\"tm-entity-image\"><img alt=\"\" class=\"tm-entity-image__pic\" height=\"24\" loading=\"lazy\" src=\"//habrastorage.org/r/w32/getpro/habr/avatars/777/0d1/e9a/7770d1e9a06340c2e8d6f1bb3ab2a18a.png\" width=\"24\"/></div></a><span class=\"tm-user-info__user\"><a class=\"tm-user-info__username\" href=\"/ru/users/nkarpov/\">\n       nkarpov\n     </a></span></span><span class=\"tm-article-snippet__datetime-published\"><time datetime=\"2021-07-22T10:37:39.000Z\" title=\"2021-07-22, 13:37\">сегодня в 13:37</time></span></div><!-- --></div><h2 class=\"tm-article-snippet__title tm-article-snippet__title_h2\"><a class=\"tm-article-snippet__title-link\" data-article-link=\"\" href=\"/ru/company/sberdevices/blog/569082/\"><span>Как улучшить распознавание русской речи до 3% WER с помощью открытых данных</span></a></h2><div class=\"tm-article-snippet__hubs\"><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/company/sberdevices/blog/\"><span>Блог компании SberDevices</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/machine_learning/\"><span>Машинное обучение</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/artificial_intelligence/\"><span>Искусственный интеллект</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/natural_language_processing/\"><span>Natural Language Processing</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/voice_interfaces/\"><span>Голосовые интерфейсы</span></a></span></div><div class=\"tm-article-snippet__labels\"><!-- --></div><!-- --><div class=\"tm-article-body tm-article-snippet__lead\">\n <div class=\"tm-article-snippet__cover tm-article-snippet__cover_cover\"><img class=\"tm-article-snippet__lead-image\" src=\"https://habrastorage.org/getpro/habr/upload_files/b20/fce/b12/b20fceb12c7a1905403dec9318ea3aa5.png\" style=\"object-position:20.25641025641% 0%;\"/></div><div class=\"article-formatted-body article-formatted-body_version-2\"><p>Меня зовут <a href=\"https://www.linkedin.com/in/nikolay-karpov-42955573\"><u>Николай</u></a>, когда в 2009 году я защищал диссертацию по распознаванию речи, скептики мне говорили, что слишком поздно, так как Microsoft и Google уже “всё сделали”. Сейчас в SberDevices я обучаю модели распознавания речи, которые используются в семействе виртуальных ассистентов Салют и других банковских сервисах. Я расскажу, как обучил модель распознавания речи, используя Common Voice и недавно открытый датасет Golos. Ошибка распознавания составила от 3 до 11 % в зависимости от типа тестовой выборки, что очень неплохо для открытой модели.</p><p>Не так давно наша команда подготовила и опубликовала общедоступный датасет <a href=\"https://github.com/sberdevices/golos\"><u>Golos</u></a>. Почему встал вопрос об обучении и публикации акустической модели QuartzNet? Во-первых, чтобы узнать, какую точность достигает система распознавания речи при обучении на новом датасете. Во-вторых, обучение само по себе ресурсоёмкое, поэтому сообществу полезно иметь в открытом доступе предобученную модель на русском языке. Полная версия статьи опубликована на сайте <a href=\"https://arxiv.org/abs/2106.10161\"><u>arxiv.org</u></a> и будет представлена на конференции <a href=\"https://www.interspeech2021.org\"><u>INRETSPEECH2021</u></a>. </p></div><a class=\"tm-article-snippet__readmore\" href=\"/ru/company/sberdevices/blog/569082/\"><span>Читать далее</span></a></div></div>,\n <div class=\"tm-article-snippet\"><div class=\"tm-article-snippet__meta-container\"><div class=\"tm-article-snippet__meta\"><span class=\"tm-user-info tm-article-snippet__author\"><a class=\"tm-user-info__userpic\" href=\"/ru/users/Alychangin/\" title=\"Alychangin\"><div class=\"tm-entity-image\"><svg class=\"tm-svg-img tm-image-placeholder tm-image-placeholder_lilac\" height=\"24\" width=\"24\"><!-- --><use xlink:href=\"/img/megazord-v24.4c3730a5.svg#placeholder-user\"></use></svg></div></a><span class=\"tm-user-info__user\"><a class=\"tm-user-info__username\" href=\"/ru/users/Alychangin/\">\n       Alychangin\n     </a></span></span><span class=\"tm-article-snippet__datetime-published\"><time datetime=\"2021-07-22T10:14:48.000Z\" title=\"2021-07-22, 13:14\">сегодня в 13:14</time></span></div><!-- --></div><h2 class=\"tm-article-snippet__title tm-article-snippet__title_h2\"><a class=\"tm-article-snippet__title-link\" data-article-link=\"\" href=\"/ru/company/quadcode/blog/569056/\"><span>История длиною в год: как мы на Greenplum 6 (DWH) мигрировали</span></a></h2><div class=\"tm-article-snippet__hubs\"><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/company/quadcode/blog/\"><span>Блог компании Quadcode</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/bigdata/\"><span>Big Data</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/data_engineering/\"><span>Data Engineering</span></a></span></div><div class=\"tm-article-snippet__labels\"><!-- --></div><!-- --><div class=\"tm-article-body tm-article-snippet__lead\">\n <div class=\"tm-article-snippet__cover tm-article-snippet__cover_cover\"><img class=\"tm-article-snippet__lead-image\" src=\"https://habrastorage.org/getpro/habr/upload_files/d68/481/e71/d68481e7162a85c4109fc71b2172c22c.png\" style=\"object-position:0% 0%;\"/></div><div class=\"article-formatted-body article-formatted-body_version-2\"><p>Привет, Хабр! Сегодня расскажем о том, почему и как мы решили мигрировать на Greenplum шестой версии с Greenplum пятой версии. Сразу скажем, что мы каждый день обрабатываем огромное количество данных — шутка ли, у одного из наших клиентов 80 млн пользователей, из которых каждый день активны до 90 тысяч из 178 стран.</p></div><a class=\"tm-article-snippet__readmore\" href=\"/ru/company/quadcode/blog/569056/\"><span>Читать далее</span></a></div></div>,\n <div class=\"tm-article-snippet\"><div class=\"tm-article-snippet__meta-container\"><div class=\"tm-article-snippet__meta\"><span class=\"tm-user-info tm-article-snippet__author\"><a class=\"tm-user-info__userpic\" href=\"/ru/users/GolovinDS/\" title=\"GolovinDS\"><div class=\"tm-entity-image\"><img alt=\"\" class=\"tm-entity-image__pic\" height=\"24\" loading=\"lazy\" src=\"//habrastorage.org/r/w32/getpro/habr/avatars/d27/ccd/743/d27ccd743d8bccbf1a885e5dadaa7ffe.jpg\" width=\"24\"/></div></a><span class=\"tm-user-info__user\"><a class=\"tm-user-info__username\" href=\"/ru/users/GolovinDS/\">\n       GolovinDS\n     </a></span></span><span class=\"tm-article-snippet__datetime-published\"><time datetime=\"2021-07-22T10:13:27.000Z\" title=\"2021-07-22, 13:13\">сегодня в 13:13</time></span></div><!-- --></div><h2 class=\"tm-article-snippet__title tm-article-snippet__title_h2\"><a class=\"tm-article-snippet__title-link\" data-article-link=\"\" href=\"/ru/company/otus/blog/569078/\"><span>Тестирование или управление качеством. Часть 3. Что такое качество?</span></a></h2><div class=\"tm-article-snippet__hubs\"><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/company/otus/blog/\"><span>Блог компании OTUS</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/it_testing/\"><span>Тестирование IT-систем</span></a></span></div><div class=\"tm-article-snippet__labels\"><div class=\"tm-article-snippet__label\"><span>\n           Перевод\n         </span></div></div><!-- --><div class=\"tm-article-body tm-article-snippet__lead\">\n <div class=\"tm-article-snippet__cover tm-article-snippet__cover_cover\"><img class=\"tm-article-snippet__lead-image\" src=\"https://habrastorage.org/getpro/habr/upload_files/6e2/4e4/625/6e24e46254e80a6be7da970e64c3862e.png\" style=\"object-position:0% 0%;\"/></div><div class=\"article-formatted-body article-formatted-body_version-2\"><p>В двух последних постах <a href=\"https://habr.com/ru/company/otus/blog/567348/\"><u>Что такое тестирование?</u></a> и <a href=\"https://habr.com/ru/company/otus/blog/568744/\"><u>Организация тестирования</u></a> я поделилась своими соображениями об испытаниях. Хотя между понятиями «тестирование» и «качество» есть тесная связь, одно из них не обязательно подразумевает второе. Тестирование лишь дает нам какое-то представление о качестве.</p><p>Я хотела завершить эту серию публикаций постом, полностью посвященным качеству и управлению качеством, но у меня оказалось так много мыслей на эту тему, что его пришлось разделить на две части. В первой из них я расскажу о своем взгляде на качество.</p></div><a class=\"tm-article-snippet__readmore\" href=\"/ru/company/otus/blog/569078/\"><span>Читать далее</span></a></div></div>,\n <div class=\"tm-article-snippet\"><div class=\"tm-article-snippet__meta-container\"><div class=\"tm-article-snippet__meta\"><span class=\"tm-user-info tm-article-snippet__author\"><a class=\"tm-user-info__userpic\" href=\"/ru/users/Alexandr_ARgument/\" title=\"Alexandr_ARgument\"><div class=\"tm-entity-image\"><svg class=\"tm-svg-img tm-image-placeholder tm-image-placeholder_pink\" height=\"24\" width=\"24\"><!-- --><use xlink:href=\"/img/megazord-v24.4c3730a5.svg#placeholder-user\"></use></svg></div></a><span class=\"tm-user-info__user\"><a class=\"tm-user-info__username\" href=\"/ru/users/Alexandr_ARgument/\">\n       Alexandr_ARgument\n     </a></span></span><span class=\"tm-article-snippet__datetime-published\"><time datetime=\"2021-07-22T10:12:26.000Z\" title=\"2021-07-22, 13:12\">сегодня в 13:12</time></span></div><!-- --></div><h2 class=\"tm-article-snippet__title tm-article-snippet__title_h2\"><a class=\"tm-article-snippet__title-link\" data-article-link=\"\" href=\"/ru/post/569076/\"><span>Как мы SaaS решение переносили на сервера клиента. Стоит ли оно того?</span></a></h2><div class=\"tm-article-snippet__hubs\"><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/cms/\"><span>CMS</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/it-infrastructure/\"><span>IT-инфраструктура</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/business_models/\"><span>Бизнес-модели</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/devops/\"><span>DevOps</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/arvr/\"><span>AR и VR</span></a></span></div><div class=\"tm-article-snippet__labels\"><div class=\"tm-article-snippet__label\"><span>\n           Из песочницы\n         </span></div></div><!-- --><div class=\"tm-article-body tm-article-snippet__lead\">\n <div class=\"tm-article-snippet__cover tm-article-snippet__cover_cover\"><img class=\"tm-article-snippet__lead-image\" src=\"https://habrastorage.org/getpro/habr/upload_files/efd/ded/77d/efdded77d1c9c66883589c03e4a340e9.jpg\" style=\"object-position:0% 0%;\"/></div><div class=\"article-formatted-body article-formatted-body_version-2\"><p>Мы развиваем российскую платформу по управлению контентом дополненной реальности: в web-редакторе совмещается digital контент и маркер (как привило, это фотография реального объекта), а с помощью мобильного приложения, в AR режиме, digital контент накладывается на маркер. </p><p>В данной статье хочу поделиться опытом с теми, кто начинает развивать свои B2B SaaS продукты и может столкнуться с вопросом переноса своего решения на внутренние ИТ мощности корпоративных заказчиков. Опишу, не столько технические нюансы, сколько организационные и архитектурные. Хочу акцентировать внимание на объеме дополнительных работ, которые необходимо учитывать при продаже внедрения Enterprise клиенту. Тем, кто работает изначально по модели внедрения ПО, читать будет не особо интересно, а SaaS проектам надеюсь, что будет полезно. </p><p>Заказчика не называю.</p></div><a class=\"tm-article-snippet__readmore\" href=\"/ru/post/569076/\"><span>Читать далее</span></a></div></div>,\n <div class=\"tm-article-snippet\"><div class=\"tm-article-snippet__meta-container\"><div class=\"tm-article-snippet__meta\"><span class=\"tm-user-info tm-article-snippet__author\"><a class=\"tm-user-info__userpic\" href=\"/ru/users/Magvai69/\" title=\"Magvai69\"><div class=\"tm-entity-image\"><img alt=\"\" class=\"tm-entity-image__pic\" height=\"24\" loading=\"lazy\" src=\"//habrastorage.org/r/w32/getpro/habr/avatars/9ee/105/1fa/9ee1051faa2d292369aa85cebf5e363a.jpg\" width=\"24\"/></div></a><span class=\"tm-user-info__user\"><a class=\"tm-user-info__username\" href=\"/ru/users/Magvai69/\">\n       Magvai69\n     </a></span></span><span class=\"tm-article-snippet__datetime-published\"><time datetime=\"2021-07-22T10:09:10.000Z\" title=\"2021-07-22, 13:09\">сегодня в 13:09</time></span></div><!-- --></div><h2 class=\"tm-article-snippet__title tm-article-snippet__title_h2\"><a class=\"tm-article-snippet__title-link\" data-article-link=\"\" href=\"/ru/company/flant/blog/568924/\"><span>Мониторинг PostgreSQL. Расшифровка аудиочата Data Egret и Okmeter</span></a></h2><div class=\"tm-article-snippet__hubs\"><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/company/flant/blog/\"><span>Блог компании Флант</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/sys_admin/\"><span>Системное администрирование</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/postgresql/\"><span>PostgreSQL</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/db_admins/\"><span>Администрирование баз данных</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/interviews/\"><span>Интервью</span></a></span></div><div class=\"tm-article-snippet__labels\"><!-- --></div><!-- --><div class=\"tm-article-body tm-article-snippet__lead\">\n <div class=\"tm-article-snippet__cover tm-article-snippet__cover_cover\"><img class=\"tm-article-snippet__lead-image\" src=\"https://habrastorage.org/getpro/habr/upload_files/ffd/ab7/e9d/ffdab7e9d48eac39b453cfc519389774.jpeg\" style=\"object-position:0% 0%;\"/></div><div class=\"article-formatted-body article-formatted-body_version-2\"><p>Представляем текстовую версию <a href=\"https://www.youtube.com/watch?v=GT-bK-uWU-k\"><u>недавнего разговора</u></a> с коллегами из Data Egret — компании, которая специализируется на поддержке PostgreSQL. Ведущий инженер команды Okmeter Владимир Гурьянов пообщался с Ильей Космодемьянским (CEO Data Egret) и Алексеем Лесовским (senior DBA Data Egret). Обсудили, как мониторить PostgreSQL, какие бывают ошибки при выборе и настройке систем мониторинга, кто такие DBA и какие soft skills для них важны, а также затронули более хардкорные темы. Пост объемный, но он того стоит.</p></div><a class=\"tm-article-snippet__readmore\" href=\"/ru/company/flant/blog/568924/\"><span>Читать далее</span></a></div></div>,\n <div class=\"tm-article-snippet\"><div class=\"tm-article-snippet__meta-container\"><div class=\"tm-article-snippet__meta\"><span class=\"tm-user-info tm-article-snippet__author\"><a class=\"tm-user-info__userpic\" href=\"/ru/users/hse_spb/\" title=\"hse_spb\"><div class=\"tm-entity-image\"><img alt=\"\" class=\"tm-entity-image__pic\" height=\"24\" loading=\"lazy\" src=\"//habrastorage.org/r/w32/getpro/habr/avatars/f32/9ca/f7a/f329caf7af39c3758cab77cbf5c92be1.jpg\" width=\"24\"/></div></a><span class=\"tm-user-info__user\"><a class=\"tm-user-info__username\" href=\"/ru/users/hse_spb/\">\n       hse_spb\n     </a></span></span><span class=\"tm-article-snippet__datetime-published\"><time datetime=\"2021-07-22T10:07:01.000Z\" title=\"2021-07-22, 13:07\">сегодня в 13:07</time></span></div><!-- --></div><h2 class=\"tm-article-snippet__title tm-article-snippet__title_h2\"><a class=\"tm-article-snippet__title-link\" data-article-link=\"\" href=\"/ru/company/hsespb/blog/569070/\"><span>Многопользовательская сетевая игра Ticket to Ride</span></a></h2><div class=\"tm-article-snippet__hubs\"><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/company/hsespb/blog/\"><span>Блог компании Питерская Вышка</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/programming/\"><span>Программирование</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/cpp/\"><span>C++</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/gamedev/\"><span>Разработка игр</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/study/\"><span>Учебный процесс в IT</span></a></span></div><div class=\"tm-article-snippet__labels\"><!-- --></div><!-- --><div class=\"tm-article-body tm-article-snippet__lead\">\n <div class=\"tm-article-snippet__cover tm-article-snippet__cover_cover\"><img class=\"tm-article-snippet__lead-image\" src=\"https://habrastorage.org/getpro/habr/upload_files/3e1/3ef/dc8/3e13efdc8832ff524474a27f98dad063.png\" style=\"object-position:0% 0%;\"/></div><div class=\"article-formatted-body article-formatted-body_version-2\"><p>Привет, Хабр! Мы — Тимофей Василевский, Сергей Дымашевский и Максим Чайка — только что окончили первый курс бакалавриата «<a href=\"https://spb.hse.ru/ba/appmath/\"><u>Прикладная математика и информатика</u></a>» в Питерской Вышке. В качестве семестрового проекта по C++ мы написали симулятор всем известной настольной игры Ticket to ride. Что у нас получилось, а что нет, читайте под катом.</p></div><a class=\"tm-article-snippet__readmore\" href=\"/ru/company/hsespb/blog/569070/\"><span>Читать далее</span></a></div></div>,\n <div class=\"tm-article-snippet\"><div class=\"tm-article-snippet__meta-container\"><div class=\"tm-article-snippet__meta\"><span class=\"tm-user-info tm-article-snippet__author\"><a class=\"tm-user-info__userpic\" href=\"/ru/users/Asmodayppl/\" title=\"Asmodayppl\"><div class=\"tm-entity-image\"><svg class=\"tm-svg-img tm-image-placeholder tm-image-placeholder_lilac\" height=\"24\" width=\"24\"><!-- --><use xlink:href=\"/img/megazord-v24.4c3730a5.svg#placeholder-user\"></use></svg></div></a><span class=\"tm-user-info__user\"><a class=\"tm-user-info__username\" href=\"/ru/users/Asmodayppl/\">\n       Asmodayppl\n     </a></span></span><span class=\"tm-article-snippet__datetime-published\"><time datetime=\"2021-07-22T10:00:17.000Z\" title=\"2021-07-22, 13:00\">сегодня в 13:00</time></span></div><!-- --></div><h2 class=\"tm-article-snippet__title tm-article-snippet__title_h2\"><a class=\"tm-article-snippet__title-link\" data-article-link=\"\" href=\"/ru/company/arenadata/blog/566182/\"><span>Как и зачем мы сделали Spark-коннектор к Greenplum</span></a></h2><div class=\"tm-article-snippet__hubs\"><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/company/arenadata/blog/\"><span>Блог компании Arenadata</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/bigdata/\"><span>Big Data</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/data_warehouse/\"><span>Хранилища данных</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/distributed_systems/\"><span>Распределённые системы</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/data_engineering/\"><span>Data Engineering</span></a></span></div><div class=\"tm-article-snippet__labels\"><!-- --></div><!-- --><div class=\"tm-article-body tm-article-snippet__lead\">\n <div class=\"tm-article-snippet__cover tm-article-snippet__cover_cover\"><img class=\"tm-article-snippet__lead-image\" src=\"https://habrastorage.org/getpro/habr/upload_files/258/c89/751/258c897519179f9bfd04e21fe5fe125d.jpg\" style=\"object-position:0% 0%;\"/></div><div class=\"article-formatted-body article-formatted-body_version-2\"><p>Всем привет! Меня зовут Андрей, я работаю системным архитектором в Arenadata. В этой статье расскажу, как и зачем мы сделали свой инструмент для обмена данными между Arenadata DB (аналитическая MPP-СУБД на базе Greenplum) и фреймворком для распределенной обработки данных Apache Spark (входит в экосистему Arenadata Hadoop). </p></div><a class=\"tm-article-snippet__readmore\" href=\"/ru/company/arenadata/blog/566182/\"><span>Читать далее</span></a></div></div>,\n <div class=\"tm-article-snippet\"><div class=\"tm-article-snippet__meta-container\"><div class=\"tm-article-snippet__meta\"><span class=\"tm-user-info tm-article-snippet__author\"><a class=\"tm-user-info__userpic\" href=\"/ru/users/ip63/\" title=\"ip63\"><div class=\"tm-entity-image\"><img alt=\"\" class=\"tm-entity-image__pic\" height=\"24\" loading=\"lazy\" src=\"//habrastorage.org/r/w32/getpro/habr/avatars/081/c5d/66f/081c5d66f60cffa267483ccc69e38838.jpg\" width=\"24\"/></div></a><span class=\"tm-user-info__user\"><a class=\"tm-user-info__username\" href=\"/ru/users/ip63/\">\n       ip63\n     </a></span></span><span class=\"tm-article-snippet__datetime-published\"><time datetime=\"2021-07-22T10:00:07.000Z\" title=\"2021-07-22, 13:00\">сегодня в 13:00</time></span></div><!-- --></div><h2 class=\"tm-article-snippet__title tm-article-snippet__title_h2\"><a class=\"tm-article-snippet__title-link\" data-article-link=\"\" href=\"/ru/post/568798/\"><span>Как в Trello оценить процессные задачи и построить их визуализацию?</span></a></h2><div class=\"tm-article-snippet__hubs\"><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/programming/\"><span>Программирование</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/api/\"><span>API</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/r/\"><span>R</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/data_visualization/\"><span>Визуализация данных</span></a></span></div><div class=\"tm-article-snippet__labels\"><div class=\"tm-article-snippet__label\"><span>\n           Tutorial\n         </span></div></div><!-- --><div class=\"tm-article-body tm-article-snippet__lead\">\n <div class=\"tm-article-snippet__cover tm-article-snippet__cover_cover\"><img class=\"tm-article-snippet__lead-image\" src=\"https://habrastorage.org/getpro/habr/upload_files/366/a1f/273/366a1f273e2b8f4a6b4bca02d65170e8.png\" style=\"object-position:0% 0%;\"/></div><div class=\"article-formatted-body article-formatted-body_version-2\"><p>Если вы задавались вопросом:<br/>- \"Как четко (<em>или почти четко</em>) измерить эффективность процессной работы программистов, маркетологов, аналитиков, дизайнеров и на основе этих измерений построить визуализацию?\" - то эта статья будет вам интересна.</p><p>В решении задачи используем язык программирования <strong>R</strong>, графики на <strong>ggplot</strong> и <strong>Trello</strong> как инструмент таск-менеджмента в том виде в котором ранее скорее всего вы его не использовали.</p></div><a class=\"tm-article-snippet__readmore\" href=\"/ru/post/568798/\"><span>Разберем подробно</span></a></div></div>,\n <div class=\"tm-article-snippet\"><div class=\"tm-article-snippet__meta-container\"><div class=\"tm-article-snippet__meta\"><span class=\"tm-user-info tm-article-snippet__author\"><a class=\"tm-user-info__userpic\" href=\"/ru/users/Promwad/\" title=\"Promwad\"><div class=\"tm-entity-image\"><img alt=\"\" class=\"tm-entity-image__pic\" height=\"24\" loading=\"lazy\" src=\"//habrastorage.org/r/w32/getpro/habr/avatars/e4b/dc5/d7a/e4bdc5d7ac683edcb2a87e974ab1d291.png\" width=\"24\"/></div></a><span class=\"tm-user-info__user\"><a class=\"tm-user-info__username\" href=\"/ru/users/Promwad/\">\n       Promwad\n     </a></span></span><span class=\"tm-article-snippet__datetime-published\"><time datetime=\"2021-07-22T10:00:03.000Z\" title=\"2021-07-22, 13:00\">сегодня в 13:00</time></span></div><!-- --></div><h2 class=\"tm-article-snippet__title tm-article-snippet__title_h2\"><a class=\"tm-article-snippet__title-link\" data-article-link=\"\" href=\"/ru/post/568876/\"><span>Искусство отладки FPGA: как сократить срок тестирования за счет грамотной разработки</span></a></h2><div class=\"tm-article-snippet__hubs\"><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/hi/\"><span>Высокая производительность</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/system_programming/\"><span>Системное программирование</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/debug/\"><span>Отладка</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/fpga/\"><span>FPGA</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/electronics/\"><span>Производство и разработка электроники</span></a></span></div><div class=\"tm-article-snippet__labels\"><!-- --></div><!-- --><div class=\"tm-article-body tm-article-snippet__lead\">\n <div class=\"tm-article-snippet__cover tm-article-snippet__cover_cover\"><img class=\"tm-article-snippet__lead-image\" src=\"https://habrastorage.org/getpro/habr/upload_files/9bf/323/2cc/9bf3232cc174c8072b9845ed90fae2a9.jpg\" style=\"object-position:0% 0%;\"/></div><div class=\"article-formatted-body article-formatted-body_version-2\"><p>Давайте попробуем оптимизировать самый времязатратный этап разработки устройств на базе ПЛИС — отладку прошивки. В этой статье мы расскажем о принципе 20/80 при планировании времени, рассмотрим инструменты для отладки FPGA, вспомним Гордона Мура и Уинстона Черчилля (да-да), затроним отладку сложных распределенных систем и внешних интерфейсов, а в конце — разберемся с типичными ошибками и поделимся полезными практическими советами.</p><p>Для начала рассмотрим типовой цикл разработки и моделирования FPGA-прошивки:</p></div><a class=\"tm-article-snippet__readmore\" href=\"/ru/post/568876/\"><span>Читать далее</span></a></div></div>,\n <div class=\"tm-article-snippet\"><div class=\"tm-article-snippet__meta-container\"><div class=\"tm-article-snippet__meta\"><span class=\"tm-user-info tm-article-snippet__author\"><a class=\"tm-user-info__userpic\" href=\"/ru/users/Stedihabr/\" title=\"Stedihabr\"><div class=\"tm-entity-image\"><svg class=\"tm-svg-img tm-image-placeholder tm-image-placeholder_lilac\" height=\"24\" width=\"24\"><!-- --><use xlink:href=\"/img/megazord-v24.4c3730a5.svg#placeholder-user\"></use></svg></div></a><span class=\"tm-user-info__user\"><a class=\"tm-user-info__username\" href=\"/ru/users/Stedihabr/\">\n       Stedihabr\n     </a></span></span><span class=\"tm-article-snippet__datetime-published\"><time datetime=\"2021-07-22T10:00:02.000Z\" title=\"2021-07-22, 13:00\">сегодня в 13:00</time></span></div><!-- --></div><h2 class=\"tm-article-snippet__title tm-article-snippet__title_h2\"><a class=\"tm-article-snippet__title-link\" data-article-link=\"\" href=\"/ru/company/itsoft/blog/568998/\"><span>Компьютерная ностальгия по девяностым</span></a></h2><div class=\"tm-article-snippet__hubs\"><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/company/itsoft/blog/\"><span>Блог компании ITSOFT</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/antikvariat/\"><span>Старое железо</span></a></span></div><div class=\"tm-article-snippet__labels\"><div class=\"tm-article-snippet__label\"><span>\n           Перевод\n         </span></div></div><!-- --><div class=\"tm-article-body tm-article-snippet__lead\">\n <div class=\"tm-article-snippet__cover tm-article-snippet__cover_cover\"><img class=\"tm-article-snippet__lead-image\" src=\"https://habrastorage.org/getpro/habr/upload_files/f65/96e/b53/f6596eb536edb4da38e6ab962d823fb2.png\" style=\"object-position:0% 30%;\"/></div><div class=\"article-formatted-body article-formatted-body_version-2\"><p>Если вы находитесь на этом сайте, то наверняка разбираетесь в компьютерах. Я никакой не компьютерный эксперт (далек от этого), я — обычный пользователь, и хотел бы поделиться первыми и самыми теплыми воспоминаниями о компьютерах и интернете.  </p></div><a class=\"tm-article-snippet__readmore\" href=\"/ru/company/itsoft/blog/568998/\"><span>Читать далее</span></a></div></div>,\n <div class=\"tm-article-snippet\"><div class=\"tm-article-snippet__meta-container\"><div class=\"tm-article-snippet__meta\"><span class=\"tm-user-info tm-article-snippet__author\"><a class=\"tm-user-info__userpic\" href=\"/ru/users/F5Habr/\" title=\"F5Habr\"><div class=\"tm-entity-image\"><svg class=\"tm-svg-img tm-image-placeholder tm-image-placeholder_blue\" height=\"24\" width=\"24\"><!-- --><use xlink:href=\"/img/megazord-v24.4c3730a5.svg#placeholder-user\"></use></svg></div></a><span class=\"tm-user-info__user\"><a class=\"tm-user-info__username\" href=\"/ru/users/F5Habr/\">\n       F5Habr\n     </a></span></span><span class=\"tm-article-snippet__datetime-published\"><time datetime=\"2021-07-22T09:54:05.000Z\" title=\"2021-07-22, 12:54\">сегодня в 12:54</time></span></div><!-- --></div><h2 class=\"tm-article-snippet__title tm-article-snippet__title_h2\"><a class=\"tm-article-snippet__title-link\" data-article-link=\"\" href=\"/ru/company/factory5/blog/569066/\"><span>Платформы анализа данных: что они умеют и как понять, нужны ли они вашему бизнесу</span></a></h2><div class=\"tm-article-snippet__hubs\"><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/company/factory5/blog/\"><span>Блог компании Factory5</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/bigdata/\"><span>Big Data</span></a></span></div><div class=\"tm-article-snippet__labels\"><!-- --></div><!-- --><div class=\"tm-article-body tm-article-snippet__lead\">\n <div class=\"tm-article-snippet__cover tm-article-snippet__cover_cover\"><img class=\"tm-article-snippet__lead-image\" src=\"https://habrastorage.org/getpro/habr/upload_files/dde/090/361/dde09036133c236db2e5c5038ebaf758.png\" style=\"object-position:0% 0%;\"/></div><div class=\"article-formatted-body article-formatted-body_version-2\"><p>Рынок ИТ- продуктов переполнен предложениями платформенных решений для анализа больших данных: их обсуждают, рекомендуют и внедряют, но всем ли они необходимы? Алексей Ершов, эксперт по продуктам Factory5 (входит в группу Ctrl2GO), ответил на главные вопросы об аналитических платформах для ИТ-директоров, менеджеров проектов и других участников data science инициатив на предприятиях.  </p></div><a class=\"tm-article-snippet__readmore\" href=\"/ru/company/factory5/blog/569066/\"><span>Читать далее</span></a></div></div>,\n <div class=\"tm-article-snippet\"><div class=\"tm-article-snippet__meta-container\"><div class=\"tm-article-snippet__meta\"><span class=\"tm-user-info tm-article-snippet__author\"><a class=\"tm-user-info__userpic\" href=\"/ru/users/vgaidadei/\" title=\"vgaidadei\"><div class=\"tm-entity-image\"><svg class=\"tm-svg-img tm-image-placeholder tm-image-placeholder_lilac\" height=\"24\" width=\"24\"><!-- --><use xlink:href=\"/img/megazord-v24.4c3730a5.svg#placeholder-user\"></use></svg></div></a><span class=\"tm-user-info__user\"><a class=\"tm-user-info__username\" href=\"/ru/users/vgaidadei/\">\n       vgaidadei\n     </a></span></span><span class=\"tm-article-snippet__datetime-published\"><time datetime=\"2021-07-22T09:39:01.000Z\" title=\"2021-07-22, 12:39\">сегодня в 12:39</time></span></div><!-- --></div><h2 class=\"tm-article-snippet__title tm-article-snippet__title_h2\"><a class=\"tm-article-snippet__title-link\" data-article-link=\"\" href=\"/ru/post/569058/\"><span>Easy canvas — простая js библиотека, которая невероятно упрощает работу с canvas html</span></a></h2><div class=\"tm-article-snippet__hubs\"><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/javascript/\"><span>JavaScript</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/html5/\"><span>HTML</span></a></span><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/canvas/\"><span>Canvas</span></a></span></div><div class=\"tm-article-snippet__labels\"><div class=\"tm-article-snippet__label\"><span>\n           Из песочницы\n         </span></div></div><!-- --><div class=\"tm-article-body tm-article-snippet__lead\">\n <div class=\"tm-article-snippet__cover tm-article-snippet__cover_cover\"><img class=\"tm-article-snippet__lead-image\" src=\"https://habrastorage.org/getpro/habr/upload_files/ae6/5e3/e12/ae65e3e12f5c4283ef63ab959043741f.png\" style=\"object-position:0% 0%;\"/></div><div class=\"article-formatted-body article-formatted-body_version-2\"><p><strong><a href=\"https://github.com/fakt309/easycanvas\" rel=\"noopener noreferrer nofollow\">Ссылка на GitHub</a></strong></p><p>Очень простая библиотека javascript, которая сильно упрощает работу с canvas html.</p></div><a class=\"tm-article-snippet__readmore\" href=\"/ru/post/569058/\"><span>Читать</span></a></div></div>,\n <div class=\"tm-article-snippet\"><div class=\"tm-article-snippet__meta-container\"><div class=\"tm-article-snippet__meta\"><span class=\"tm-user-info tm-article-snippet__author\"><a class=\"tm-user-info__userpic\" href=\"/ru/users/chemtech/\" title=\"chemtech\"><div class=\"tm-entity-image\"><img alt=\"\" class=\"tm-entity-image__pic\" height=\"24\" loading=\"lazy\" src=\"//habrastorage.org/r/w32/getpro/habr/avatars/e18/935/57e/e1893557eeaacf388b0e596d910014c8.jpg\" width=\"24\"/></div></a><span class=\"tm-user-info__user\"><a class=\"tm-user-info__username\" href=\"/ru/users/chemtech/\">\n       chemtech\n     </a></span></span><span class=\"tm-article-snippet__datetime-published\"><time datetime=\"2021-07-22T09:18:25.000Z\" title=\"2021-07-22, 12:18\">сегодня в 12:18</time></span></div><!-- --></div><h2 class=\"tm-article-snippet__title tm-article-snippet__title_h2\"><a class=\"tm-article-snippet__title-link\" data-article-link=\"\" href=\"/ru/post/569054/\"><span>13 рекомендаций по использованию Helm</span></a></h2><div class=\"tm-article-snippet__hubs\"><span class=\"tm-article-snippet__hubs-item\"><a class=\"tm-article-snippet__hubs-item-link\" href=\"/ru/hub/devops/\"><span>DevOps</span></a></span></div><div class=\"tm-article-snippet__labels\"><div class=\"tm-article-snippet__label\"><span>\n           Перевод\n         </span></div></div><!-- --><div class=\"tm-article-body tm-article-snippet__lead\">\n <!-- --><div class=\"article-formatted-body article-formatted-body_version-1\"><p>Helm — незаменимый инструмент для развертывания приложений в кластерах Kubernetes. Но только следуя передовому опыту, вы действительно ощутите преимущества Helm. Вот 13 рекомендаций, которые помогут вам создавать, использовать и обновлять приложения с помощью Helm.</p></div><a class=\"tm-article-snippet__readmore\" href=\"/ru/post/569054/\"><span>Читать дальше →</span></a></div></div>]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking for all articles\n",
    "posts = soup.find_all('div', class_='tm-article-snippet')\n",
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "christian-aircraft",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['2021-07-22T11:03:04.000Z',\n '2021-07-22T11:01:53.000Z',\n '2021-07-22T11:01:36.000Z',\n '2021-07-22T10:59:35.000Z',\n '2021-07-22T10:59:31.000Z',\n '2021-07-22T10:53:18.000Z',\n '2021-07-22T10:50:27.000Z',\n '2021-07-22T10:37:39.000Z',\n '2021-07-22T10:14:48.000Z',\n '2021-07-22T10:13:27.000Z',\n '2021-07-22T10:12:26.000Z',\n '2021-07-22T10:09:10.000Z',\n '2021-07-22T10:07:01.000Z',\n '2021-07-22T10:00:17.000Z',\n '2021-07-22T10:00:07.000Z',\n '2021-07-22T10:00:03.000Z',\n '2021-07-22T10:00:02.000Z',\n '2021-07-22T09:54:05.000Z',\n '2021-07-22T09:39:01.000Z',\n '2021-07-22T09:18:25.000Z']"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracting dates for all articles\n",
    "posts_date = list(map(lambda x: str(x.find('span', class_='tm-article-snippet__datetime-published').time)[16:40], posts))\n",
    "posts_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "unique-relevance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['Яндекс открывает датасеты Беспилотных автомобилей, Погоды и Переводчика, чтобы помочь решить проблему сдвига данных в ML',\n 'О небольших, но бесяще важных различиях текстовых редакторов',\n 'Горячая перезагрузка .NET: новая возможность для редактирования кода во время выполнения приложений',\n 'Radxa ROCK 3A: конкурент Raspberry Pi со слотом M.2 для NVMe SSD',\n 'Памятка по жизненному циклу Android — часть I. Отдельные Activity',\n 'Гайд: как создавать собственные активности для RPA-платформ',\n 'Преобразуем CentOS в RHEL с помощью Convert2RHEL и Red Hat Satellite',\n 'Как улучшить распознавание русской речи до 3% WER с помощью открытых данных',\n 'История длиною в год: как мы на Greenplum 6 (DWH) мигрировали',\n 'Тестирование или управление качеством. Часть 3. Что такое качество?',\n 'Как мы SaaS решение переносили на сервера клиента. Стоит ли оно того?',\n 'Мониторинг PostgreSQL. Расшифровка аудиочата Data Egret и Okmeter',\n 'Многопользовательская сетевая игра Ticket to Ride',\n 'Как и зачем мы сделали Spark-коннектор к Greenplum',\n 'Как в Trello оценить процессные задачи и построить их визуализацию?',\n 'Искусство отладки FPGA: как сократить срок тестирования за счет грамотной разработки',\n 'Компьютерная ностальгия по девяностым',\n 'Платформы анализа данных: что они умеют и как понять, нужны ли они вашему бизнесу',\n 'Easy canvas — простая js библиотека, которая невероятно упрощает работу с canvas html',\n '13 рекомендаций по использованию Helm']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracting headers for all articles\n",
    "posts_title = list(map(lambda x: x.find('h2', class_='tm-article-snippet__title tm-article-snippet__title_h2').text, posts))\n",
    "posts_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "executed-mystery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['https://habr.com/ru/company/yandex/blog/568672/',\n 'https://habr.com/ru/company/r7-office/blog/568942/',\n 'https://habr.com/ru/company/otus/blog/569094/',\n 'https://habr.com/ru/company/selectel/blog/568724/',\n 'https://habr.com/ru/post/569092/',\n 'https://habr.com/ru/company/uipath/blog/569090/',\n 'https://habr.com/ru/company/redhatrussia/blog/569084/',\n 'https://habr.com/ru/company/sberdevices/blog/569082/',\n 'https://habr.com/ru/company/quadcode/blog/569056/',\n 'https://habr.com/ru/company/otus/blog/569078/',\n 'https://habr.com/ru/post/569076/',\n 'https://habr.com/ru/company/flant/blog/568924/',\n 'https://habr.com/ru/company/hsespb/blog/569070/',\n 'https://habr.com/ru/company/arenadata/blog/566182/',\n 'https://habr.com/ru/post/568798/',\n 'https://habr.com/ru/post/568876/',\n 'https://habr.com/ru/company/itsoft/blog/568998/',\n 'https://habr.com/ru/company/factory5/blog/569066/',\n 'https://habr.com/ru/post/569058/',\n 'https://habr.com/ru/post/569054/']"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracting links for all articles\n",
    "posts_link = list(map(lambda x: 'https://habr.com' + x.find('a', class_='tm-article-snippet__title-link').get('href'), posts))\n",
    "posts_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "organizational-juvenile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['\\n\\r\\nВ рамках конкурса Shifts Challenge мы выкладываем в открытый доступ крупнейший в мире датасет для обучения беспилотных автомобилей, а также данные Яндекс.Переводчика и Погоды. Приглашаем исследователей в области машинного обучения присоединиться к поиску решения проблемы сдвига распределения данных в реальном мире по отношению к тому, с чем моделям приходится иметь дело при обучении.\\n\\r\\nМеня зовут Андрей Малинин, я старший исследователь в Yandex Research. Сегодня я расскажу о проблеме, о наших датасетах, а также о конкурсе, который мы проводим в рамках международной конференции NeurIPS 2021 совместно с учеными из Оксфордского и Кембриджского университетов.\\n\\nСдвиг распределения данных\\r\\nЧтобы разработать модель, обычно используются три выборки данных: обучающая, отладочная и проверочная (те данные, с которыми модели придется работать в реальном мире). Считается, что эти три выборки, с одной стороны, независимы друг от друга (данные, например, в отладочной и проверочной выборках не должны пересекаться), а во-вторых, имеют одинаковое распределение. Исходя из этого, можно предположить, что модель, которая хорошо работает на отладочных данных, хорошо будет вести себя и в реальном мире — так как данные, пусть и разные, распределены одинаково.\\n\\r\\nК сожалению, в реальном мире все намного сложнее: разрабатывая модель, мы не можем наперед просчитать все ситуации, в которые она попадет. Наша модель в любой момент может столкнуться с так называемым сдвигом распределения (англ. distributional shift) — то есть ситуации, в которые она попадет, будут ей незнакомы просто потому, что она никогда не видела таких данных и не знает, как себя вести.\\n\\r\\nЕсли сдвиг распределения данных в реальном мире значителен относительно обучающей выборки (например, модель машинного перевода, которая училась на классической литературе, вдруг возьмется переводить твиты), недостаточно надежные модели будут ошибаться. Поэтому, попадая в реальный мир, модели должны быть готовы справляться со сдвигом, который их там ждет.\\n\\r\\nЭтим, однако, дело не ограничивается. Чтобы модель работала эффективно и безошибочно, о вероятности ошибки лучше знать заранее: следовательно, модель должна давать оценку неопределенности относительно своих решений. Поэтому мы хотим добиться от моделей, с одной стороны, устойчивой работы при сдвиге распределения, а с другой — умения оценивать неопределенность.\\n\\r\\nСейчас большинство исследований, посвященных устойчивости к сдвигу распределения и неопределенности, нацелены на решение задачи классификации изображений. Задача эта, безусловно, очень важна, но в масштабных промышленных проектах, помимо классификации изображений, есть и другие — включая те, в которых влияние ошибок, вызванных сдвигами распределения, может быть довольно значительным.\\n\\nShifts Challenge\\r\\nМы хотим помочь ML-сообществу преодолеть разрыв между исследованиями и технологиями в реальном мире, поэтому объявляем о соревнованиях Shifts Challenge, которые проводим вместе с Оксфордским и Кембриджским университетами в рамках конференции NeurIPS 2021. \\n\\r\\nЦель соревнования — повысить осведомленность сообщества о влиянии сдвигов распределения данных на работу моделей, разработать модели, устойчивые к сдвигам, а также научиться определять сдвиги, оценивая степень неопределенности в прогнозах.\\n\\r\\nВ конкурсе три трека, для которых мы подготовили датасеты из разных сфер применения моделей: прогнозирования погоды, машинного перевода и предсказания участников движения на дороге. \\n\\r\\nКонкурс проводится в два этапа:\\n\\nПервый этап, с 16 июля по 17 октября. Мы публикуем обучающий и отладочный датасеты. Участники разрабатывают модели и загружают их на отладочный лидерборд.\\n\\nВторой этап, с 17 октября по 31 октября. Мы публикуем проверочную выборку. У участников есть две недели, чтобы настроить модели и загрузить их предсказания на сайт. Все результаты появятся в проверочном лидерборде.\\n\\r\\nПосле этого мы в течение месяца будем проверять решения участников, а 30 ноября объявим победителей: участники, занявшие призовые места, получат денежные призы.\\n\\r\\nА теперь подробнее расскажу о треках. \\n\\nПредсказание траекторий движения\\r\\nПроблема прогнозирования движения — одна из самых важных в сфере автономного вождения: беспилотному автомобилю, как и любому другому, требуется время на изменение скорости и направления. Чтобы обеспечить безопасную и комфортную поездку, модуль планирования должен предсказывать, где окажутся другие автомобили и прочие участники движения (например, пешеходы), на несколько секунд вперед. \\n\\n\\r\\nБольшинство компаний, разрабатывающих беспилотные автомобили, тестируют свои технологии в одной или нескольких локациях. Мир при этом невероятно разнообразен, и в новых городах или странах автомобили будут встречаться с абсолютно новыми для себя условиями движения, будь то другая погода или различия в ПДД. Кроме того, даже в очень похожих ситуациях манера вождения в разных странах может отличаться. Например, в Тель-Авиве водители чаще торопятся въехать на круговой перекресток, а в Иннополисе наоборот спешат выехать с него.\\n\\r\\nПоэтому очень важно научить беспилотный транспорт применять в новых для него местах как можно больше накопленных знаний и опыта — то есть справляться с тем самым сдвигом распределения. Другими словами, беспилотный автомобиль, который учился ездить безоблачным летом, должен справиться с поездкой по заснеженным дорогам.\\n\\r\\nВ рамках этого трека мы публикуем крупнейший в мире датасет прогнозирования движения транспортных средств. Он собран разработчиками беспилотных автомобилей Яндекса и содержит 600 тысяч размеченных сцен (это более 1600 часов вождения): в датасет вошли поездки беспилотников по городам России, США и Израиля в самую разную погоду — от безоблачного ясного дня до заснеженной ветреной ночи. Суть задания в этом треке: спрогнозировать для каждого транспортного средства 5-секундные траектории движения, а также выдать понятную метрику неопределенности прогноза.\\n\\nМашинный перевод\\n\\n\\r\\nСервисы машинного перевода, такие как Яндекс.Переводчик, часто сталкиваются с нетипичным и необычным использованием языка, включая сленг, ненормативную лексику, плохую грамматику, орфографические и пунктуационные ошибки, а также эмодзи — язык, а особенно повседневный онлайн-вариант, все же достаточно живой. Для современных моделей, которые используются в машинном переводе, такой язык представляет серьезную проблему, так как большинство переводчиков обучаются на чуть более формальном языке: классической литературе, юридических документах или статьях Википедии. \\n\\r\\nДля качественного машинного перевода важно, чтобы модели были устойчивы к нестандартной лексике и ошибкам и в любом случае выдавали качественный перевод — даже если перевести нужно фразу из мема. \\n\\r\\nВ треке перевода мы использовали для обучения англо-русский корпус WMT’20, который в основном состоит из государственных и новостных текстов. В них мало ошибок, язык формальный — именно на таких «стерильных» текстах обучаются многие системы машинного перевода. \\n\\r\\nДанные для отладки и проверки модели состоят из двух частей: со сдвигом и без. Данные без сдвига взяты из англо-русского корпуса Newstest’19, а также из корпуса новостных текстов, собранных службой Global Voices и переведенных Яндексом. Данные со сдвигом для отладки взяты из подготовленного для WMT Robustness Challenge корпуса Reddit и также переведены Яндексом. Для проверки модели на данных со сдвигом мы также собрали, перевели и разметили дополнительные данные с Reddit.\\n\\r\\nЗадача модели — перевести предложение с исходного языка на целевой. Важно, чтобы модель была устойчива к неформальному и нетипичному использованию языка, обеспечивала высокое качества перевода на выборках со сдвигом распределения и указывала высокую степень неопределенности на тех примерах, на которых она ошибается.\\n\\nПредсказание погоды\\n\\n\\r\\nСоставление прогноза погоды с помощью моделей — тоже задача нетривиальная. Во-первых, данные по климатическим зонам могут быть нерепрезентативны: где-то метеостанций больше, где-то — меньше. Кроме того, климатические процессы довольно нестационарны (другими словами, климат весьма изменчив). Поэтому даже самые качественные модели предсказания погоды могут со временем стать неактуальными и давать неправильные прогнозы.\\n\\r\\nДля трека предсказания погоды мы предоставляем участникам данные, собранные командой Яндекс.Погоды. Они табличные, каждый временной отрезок соотнесен с разными климатическими характеристиками: показателями атмосферного давления, влажности, измерениями ожидаемой температуры на определенной широте и долготе и так далее — всего 111 характеристик.\\n\\r\\nУчастникам трека нужно разработать модель, предсказывающую измеренную метеостанцией температуру на определенной широте и долготе в определенный момент времени с учетом всех доступных измерений и прогнозов. Модель должна быть устойчива к сдвигам по времени и климатическим зонам, а также указывать высокую степень неопределенности на примерах, на которых она ошибается.\\n\\nКак начать\\r\\nПодробные условия, тайминг и описания доступных датасетов можно найти на сайте конкурса. Детали датасетов, метрик оценки и бенчмарков есть в нашей статье.\\n\\r\\nСсылки на данные доступны в нашем репозитории на GitHub. Чтобы помочь вам начать работу, мы загрузили туда примеры и сделали базовые модели доступными для скачивания.\\n\\r\\nПризываем участников челленджа присоединиться к нашему сообществу в Slack — там можно задавать вопросы организаторам и дискутировать с другими участниками.\\n\\r\\nМы надеемся, что NeurIPS 2021 Shifts Challenge приблизит нас к пониманию того, как модели могут предсказывать неопределенность, работать со сдвигами распределения в данных, и как следствие — к более надежным и безопасным технологиям.',\n \"Привет! Я из команды «Р7-Офис», и я немного гик текстового редактирования. Например, в MS Word, равно как и в нашем редакторе Р7, есть двойное зачёркивание, а вот в других русских офисах такой фичи «из коробки» нет. На самом деле это очень русская фишка: у нас есть госкомпании, которые по своим стандартам должны использовать именно двойное зачёркивание в ряде ситуаций. И если эту функцию не поддерживать, то где-то далеко в Сибири заплачет ещё один инженер. Выбор начала нумерации нумерованного списка — диалоговое окно У нас нет разницы между ядрами онлайн-редактора и десктопной версией. У большинства офисных пакетов эволюция прослеживается с тех времён, когда никакого онлайна толком не было и внутри монолита можно было построить Римскую империю. Поэтому при переходе к онлайну они обычно не трогали старую кодовую базу, а просто выписывали её основные свойства в бэклог и повторяли в новой версии. Как метод рефакторинга подход замечательный, если не считать того, что полностью скопировать исходный функционал не всегда получается (и не всегда экономически оправданно, видимо) и в итоге приходится поддерживать два разных продукта, которые нередко для пользователя выглядят как один.Но я бы хотел рассказать немного о буднях того, что происходит в «исконно славянском труЪ офисе». Начнём с подхода к открытию и сохранению docx и плавно перейдём к тому, каких функций вам не хватает. Подход к открытию и сохранению docx/xlsxЭто то, что в первую очередь волнует почти всех клиентов, потому что миграция с MS на другие решения — это почти всегда поломанные документы. А много поломанных документов часто означает конец демо и миграцию обратно, даже если нужно очень сильно исхитриться и так купить MS Office, чтобы Microsoft не заметила, что продаёт его какому-нибудь оборонному комплексу. Я сейчас шучу, конечно. Форматы *.docx и *.xlsx стали открытыми по ISO / IEC 29500: 2008, как можно догадаться, в 2008 году. Давайте ещё раз: docx/xlsx — это НЕ проприетарные форматы Microsoft (но при этом doc/xls — устаревшие проприетарные). Историческое наследие или какие-то другие причины заставляют многие офисные пакеты при открытии этих файлов выполнять следующие операции:Чтение всего файла целиком.Конвертацию в свой «родной исконный» формат, с которым работает редактор.При редактировании — работать с временным файлом целиком в родном формате.При сохранении — конвертировать обратно в docx. Внутри docx представляет собой, упрощая, XML и все вложенные файлы. Кстати, можно переименовать файл в *.zip, распаковать его и получить все картинки документа, причём правильно пронумерованные. Этот самый XML описывается не MS-стандартом, поэтому MS его просто интерпретирует и исполняет как получается. Дальше ситуация — как с браузерами: стандарт вроде один, а рендер у всех разный. Так и с документами: рендер у всех разный. И проблема в том, что при полной конвертации любая ошибка будет записана обратно в файл, и он побьётся. Следующий интересный момент в том, что будет, если записать в файл что-то, что «не ест» какой-то из офисов, например, реализацию чего-то, что не умеет обрабатывать при конвертации маковский Pages или виндовый MS Office. Где-то это полностью поломает документ, и он не будет открываться, где-то это уберёт часть элементов (например, таблицы или автофигуры) и так далее. У нас в Р7-Офис подход — интерпретация формата. То есть, когда пользователь открывает docx, мы читаем всё то, что там есть, и рендерим ему. При редактировании какого-то блока мы лезем в XML только в этом блоке, записывая его редактирование. В остальном документ сохраняется в неизменном виде, что позволяет очень аккуратно с ним работать. Если в документе есть что-то, чего мы не умеем редактировать, то оно рендерится только на просмотр. Если там есть что-то, чего мы вообще не понимаем, то оно не рендерится, но сохраняется в документе для того редактора, который это нахезал. Соответственно, обновляя код ядра редактора, мы можем последовательно улучшать работу в точках совместимости, если пользователи репортят что-то важное. Да, и ещё в пакете идут метрические аналоги типовых шрифтов, которые в итоге не ломают разметку документа: это тоже достаточно важно при миграции. Отличия для гиков Разница в архитектурах «десктоп-онлайн» часто ведёт к странным последствиям. Например, если у того же MS Office 365 — разные возможности текстового редактора на десктопе и в онлайне, то у нас — одно общее ядро. Это приводит к тому, что пользователи MS не могут, например, в онлайн-редактировании задать начало нумерации списка (и он всегда будет начинаться с того, что злой дух-покровитель грабель считает правильным); не могут удалить одну ячейку в таблице, но могут столбец или строчку, то есть просто кто-то не дописал одно диалоговое окно; не могут разбить текст на колонки и так далее. Но не надо думать, что редактор MS — это аналог WordPad'а по уровню сложности. Ещё один русский облачный пакет, например, не даст вам настроить параметры абзаца, буквицу и иероглифы, добавить водяной знак и так далее (а MS всё это поддерживает). Расскажу, отчего плакали и кололись наши пользователи. Всего за крупный релиз (с частотой примерно в полгода, например, с 5.5 на 6.0) у нас приходится около 500 разных фич, но просто покажу несколько ярких примеров. У нас есть возможность делать текст всеми заглавными, всеми строчными или капительным набором. Это пример функции, которая была очень важна первым корпоративным пользователям: видимо, часто в документах такое встречается. Не везде есть изменение стиля обтекания для таблиц (чтобы была встроенная в документ плавающая таблица), настройка диаграммы-пирога мышью (кому-то было критично «раздвигать» куски). Где-то далеко в снегах люди сатанели оттого, что нельзя вставить в таблицу сразу несколько строк или столбцов. Потому что кое-где можно только по одной или по одному соответственно. Не везде есть вставка изображений из сети по URL-адресу: это не хотлинкинг, это загрузка картинки и импорт в документ. По локальному-то адресу можно везде, а вот с URI есть проблемы. Кому-то очень не хватало направляющих при перемещении объектов, чтобы при хватании мышкой за объект можно было выравнивать его по другим объектам. Думаю, что отсутствие таких направляющих — это очень циничное издевательство над перфекционистами. Кое-где мы спасли штабного писаря (и ещё сотню маркетологов) тем, что рассказали про функцию слияния: это когда можно взять шаблон письма docx, слить с таблицей имён/должностей/адресов в xlsx и получить набор личных документов для каждого. Дембельский аккорд занял у этого человека минут на 15 времени больше, чем понадобилось для чтения соответствующего раздела справки. Но при этом кое-где нас ограничивает как раз MS Office: например, нам ничего не мешает делать фичи, которые в нём не поддерживаются, только они будут пропадать в его рендере. Так, например, если в режиме рецензирования оставить комментарий к документу, то можно оставить на него же ответ. Никто не мешает оставить ответ на ответ, чтобы получались ветки комментариев, ну, кроме тех, кто интерпретировал стандарт. В итоге мы тоже ограничиваем эти ответы вторым уровнем вложенности, чтобы документ нормально показывался везде без танцев с бубном. Вы, конечно, можете сказать, что можно вставлять звуковые файлы в контейнере при сохранении docx, а я, в свою очередь, скажу вам, что это грязное извращение. Во-первых, из-за самой идеи контейнера, а во-вторых, из-за того, что эти контейнеры отправят своими габаритами к чёртовой бабушке всю разметку документа. Даже если пользователь делал её нормально, а не пробелами и абзацами, что, надо признать, случается редко. Каких функций вам не хватает? Расскажите, пожалуйста, каких удобных вещей для текстового редактора вам не хватает: есть ли что-то, что вы делаете через 10 диалоговых окон или внешние приложения? Есть ли вещи, которые прямо важны по стандартам корпоративного документооборота, а сделать их никак не выходит? Есть ли что-то, с чем вам просто жилось бы удобнее? Я не могу обещать, что мы их сразу же запилим в следующем релизе, но в бэклог положим. С одной стороны, конечно, мы не так богаты на фичи, как пакеты, заставшие 386-е процессоры, но с другой — у нас чистая хорошая архитектура, поэтому реализовать быстро можем много чего. \",\n 'Сегодня мы с радостью представляем возможность «горячей перезагрузки» для .NET, которая уже доступна в Visual Studio 2019 16.11 (предварительная версия 1), а также в .NET 6 (предварительная версия 4) через инструмент командной строки dotnet watch. В этой статье мы расскажем, что такое горячая перезагрузка .NET, как приступить к ее использованию, как мы планируем развивать эту возможность в будущем, а также какие виды правок кода и языки поддерживаются на данный момент. Что такое «горячая перезагрузка .NET»?Горячая перезагрузка позволяет вносить изменения в исходный код приложения во время его выполнения без необходимости приостанавливать его вручную или создавать точку останова. Теперь прямо во время работы приложения можно внести в код изменение из числа тех, что поддерживаются для горячей перезагрузки, нажать кнопку «Применить изменения кода» в новом интерфейсе Visual Studio — и изменение будет сразу же применено.Горячая перезагрузка работает со множеством старых и новых типов проектов, включая WPF, Windows Forms, предварительные версии .NET MAUI, проекты кода программной части приложений ASP.NET Core, проекты консольных приложений, WinUI 3 (требуется управляемый отладчик) и многие другие. В сущности, новая возможность работает с любыми проектами на базе среды выполнения .NET Framework или CoreCLR.Мы стремились сделать горячую перезагрузку доступной независимо от того, как вы предпочитаете запускать свои приложения. Представленную сегодня версию можно использовать в отладчике Visual Studio, с которым она полностью интегрирована, а также через командную строку (dotnet watch). В следующих выпусках появятся и другие варианты. Начало работыЧтобы приступить к работе, вам потребуется либо последний выпуск предварительной версии Visual Studio, либо .NET 6 (предварительная версия 4). Ниже описано, как это сделать.Visual StudioИспользование горячей перезагрузки в Visual Studio при работе с отладчиком:Скачайте и установите Visual Studio 2019 16.11 (предварительная версия 1).Откройте проект поддерживаемого типа, например приложение WPF.Запустите приложение с подключенным отладчиком клавишей F5 (удостоверьтесь, что параметр «Разрешить отладку машинного кода» в настройках/профиле запуска отладчика отключен).Откройте файл с кодом на C#, который может многократно запускаться по действию из пользовательского интерфейса выполняемого приложения (код программной части кнопки, команда ViewModel и т. п.) или по таймеру через некий интервал. Внесите в код какое-нибудь изменение.Примените изменения кода с помощью новой кнопки Применить изменения кода (ALT + F10) на панели инструментов Visual Studio (рядом с кнопкой Продолжить). Сохранять файлы при использовании Visual Studio не нужно — можно быстро внести в код изменение и двигаться дальше.Если внесенное изменение поддерживается, обновленная логика будет применена к запущенному приложению, и вы увидите изменения в его работе при следующем выполнении обновленного кода (по действию или при выполнении активирующего условия, например по таймеру).Прочие функции отладчика — точки останова, «Изменить и продолжить», «Горячая перезагрузка XAML» и др. — будут доступны, как и раньше. Все привычные возможности будут полноценно работать параллельно с горячей перезагрузкой .NET. Если у вас что-то не работает, сообщите нам об этом!Интерфейс командной строки (CLI)Использование горячей перезагрузки из командной строки при запуске приложения с помощью dotnet watch:Установите .NET 6 (предварительная версия 4).Измените имеющийся проект ASP.NET Core, указав в качестве целевой версии платформы .NET 6.Добавьте свойство \"hotReloadProfile\": \"aspnetcore\" в профиль запуска приложения (launchSettings.json).Пример файла Properties/launchSettings.json:{\\n  \"profiles\": {\\n    \"dotnet\": {\\n      \"commandName\": \"Project\",\\n      \"hotReloadProfile\": \"aspnetcore\"\\n    }\\n  }\\n}Запустите проект с помощью команды dotnet watch и убедитесь, что в выводе указано, что горячая перезагрузка активирована.Внесите в управляемый исходный код приложения поддерживаемое изменение и сохраните файл, чтобы применить это изменение.Как и в Visual Studio, с этого момента начнет применяться новая логика: при следующем выполнении обновленного кода вы увидите изменения в работе приложения.Этот же подход можно использовать с проектами Blazor WebAssembly: следует изменить профиль горячей перезагрузки blazorwasm и далее действовать, как описано выше. Можно попробовать его даже с Windows Forms и другими типами проектов на платформе CoreCLR: для этого вручную добавьте в папку Properties файл с именем launchSettings.json и тем же содержимым, что в предыдущем примере.Разработка новой возможности продолжается, и в будущем при использовании команды dotnet watch горячая перезагрузка будет работать для всех типов приложений .NET Core без файлов launchSettings.json, но пока приходится мириться с таким ограничением. Полноценный выпуск ожидается в Visual Studio 2022 и .NET 6Сегодняшний выпуск — лишь предварительная, неполная версия нашей концепции горячей перезагрузки для разработчиков .NET. Хотя некоторые возможности доступны уже в ранних предварительных выпусках .NET 6 и Visual Studio 2019, в полной мере мы планируем реализовать этот функционал в .NET 6 (и последующих версиях .NET) и Visual Studio 2022. Это будет набор фреймворков и инструментов, обладающих максимально полными и оптимизированными возможностями.Примеры ниже позволяют составить представление о том, какие возможности мы планируем реализовать в будущих предварительных выпусках и окончательной версии:.NET Multi-platform App UI (.NET MAUI). .NET 6 (предварительная версия 4) позволит разработчикам, создающим приложения .NET MAUI, использовать горячую перезагрузку .NET с проектами для WinUI 3. В будущих выпусках мы добавим поддержку горячей перезагрузки .NET при разработке под iOS, Android и Mac Catalyst.Страницы Razor. В будущих выпусках горячая перезагрузка и команда «Изменить и продолжить» будут поддерживаться при редактировании страниц Razor для веб-сайтов и приложений Blazor (если в качестве целевой платформы указана версия .NET 6 и выше).Работа в Visual Studio без отладчика. В будущем выпуске Visual Studio 2022 мы планируем добавить поддержку горячей перезагрузки без отладчика. Это значит, что даже при запуске приложения сочетанием клавиш CTRL + F5 разработчики смогут вносить изменения в выполняемое приложение.Расширение набора поддерживаемых изменений. Вместе с несколькими рабочими группами мы работаем над тем, чтобы в будущих выпусках Visual Studio 2022 и .NET 6+ сократить список правок кода, которые не поддерживаются во время выполнения.Оптимизация фреймворков для работы с горячей перезагрузкой. В .NET 6 мы исследуем возможности оптимизации некоторых фреймворков для улучшения поддержки горячей перезагрузки. Такие улучшения планируются для ASP.NET Core, .NET MAUI и других фреймворков. За счет оптимизации разного рода горячая перезагрузка будет эффективнее работать в разных ситуациях.Таковы планы на данный момент. Они не окончательные: мы будем прислушиваться к отзывам пользователей и ориентироваться на график выпусков. Поддерживаемые и неподдерживаемые изменения и языкиНезависимо от того, как вы используете горячую перезагрузку .NET, следует помнить, что некоторые изменения кода не поддерживаются во время выполнения приложения. В этих случаях будет выводиться диалоговое окно «Грубая редакция», предлагающее перезапустить приложение для применения изменений. Работа над новой возможностью продолжается, и подробное документирование поддерживаемых изменений еще не завершено. Пока что вы можете обратиться к существующему списку правок кода, которые поддерживаются функцией «Изменить и продолжить». Поскольку горячая перезагрузка работает на базе именно этой функции, это поможет лучше понять принцип ее работы. Подробные сведения: документация по функции «Изменить и продолжить».Кроме того, хотя в примерах выше указан язык C#, в различных сценариях работы с отладчиком Visual Studio поддерживается и Visual Basic. F# в настоящее время не поддерживается в .NET 6, но мы планируем внедрить его поддержку в будущих выпусках, опираясь на отзывы пользователей. Нам важны ваши отзывыКонечно, в этой ранней предварительной версии будут ошибки. Иногда при попытке применить изменение ничего не будет происходить, иногда возможно аварийное завершение приложения и т. п. Если вы столкнетесь с какими-либо проблемами, сообщите нам о них — это не займет много времени. Ваша поддержка поможет нам эффективно устранить критические проблемы и определить приоритеты для дальнейшей работы.Для направления отзывов используйте средства обратной связи Visual Studio.Материал подготовлен в рамках курса \"C# Developer. Professional\". Если вас интересует развитие в C# разработке с нуля до Pro, предлагаем узнать про специализацию.Также приглашаем всех желающих на открытый урок «Управление конфигурациями микросервисов». На занятии обсудим один из подходов, используемых в реальных high-load проектах. РЕГИСТРАЦИЯ ',\n ' \\r\\nНа днях полку одноплатников прибыло: китайская компания выпустила модель Radxa ROCK 3A размером с кредитку. Основа одноплатника — четырехъядерный Rockchip RK3568 ARM Cortex-A55 c частотой работы ядра 2 ГГц. Графика здесь — Mali-G52. Кроме того, есть слот для накопителей M.2 для NVMe SSD.\\n\\r\\nПо словам разработчиков, девайс может на равных конкурировать с последними версиями Raspberry Pi. Подробности о нем — под катом.\\n \\nЧто это за одноплатник?\\r\\nЧто касается процессора, то он позиционируется как чип для встраиваемых систем. И да, сам одноплатник как раз встраиваемая система — использовать его можно для разных целей в самых разных отраслях. \\n\\r\\nВстроенной памяти здесь нет, так что, если она нужна, покупатель сам может установить eMMC-чип либо использовать карту памяти формата microSD. Можно работать и с SSD PCI-E 3.0 NVMe, о чем уже говорилось выше.\\n\\r\\nОЗУ здесь есть, что логично, а в качестве памяти используется LPDDR4X-3200. От емкости модуля зависит и цена одноплатника — версии отличаются друг от друга лишь объемом ОЗУ.\\n\\nРазъемы и интерфейсы\\r\\nВажный момент — на плате сразу два слота M.2. Но для SSD предназначен лишь один из них, тот, что находится с нижней стороны платы. А вот второй добавлен для того, чтобы пользователь мог установить беспроводный модуль связи Wi-Fi/Bluetooth. Насколько известно, производитель не ограничил поддержку разных модулей — устанавливать можно плату, которая есть в наличии у пользователя. Поддерживается даже Wi-Fi 6 (802.11ax).\\n\\n \\r\\nЗапитывается плата через USB-C, что, в целом, обычная ситуация для устройств такого типа. Такой порт только один — подключение любых других устройств производится через USB-A. Всего разработчики предусмотрели два USB 3.0 и два USB 2.0. ПК в состоянии работать и с SATA-накопителями, но для этого нужен специфический кабель, который подключается сразу к двум портам USB 3.0.\\n\\r\\nЕсть и Ethernet, так что, если беспроводный модуль связи не требуется, его можно не подключать. Более того, есть возможность подключить монитор через полноценный разъем HDMI 2.0.\\n\\n\\nПроцессор: Rockchip RK3568\\nОЗУ: 2 ГБ, 4 ГБ или 8 ГБ (модуль LPDDR4-3200 с частотой 1560 МГц)\\nПамять: подключаемый eMMC модуль, M.2 для PCIe 3.0 x2 NVMe SSD снизу платы), кардридер microSD\\nUSB-C: 1 x USB-C для подключения питания\\nUSB-A порты: 2 x USB 3.0 (1 x OTG и 1 x Host) плюс 2 x USB 2.0\\nСеть: Gigabit Ethernet с поддержкой PoE, плюс M.2 E для PCIe 2.0 x1/SDIO/UARTс поддержкой Wi-Fi модулей\\nВывод изображения: HDMI 2.0 и MIPI DSI\\nДополнительные интерфейсы: 40-pin GPIO и MIPI-CSI коннектор для камеры\\nРазмер: 85 мм x 54 мм\\n \\nПрограммное обеспечение \\r\\nПлата рассчитана на работу лишь с Debian 10, который умеет работать с Rockchip, но одноплатник сможет работать и на других ОС семейства Linux. Возможно, найдутся умельцы, которые адаптируют и Windows 11 — как известно, эта операционная система поддерживает ARM-процессоры, но не все подряд модели, а лишь весьма ограниченное их количество. \\n\\n\\n\\nЧто еще? \\r\\nКитайская компания Radxa — далеко не новичок на рынке одноплатников. С 2013 года, момента своего основания, компания выпустила такие устройства, как Rock Pi N10, Rock Pi E, Rock Pi S и Rock Pi 4. Насколько можно понять, большинство своих девайсов китайцы стараются противопоставить устройствам от Raspberry Foundation.\\n\\n \\r\\nОб одном из таких одноплатников мы уже писали. Это Rock Pi 4 Plus на шестиядерном ARM-процессоре Rockchip RK3399 OP1. Его китайцы поставляют на рынок в качестве альтернативы Raspberry Pi 4 Model B, но с собственной ОС и накопителем.\\n\\r\\nОС этого накопителя заслуживает отдельного упоминания, поскольку дистрибутив Twister OS Armbian имеет сразу несколько графических оболочек, имитирующих интерфейс Windows XP. Windows 98, Windows 10 или macOS.\\n\\r\\nРазработчики предусмотрели 4 разных конфигурации:\\n\\n\\n Model A 2GB LPDDR4 + 16GB eMMC: 49$\\n Model A 4GB LPDDR4 + 32GB eMMC: 65$\\n Model B 2GB LPDDR4 + 16GB eMMC: 59$\\n Model B 4GB LPDDR4 + 32GB eMMC: 75$\\n \\r\\nНу и есть еще Radxa Zero — это чуть усиленный вариант Raspberry Pi Zero. Он на $5 дороже конкурента, но зато имеет 4-ядерный процессор с частотой работы ядра 2 ГГц и 4 ГБ оперативной памяти.\\n\\n',\n 'Android спроектирован так, чтобы использование приложения пользователем было максимально интуитивным. Например, пользователи приложения могут повернуть экран, ответить на уведомление или переключиться на другое приложение, и после этих манипуляций они все так же должны иметь возможность продолжить использовать приложение без каких-либо проблем.Чтобы обеспечить такое взаимодействие с пользователем, вы должны знать, как управлять жизненными циклами компонентов. Компонентом может быть Activity, Fragment, Service, класс Application и даже сам процесс приложения. Компонент имеет жизненный цикл, в течение которого он проходит через различные состояния. Всякий раз, когда происходит переход, система уведомляет вас об этом при помощи методов жизненного цикла.  Чтобы нам было легче объяснить, как работает жизненный цикл в Android, мы определили несколько сценариев (примеров из жизни), которые сгруппированы по компонентам:Часть 1: Activity — ЖЦ одного активити (этот пост)Часть 2: Несколько Activity — навигация и бекстекЧасть 3: Fragment-ы — ЖЦ Fragment-ов и ActivityЧасть 4: ViewModel-и, прозрачные Activity and Launch ModeДиаграммы также доступны в виде шпаргалки в формате PDF для краткого ознакомления.Примечание: эти диаграммы соответствуют поведению в Android P/Jetpack 1.0.Следующие сценарии демонстрируют поведение компонентов по умолчанию, если не указано иное.Если вы обнаружили ошибки в статье или считаете, что не хватает чего-то важного, напишите об этом в комментариях.Часть 1: ActivityОдно Aсtivity - Сценарий 1. Приложение завершено и перезапущеноБудет вызван, если:Пользователь нажимает кнопку Назад илиВызван метод Activity.finish()Самый простой сценарий показывает, что происходит, когда приложение с одним активити запускается, завершается и перезапускается пользователем:Управление состояниемonSaveInstanceState не вызывается (поскольку активити завершено, вам не нужно сохранять состояние)  onCreate не имеет Bundle при повторном открытии приложения, потому что активити было завершено и состояние не нужно восстанавливать.  Одно Aсtivity - Сценарий 2. Пользователь уходитБудет вызван, если:Пользователь нажимает кнопку \"Домой\"Пользователь переключается на другое приложение (через меню «Все приложения», из уведомления, при принятии звонка и т. д.)В этом случае система остановит активити, но не завершит его сразу.Управление состояниемКогда ваше активити переходит в состояние Stopped, система использует onSaveInstanceState для сохранения состояния приложения на тот случай, если впоследствии система завершит процесс приложения (см. ниже).  Предполагая, что процесс не был убит, экземпляр активити сохраняется в памяти, сохраняя все состояние. Когда активити возвращается на передний план, вам не нужно повторно инициализировать компоненты, которые были созданы ранее.Одно Aсtivity - Сценарий 3. Изменение кофигурации  Будет вызван, если:Изменена конфигурация, такие как поворот экранаПользователь изменяил размер окна в многооконном режимеУправление состояниемИзменения конфигурации, такие как поворот или изменение размера окна, должны позволить пользователям продолжить работу с того места, где они остановились.  Активити полностью уничтожено, но состояние сохраняется и восстанавливается при создании нового экземпляра.Bundle в onCreate тот же самый, что и в onRestoreInstanceState.  Одно Aсtivity - Сценарий 4. Приложение приостановлено системой Будет вызван, если:Включён многооконный режим (API 24+) и потерян фокусДругое приложение частично покрывает работающее приложение: диалоговое окно покупки (in-app purchases), диалоговое окно получения разрешения (Runtime Permission), стороннее диалоговое авторизации и т. д.Появится окно выбора приложения (при обработке неявного интента), например диалоговое окно шейринга.Этот сценарий не применим к:Диалогам в том же приложении. Отображение AlertDialog или DialogFragment не приостанавливает базовое активити.  Уведомлениям. Пользователь, получающий новое уведомление или открывающий панель уведомлений, не приостанавливает текущее активити.  ',\n 'Платформы для роботизированной автоматизации имеют широкий спектр возможностей и позволяют использовать множество готовых действий без программирования. Однако, часто у бизнес-пользователей возникает потребность в создании своих новых активностей без привлечения разработчиков. В этом посте рассказываем, как программисты могут решать эту проблему. Статья написана при поддержке технического эксперта UiPath: Валентина Драздова.В последние несколько лет все больше компаний переходит от классической автоматизации к роботизации с использованием RPA-технологий. Современные RPA-платформы предоставляют большой набор готовых действий для создания роботов. Они дают возможность эмулировать действия пользователя путем взаимодействия с разными системами через графический интерфейс, программировать в этом случае не нужно. Однако, всех потребностей бизнеса этими действиями не покроешь, поэтому платформы дают  возможность пользователю написать часть кода самому непосредственно в схеме робота. Этого вполне достаточно для вызова других библиотек и программ, но такой подход неудобен, когда роботизацией хотят заниматься не только программисты, а люди из бизнеса — citizen developers). В этой статье мы на примере UiPath расскажем, как программисты могут создавать свои активности для RPA-платформы UiPath так, чтобы с ними могли работать бизнес-пользователи при роботизации процессов, не привлекая профессиональных разработчиков.Мы решили рассмотреть создание пользовательских активностей в UiPath на примере с универсальной библиотекой регистрации посещений выставок или бизнес-центров. Она предоставляет функционал работы с реестром посещений, позволяет регистрировать приходящие лица (метод AddVisitor) и получать таблицу посетителей (метод GetVisitors). Список посетителей формируется в экземпляре класса Registrator, он существует от момента создания экземпляра класса до завершения работы с ним. Готовим рабочую средуРассмотрим разработку активностей в Microsoft Visual Studio с использованием специального расширения «UiPath Activity Creator», которое можно бесплатно установить из Visual Studio Marketplace:После установки расширения вам необходимо закрыть студию и принять лицензионное соглашение. Создаем решение и знакомимся со структуройПосле установки расширения вам станет доступен новый шаблон проекта – «UiPath Standard Activity Project»Создайте свой проект используя этот шаблон. Наш проект мы назвали «UiPath.Habr.Visitors». В результате было создано решение из пяти проектов:UiPath.Shared.Activities – проект с общим кодом от UiPath, предназначен для облегчения разработки активностей (не изменяйте код этого проекта).UiPath.Shared.Activities.Design – Проект с общим кодом от UiPath, предназначен для облегчения разработки визуальной части активностей, которая будет отображаться в UiPath Studio (не изменяйте код этого проекта)ИМЯ_ВАШЕГО_ПРОЕКТА – Базовый проект решения, предназначен для бизнес-логики ваших активностей.ИМЯ_ВАШЕГО_ПРОЕКТА.Activities – Проект, содержащий классы с активностями. Именно в этом проекте должно происходить получение аргументов, вызов бизнес-логики и возврат аргументов.ИМЯ_ВАШЕГО_ПРОЕКТА.Activities.Design – Проект, содержащий визуальные компоненты активностей (.XAML), которые будут отображаться в UiPath Studio. Создаем активностиДля того, чтобы создать свою активность – необходимо добавить класс активности в проект ИМЯ_ВАШЕГО_ПРОЕКТА.Activities и визуальный компонент активности в проект ИМЯ_ВАШЕГО_ПРОЕКТА.Activities.DesignСамый простой способ сделать это – воспользоваться мастером по созданию активностей, который доступен в меню «Расширения -> UiPath -> Add Activities»После открытия мастер предложит создать активности вручную или импортировать их из JSON-файла. Мы используем первый вариант.В открывшемся окне вы можете создать разные активности при помощи кнопки «+». Каждая активность содержит следующие параметры:Name – имя активности (рекомендуем писать на английском), которое будет отображено в списке активностей. Description – описание активности, которое будет появляться во всплывающем окнеType – тип активности: Simple – обычная активность (одно действие).Scope – активность с контейнером для других активностей, которые могут использовать данные из скоуп-активности.Properties – набор аргументов активности (поговорим об этом ниже).Timeout – переключатель «имеется ли у действия время ожидания?». Если включить этот параметр – можно будет настраивать время, через которое активность будет останавливаться, даже если она еще не завершилась.Icon – иконка для вашей активности.АргументыУ каждой активности могут быть свои аргументы со следующими свойствами:Category – Раздел в панели «Свойства», в котором будет размещаться данный аргументName – Имя аргументаDescription – Описание аргумента, которое будет появляться во всплывающем окнеDirection – Направление передачи :In – Входной аргумент (передача информации от робота в активность)Out – Выходной аргумент (или результат - передача информации от активности к роботу) InOut – Входной-Выходной аргумент (передача информации от робота в активность и обратно)None – Не использовать динамические значения (передаваемые через переменные), настройка вручную в UiPath Studio, рекомендуется использовать для Boolean (будет отображен в виде чекбокса), Enum (будет отображен в виде фиксированного списка) Type – Тип данных для этого аргумента (любой тип данных .NET, в том числе из подключенных библиотек) Options Required – является ли аргумент обязательным при настройке робота в  UiPath StudioАргументы активности Add Visitor (регистрации приходящего лица) Разбираемся, как устроены активностиПосле того, как вы создали свои первые активности при помощи специального расширения, мы можем рассмотреть подробнее из чего они состоят. Классы активностей располагаются в проекте ИМЯ_ВАШЕГО_ПРОЕКТА.Activities, в подпапке «Activities»:Классы обычных (Simple) и скоуп (Scope) активностей похожи, но немного отличаются, поэтому рассмотрим их по отдельности. Класс активности типа SimpleКласс состоит из трех регионов:Регион PropertiesСодержит определение входящих и исходящих аргументов активности. Обратите внимание, что у каждого параметра имеются параметры с локализацией:LocalizedCategory – имя строки в ресурсах проекта, в которой содержится название категории (группы), которая будет отображена в списке аргументов в окне «Параметры»LocalizedDisplayName – имя строки в ресурсах, в которой содержится название аргументаLocalizedDescription – имя строки в ресурсах, в которой содержится описание аргумента, которое будет появляться при наведении мышкой.Также вы можете заметить, что все входные параметры используют тип InArgument, а выходные – OutArgument.Когда используется данный тип – в UiPath Studio будет предложено ввести в качестве параметра переменную типа, обрамленного в угловые скобки < >.В случае, если вы хотите, чтобы пользователи вашей активности имели возможность делать выбор из списка готовых значений (без возможности использовать значения переменных), то вместо InArgument<bool> вам надо использовать просто тип данных bool . Если вы хотите предоставить пользователю список из ограниченного списка значений – воспользуйтесь перечислением enum.Регион ConstructorsРегион содержит конструктор класса. Если вам необходимо выполнять какие-либо действия до непосредственного выполнения активности – вы можете сделать их здесь.Регион Protected MethodsРегион содержит методы, отвечающие за работу активностиМетод CacheMetadata предоставляет возможность провести проверку корректности использованных параметров на этапе проектирования роботаМетод ExecuteAsync содержит код, который выполняется непосредственно при выполнении активности роботом. Условно разбит на три части:Часть Object Container и Inputs –происходит загрузка контейнера объектов (что позволяет получить данные из других активностей, в частности из Scope), а также входных параметров. Все параметры, использующие тип данных InArgument можно получить с использованием функции Get, указав в качестве параметра переменную с контекстом (context), например:var plannedNumberOfVisitors = PlannedNumberOfVisitors.Get(context)В случае, если вы используете «обычные» типы данных – вы можете обращаться к параметрам напрямую.Часть Add execution logic HERE – предназначена для вашего кода, где вы будете писать свой бизнес-код (или вызывать другие библиотеки).Часть Outputs – предназначена для вывода результатов из активности, выполняется с использованием анонимной структуры, где указывается параметр типа OutArgument с вызовом функции Set, где первый параметр – контекст (context), второй – значение. Например:return (ctx) => { Table.Set(ctx, resultDataTable); } Пример метода ExecuteAsync активности Get VisitorsКласс активности типа ScopeРассмотрим основные отличия данного типа классаРегион PropertiesПомимо входящих и исходящих аргументов содержит:Body — коллекция дочерних активностей,ParentContainerPropertyTag - тэг родительской активности, желательно назвать уникально по своему_objectContainer – контейнер объектов для дочерних активностейРегион Protected MethodsМетод ExecuteAsync в отличие от метода из класса типа Simple, возвращает не содержимое для исходящих аргументов, а запускает выполнение дочерних активностейРегион EventsМетод OnFaulted выполняется в случае, если одна из дочерних активностей вызвала ошибку.Метод OnCompleted выполняется в случае, если все дочерние активности завершились успешноРегион CleanupМетод Cleanup предназначен для освобождения памяти и занятых ресурсов.Связываем Scope и Simple активностиЧтобы работать с данными, которые были  инициализированы в рамках Scope-активности, вам необходимо сначала передать эти данные в контейнер объектов в Scope-активности. Сделать это можно в методе ExecuteAsync перед запуском дочерних активностей. Для примера, если вы хотите передать всю Scope-активность, вам достаточно написать следующий код:_objectContainer.Add(this);Чтобы получить нужный объект в Simple-активности, вам необходимо получить объект из контейнера объектов в методе ExecuteAsync после его инициализации. Для примера получим родительскую Scope-активность:var parentRegistrator = objectContainer.Get<Registrator>();Формирование Nuget-пакетовЕсли вы знаете как пользоваться консольной утилитой nuget.exe, вы можете пропустить этот раздел и сформировать пакет самостоятельно для проекта «ИМЯ_ВАШЕГО_ПРОЕКТА.Activities.Design». Если у вас нет больших компетенций в формировании Nuget-пакетов, то вы можете воспользоваться очень простым способом. Для этого нажмите правой кнопкой мыши по проекту «ИМЯ_ВАШЕГО_ПРОЕКТА.Activities.Design» в обозревателе проектов и выберите пункт «Опубликовать…» (Publish…)По умолчанию в созданном из шаблона проекте нет профилей публикации, поэтому Visual Studio предложит вам создать собственный. Выберите профиль «Папка» (Folder) и укажите директорию на вашем компьютере, где у вас располагается локальный источник пакетов (если его еще нет – мы создадим его в следующем разделе, пока просто создайте директорию). В рамках примера для данной статьи будет использована директория «C:\\\\MyNuGet»После создания профиля вы сможете публиковать ваш NuGet-пакет, а также управлять настройками публикации:При выполнении публикации будет осуществлена сборка решения в соответствии с указанными параметрами, после чего собранные библиотеки будет запакованы в NuGet-пакет и сохранены по указанному адресу.Если вы все сделали правильно – в целевой директории появится пакет с версией 0.1.0:Изменение версии пакетаЕсли вы внесете изменения в свою активность и соберете ее снова, то можете обнаружить, что вы снова получите пакет с версией 0.1.0. Так как версия пакета не поменялась – вы не сможете обновить активность через менеджер пакетов UiPath. Чтобы изменить номер версии – вам необходимо открыть в текстовом редакторе файл проекта «ИМЯ_ВАШЕГО_ПРОЕКТА.Activities.Design» (его расширение – csproj)Вам необходимо найти тэги «PackageVersion» и изменить версию с 0.1.0 на следующую.Добавление использованных библиотек в NuGet-пакетВ случае, если вы использовали сторонние библиотеки или подключили дополнительные проекты в ваше решение, то могли заметить, что их нет в NuGet-пакете. Если вам необходимо сделать так, чтобы эти библиотеки или проекты появились в NuGet-пакете, добавьте их так же в проект «ИМЯ_ВАШЕГО_ПРОЕКТА.Activities.Design», установите свойство «Копировать локально» в «Да» (Copy Local - Yes), после чего откройте его в текстовом редакторе, найдите строчку<_ReferenceCopyLocalPaths Include=\"@(ReferenceCopyLocalPaths-&gt;WithMetadataValue(\\'ReferenceSourceTarget\\', \\'ProjectReference\\')-&gt;WithMetadataValue(\\'PrivateAssets\\', \\'All\\'))\" />И оставьте только:<_ReferenceCopyLocalPaths Include=\"@(ReferenceCopyLocalPaths)\" />Использование в UiPathЧтобы использовать ваш NuGet-пакет в UiPath вам необходимо добавить его в источник пакетов. Когда вы будете распространять свои активности на всю компанию – рекомендуется добавить его в источник пакетов вашего оркестратора. Создание своего источника пакетовЕсли вы только разрабатываете и отлаживаете свои активности, то вам лучше воспользоваться локальным источником пакетов. Чтобы создать локальный источник пакетов вам необходимо в окне «Проект» нажать правой кнопкой мыши по разделу «Зависимости», выбрать пункт «Управление». В открывшемся окне перейдите на закладку «Настройки», справа над нижним списком будет кнопка «+» (плюс), нажмите ее, после чего в поле «Название» введите название для вашего локального источника (придумайте свое), а в поле «Источник» введите адрес директории с вашими пакетами (можете воспользоваться кнопкой «…» для простого выбора папки) и нажмите кнопку «Добавить».Установка пакетаПерейдите на вкладку своего источника, где вы сможете увидеть свой пакет. Выбрав пакет вы сможете увидеть все версии пакета, доступные в вашем источнике пакетов. Установите ваш пакет как любой другой пакет, доступный вам в UiPath:После установки пакета ваши активности будут доступны в общем списке активностей:Отладка ваших активностейИногда вам может потребоваться отладка ваших активностей непосредственно во время работы робота. Для этого вам необходимо поставить точку останова на вашей активности в UiPath Studio, запустите робота и дождитесь его остановки перед выполнением активности:Перейдите в Visual Studio, откройте класс, соответствующий вашей активности, поставьте точки останова в том месте метода ExecuteAsync, где вам требуется начать отладку, далее откройте меню «Отладка – Присоединиться к процессу» (Debug – Attach to process…)В открывшемся окне найдите «UiPath.Executor» в списке процессов и нажмите кнопку «Attach»После того, как Visual Studio запустит режим отладки, вернитесь в UiPath Studio и продолжите выполнение робота. После этого Visual Studio выполнит остановку исполнения кода именно в том месте, где вы поставили точку останова и вы сможете выполнить отладку.Если в Visual Studio открыт код более новой версии, чем находится в установленном в UiPath пакете, подключение может не удастся, либо процесс отладки будет выглядеть некорректным.   Если у вас не работает отладка – попробуйте при сборке NuGet-пакета выбрать конфигурацию Debug.Что еще можно сделать со своими активностямиВ этой статье мы рассказали, как можно быстро начать разрабатывать свои активности для UiPath, чтобы дать больше возможностей при роботизации вашим коллегам. Часть советов из этой статьи можно назвать «вредными», так как в некоторых моментах стоит внимательней подойти к настройке каких-то деталей. Вот несколько рекомендаций, которые помогут вам сделать ваши активности более качественными:Сделайте свои активности более привлекательными, в проекте «ИМЯ_ВАШЕГО_ПРОЕКТА.Activities.Design» можно найти XAML-файлы, изменяя которые можно сделать более удобный интерфейс взаимодействия с вашими активностями в UiPath Studio.Разберитесь с файлами ресурсов, обратите внимание, что по умолчанию все текстовые надписи сделаны на «нейтральном» языке. Приведите локализацию в порядок, как правило нейтральный язык должен содержать английский вариант, а русский вариант должен быть реализован с использованием локализованного файла ресурсов. Если вы не знаете как настраивать локализацию через файлы ресурсов — мы вам расскажем об этом в одной из следующих статей.Если вы сделали универсальную активность, которая будет полезна не только лично для вас или вашей компании, но и для мирового сообщества, то вы можете опубликовать ее в магазине UiPath (и даже заработать на этом денег). Подробнее про публикацию своих активностей в магазине UiPath мы расскажем в следующих статьях.Позаботьтесь о модульном тестировании ваших активностей, а также не забудьте опубликовать исходный код в репозитории вашей компании.',\n 'В апреле 2021 утилита Convert2RHEL стала официально поддерживаемым компонентом Red Hat Enterprise Linux (RHEL). Эта утилита позволяет взять машину с CentOS или Oracle Linux и преобразовать ее в систему RHEL с сохранением настроек и приложений. Чтобы еще больше упростить жизнь системным администраторам, Convert2RHEL также помогает заранее выявить проблемы совместимости, которые могут возникнуть при такой миграции. Сегодня мы расскажем, как провести миграцию с помощью утилиты Convert2RHEL и средства системного управления Red Hat Satellite, которое входит в add-on к Red Hat Enterprise Linux Red Hat Smart Management.Отметим, что вместо того, чтобы проводить такую миграцию самостоятельно, можно обратиться к специалистам Red Hat Consulting, которые помогут ускорить процесс и сократить риски при переносе критических рабочих нагрузок на RHEL. Если же вы решили сделать все своими руками, то начать стоит с изучения документации и пилотной конвертации, и лишь затем приступать к массовой миграции.Прежде всего, проверьте, подходит ли вам Convert2RHEL, поскольку она поддерживает преобразование с дистрибутивов CentOS Linux и Oracle Linux версий 7 или 8 на архитектуре x86_64, и только по следующим траекториям:ОткудаКудаCentOS Linux 7RHEL 7CentOS Linux 8RHEL 8Oracle Linux 7RHEL 7Oracle Linux 8RHEL 8В этом посте мы разберем преобразование CentOS Linux 8 в RHEL 8. Сам процесс сводится к замене пакетов исходной системы на RHEL-аналоги.Понятно, что для этого Convert2RHEL должна иметь доступ к репозиторию той версии RHEL, на которую выполняется переход. Различные варианты того, как это сделать, разбираются в документации, например, это может быть непосредственный доступ к Red Hat Content Delivery Network через Subscription Manager, либо работа через Red Hat Satellite, либо использование вашего собственного репозитория. Если вы уже применяете Satellite и хотите включить в нее конвертируемые системы, доступ к репозиториям рекомендуется организовать через Satellite.Настройка Satellite Первый шаг – убедиться, что в среде Satellite настроены все необходимые ресурсы. Здесь мы разбираем перенос на RHEL 8, но для RHEL 7 все очень схоже, подробнее см. документацию.Скорее всего, Satellite у вас уже настроен нужным образом, и останется только проверить конфигурацию. А именно, убедиться, что следующие репозитории включены и синхронизированы:rhel-8-for-x86_64-appstream-rpms  - Red Hat Enterprise Linux 8 for x86_64 - AppStream (RPMs)rhel-8-for-x86_64-baseos-rpms - Red Hat Enterprise Linux 8 for x86_64 - BaseOS (RPMs)Репозиторий Satellite Tools для соответствующих версий RHEL 8 (например,Red Hat Satellite Tools 6.9 for RHEL 8 x86_64).Подробнее о том, как включить и синхронизировать репозитории в Satellite, см. документацию.Далее, надо проверить, что эти репозитории оформлены как Content View, и этот Content View опубликован и распространяется по мере необходимости. В нашем примере мы используем Content View с именем RHEL 8. Подробнее о Content View и о том, как всё сделать, можно прочитать в документации.И наконец, надо убедиться, что был создан ключ активации, настроенный на использование нашего Content View. Кроме того, к ключу активации должны быть добавлены соответствующие подписки RHEL, поскольку для конвертируемой системы потребуется действующая подписка. В нашем примере ключ активации называется RHEL8. Подробнее обо всем этом можно узнать из документации.Конвертация системы в RHELИтак, Satellite настроен, и можно переходить к конвертации. Первый шаг – на всякий случай создать полную резервную копию исходного узла CentOS Linux 8, чтобы быстро откатить, если что-то пойдет не так.Затем проверяем содержимое /etc/centos-release, уточняя версию используемой CentOS:# cat /etc/centos-release\\nCentOS Linux release 8.3.2011Далее скачиваем ключ Red Hat RPM Signing GPG key:# curl -o /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release https://www.redhat.com/security/data/fd431d51.txtТеперь настраиваем YUM-репозиторий Convert2RHEL:# curl -o /etc/yum.repos.d/convert2rhel.repo https://ftp.redhat.com/redhat/convert2rhel/8/convert2rhel.repoЗатем устанавливаем Convert2RHEL следующей командой:# yum -y install convert2rhelС сервера Satellite понадобится скачать пакет katello-ca-consumer-latest (в строке ниже замените выделенное курсивом имя сервера Satellite на свое):# curl --insecure --output /usr/share/convert2rhel/subscription-manager/katello-ca-consumer-latest.noarch.rpm https://satellite.example.com/pub/katello-ca-consumer-latest.noarch.rpmПрежде чем конвертировать CentOS Linux, ее надо обновить:# yum updateУстановите доступные обновления и, если потребуется, перезагрузите систему.Всё, мы готовы запускать утилиту convert2rhel. Запустим ее с опцией --org, чтобы задать имя организации, которое будет использоваться при регистрации этой системы в Satellite (у нас организация называется test). Кроме того, через параметр --activationkey зададим ключ Satellite Activation Key, который будет использоваться для регистрации этой системы (у нас этот ключ называется RHEL8).# convert2rhel --org test --activationkey RHEL8Convert2RHEL начинает с того, что показывает лицензионное соглашение, а затем несколько минут собирает сведения о системе, включая вывод команды rpm -Va. Затем утилита выдает список пакетов, которые не подлежат конвертации и будут удалены, и просит подтвердить продолжение конвертации:Затем на экран выводится список пакетов, которые относятся к Subscription Manager и будут заменены, и предлагается подтвердить продолжение конвертации:После чего происходит установка пакетов Red Hat Subscription Manager, и система регистрируется на сервере Satellite.Затем отображается список пакетов, которые содержат файлы репозиториев и будут удалены, и предлагается подтвердить продолжение конвертации:И, наконец, появляется последний запрос на подтверждение конвертации, где утилита сообщает, что не сможет автоматически откатить изменения и, если что, это придется делать вручнуюПосле чего Convert2RHEL больше ничего не спрашивает и выполняет преобразование, время которого зависит от того, сколько пакетов установлено на исходной системе, от ее аппаратной производительности и от других факторов. На этой стадии выполняется несколько действий, включая переустановку пакетов CentOS Linux с заменой на пакеты из соответствующей версии RHEL:По завершении процесса Convert2RHEL напоминает о необходимости перезагрузить систему.Это можно сделать командой reboot:# rebootЕсли при конвертации возникли проблемы, обратитесь к лог-файлу /var/log/convert2rhel/convert2rhel.log, чтобы попытаться выяснить их причину.Что надо сделать после перезагрузки системыПервым делом – войти в систему и посмотреть файл /etc/redhat-release на предмет текущей версии Red Hat Enterprise Linux.# cat /etc/redhat-release\\nRed Hat Enterprise Linux release 8.3 (Ootpa)Также рекомендуется посмотреть список присутствующих в системе пакетов, которые не относятся к репозиториям RHEL. Это можно сделать так:# yum list extras --disablerepo=\"*\" --enablerepo=rhel-8-for-x86_64-baseos-rpms,rhel-8-for-x86_64-appstream-rpmsВ нашем примере три таких пакета: centos-gpg-keys, convert2rhel и katello-ca-consumer.katello-ca-consumer нужен для взаимодействия с Satellite, а вот centos-gpg-keys и convert2rhel можно удалить:# yum remove convert2rhel centos-gpg-keysКроме того, удалим файл convert2rhel.repo (репозиторий convert2rhel):# rm /etc/yum.repos.d/convert2rhel.repoСледующий шаг – включить репозиторий satellite-tools и установить пакет katello-host-tools, выполнив следующие команды (это команды для связки Satellite 6.9–RHEL 8, подкорректируйте их, если у вас другие версии Satellite и RHEL):# subscription-manager repos --enable=satellite-tools-6.9-for-rhel-8-x86_64-rpms\\n# yum -y install katello-host-toolsИ наконец последний, необязательный, шаг, который можно выполнить на узле – установить агента Insights и зарегистрировать систему в Insights, выполнив следующие команды:# yum install -y insights-client\\n# insights-client --registerЗатем надо залогиниться в веб-интерфейс Satellite и проверить, что наш узел правильно зарегистрирован. Для этого идем в Hosts -> Content Hosts и щелкаем систему, с которой мы работали. Проверим, что Content View задан правильно и Subscription Status имеет значение Fully entitled:Теперь посмотрим, как наша конвертированная система выглядит в Red Hat Insights. Это можно сделать либо на сайте Insights, либо в веб-интерфейсе Satellite (меню Insights и затем Inventory). Система проактивного управления Red Hat Insights for RHEL повышает операционную эффективность, улучшает управление рисками безопасности и входит в подписку RHEL. Расширенная аналитика Insights помогает выявлять и приоритизировать риски, связанные с операциями, безопасностью и бизнесом, а также позволяет вам мониторить соблюдение политик и понимать, как конфигурации меняются с течением времени, подробнее см. страницу продукта Red Hat Insights. ЗаключениеИтак, мы пошагово разобрали, как с помощью Convert2RHEL и Satellite преобразовать сервер CentOS Linux 8 в полностью поддерживаемую систему RHEL 8. Как видите, стандартизировать ИТ-среду, сократив номенклатуру операционных систем, стало гораздо проще. Выполнив такую конвертацию, вы получаете доступ к последним обновлениям RHEL, а также ко всем ее преимуществам в плане производительности, поддержки, управляемости и безопасности.Приступая к реализации любого масштабного проекта по конвертации, не забывайте о рассмотренных в этой статье инструментах, которые помогут добиться успеха: о Red Hat Satellite, которая упростит системное управление, и о Red Hat Insights, которая поможет проактивно выявлять и устранять широкий спектр потенциальных проблем.Дополнительную информацию по конвертации ОС можно найти в базе знаний Red Hat, в разделе технической документации. Вы также можете оставить заявку на проведение индивидуальной встречи со специалистами Red Hat, которые помогут спланировать миграцию.',\n 'Меня зовут Николай. Когда в 2009 году я защищал диссертацию по распознаванию речи, скептики мне говорили, что слишком поздно, так как Microsoft и Google уже “всё сделали”. Сейчас в SberDevices я обучаю модели распознавания речи, которые используются в семействе виртуальных ассистентов Салют и других банковских сервисах. Я расскажу, как обучил модель распознавания речи, используя Common Voice и недавно открытый датасет Golos. Ошибка распознавания составила от 3 до 11 % в зависимости от типа тестовой выборки, что очень неплохо для открытой модели.Не так давно наша команда подготовила и опубликовала общедоступный датасет Golos. Почему встал вопрос об обучении и публикации акустической модели QuartzNet? Во-первых, для того, чтобы узнать, какую точность достигает система распознавания речи при обучении на новом датасете. Во-вторых, обучение само по себе ресурсоёмкое, поэтому сообществу полезно иметь в открытом доступе предобученную модель на русском языке. Полная версия статьи опубликована на сайте arxiv.org и будет представлена на конференции INRETSPEECH2021. Описание данныхНа момент скачивания данных из Common Voice (CV) проверенная часть датасета, которая не относится к Test- и Dev-частям, составляла 100.48 часа. Эта часть использовалась в составе тренировочной выборки. Выборки Test и Dev длительностью по 13.33 и 12.44 часов соответственно использовались только для оценки качества в процессе обучения.Датасет Golos состоит из двух доменов – Crowd и Farfield. Тренировочная часть составляет 1227.4 часов, тестовая – 12.6 часов. Основной плюс его в том, что разметка данных в нём трижды проверенная очень качественная. Подробнее о структуре и способе формирования можно почитать в статье: “Golos — самый большой русскоязычный речевой датасет, размеченный вручную, теперь в открытом доступе”.Акустическая модельЗадача акустической модели – получать на вход аудиозапись и выдавать логарифм вероятностей символов для каждого временного фрейма записи. Пример такого выходного массива приведён на цветовой диаграмме, обычно его называют “логитами”.На цветовой диаграмме по оси Х – номер фрейма, каждый длинной 20 миллисекунд. По оси Y – 34 символа из алфавита, включая пробел и бланк (|). По оси Z – величина логарифма вероятности, отображённая цветовой шкалой. Красным цветом обозначены максимальные значения. По ним можно прочитать распознанное слово «привет». Здесь используется простой алгоритм декодирования логитов, основанный на функции argmax, обычно называемый “жадным” (greedy).В качестве акустической модели мы выбрали нейронную сеть с архитектурой QuartzNet15x5, так как в тот момент она была SOTA (State Of The Art) по скорости и качеству. Её архитектура состоит из последовательных свёрточных слоёв, как показано в таблице:БлокRKCSC11332561B15332563B25395123B35515123B45635123B55755123C21875121C31110241C411341В начале идёт блок С1, за которым следует группа из пяти блоков B2 - B5. Блоки в группе идентичны, каждый Bk состоит из повторяющихся R-раз свёрток размером K и числом каналов на выходе C. Каждый блок повторяется S раз. Далее идут три слоя (C2, C3, C4). Размерность на выходе сети равна количеству символов алфавита – 34. Полный конфиг доступен в открытой библиотеке NeMo, которую мы использовали для обучения: https://github.com/NVIDIA/NeMo. Стоит сказать, что кроме QuartzNet там много других интересных моделей, которые интересно попробовать, например, для синтеза речи, распознавания диктора и т.д.Акустическую модель мы обучали, перемешивая случайным образом объединённый тренировочный набор Golos + Common Voice. В процессе обучения его оценка проводилась на четырёх тестовых наборах: Golos Crowd и Farfield, Common Voice Dev и Test. Аугментация данных производилась при помощи маскирования частотных и временных полос, аналогично со SpecAugment, Dropout между слоями не применялся.  Для экономии объёма памяти и времени мы обучали модель в режиме mixed-precision. Использовались 16 видеокарт Nvidia Tesla v100 с размером батча 88 на одну видеокарту и аккумуляцией градиента по 10 батчам. Таким образом эффективный размер батча составил 16x88x10 = 14080. Ошибки распознавания WER (Word Error Rate) в ходе трёх экспериментов с разным числом шагов (10000, 20000, 50000) и применением трансферного обучения представлены на графиках:а) WER на выборке Golos Сrowd Test           \\tб) WER на выборке Golos Farfield Test\\nв) WER на выборке Common Voice Dev       \\tг) WER на выборке Common Voice Test\\nОжидаемо, что чем больше количество шагов в эксперименте, тем лучше обучилась модель и тем меньше ошибок распознавания.Трансферное обучениеВообще, трансферное обучение или Transfer Learning – очень популярная техника в глубоком обучении, например обработке изображений. В нашем случае в каталоге NGC доступна акустическая модель QuartzNet15x5, обученная на английском. В английском алфавите меньше символов, поэтому для переноса требуется заменить последний слой (голову). При этом можно не выбрасывать созвучные буквы, а переиспользовать их, сгенерировав веса только для новых букв. Подробно о том, как это можно делать, я рассказывал на конференции GTC21: https://youtu.be/Tavu52IrTFM.Сравним два эксперимента: обучение модели, начиная со случайных и с предобученных весов на английском языке. Для верности проведём их дважды, с разным числом шагов. Получается четыре эксперимента, которые соответствуют кривым четырёх цветов на рисунке:а) WER на выборке Golos Сrowd Test              \\tб) WER на выборке Golos Farfield Test·      Красный – старт с английских весов и 10000 шагов обучения.·      Фиолетовый – старт с английских весов и 20000 шагов обучения.·      Зелёный – старт со случайных весов и 10000 шагов обучения.·      Синий – старт со случайных весов и 20000 шагов обучения.В процессе обучения анализировалась величина WER на двух тестовых датасетах Golos: Сrowd и Farfield. В таблице ниже приведены значения итоговой величины WER: Golos Crowd Test WERGolos Farfield Test WERСлучайный старт, 10000 шагов28.84%52.82%«Английский» старт, 10000 шагов5.095%17.13%Случайный старт, 20000 шагов26.24%50.82%«Английский» старт, 20000 шагов4.629%15.95%Видно, что использование предобученных «английских» весов уменьшает ошибку распознавания русской акустической модели с 26.24% до 4.629% на наборе Crowd и с 50.82% до 15.95%  – на наборе Farfield.Языковая модельПри помощи алгоритма Beam Search вместе с языковой моделью можно ещё немного улучшить качество распознавания речи. Это происходит благодаря добавлению дополнительного знания о структуре языка, которого не хватило в обучающей выборке акустической модели. На вход алгоритма Beam Search поступают логиты, на выходе имеем текст.Мы сделали языковую модель, используя корпус Common Crawl на русском языке и KenLM Toolkit. Common Crawl – это коллекция текстовых данных, собранных из интернета автоматически, доступная для свободного скачивания. KenLM Toolkit позволяет создавать, обрабатывать и, самое главное, быстро применять N-граммные языковые модели.Всего мы сделали три разные 3-граммные языковые модели. Первая составлена из очищенных и предобработанных текстов корпуса Common Crawl. Предобработка заключалась в удалении знаков пунктуации и других лишних символов. Вторая модель построена исключительно на основе текстов транскрипций тренировочной части датасета Golos. Третья является комбинацией первых двух языковых моделей с равными весами (50 / 50).В таблице ниже приведены результаты инференса на тестовых подмножествах после применения языковых моделей алгоритмом Beam Search с параметрами: beam size=16, alpha=2, beta=1.5. Alpha – это вес (важность) N-граммной языковой модели. Beta – это штраф за длинную последовательность слов. Процент WER (Word Error Rate) для различных тестовых сетов:Decoder \\\\ Test setCrowd Golos TestFarfield Golos TestCommon Voice DevCommon Voice TestGreedy4.389 %14.949 %9.314 %11.278 %Beam Search + Common Crawl LM4.709 %12.503 %6.341 %7.976 %Beam Search + Golos LM3.548 %12.384 %--Beam Search + Common Crawl, Golos LM3.318 %11.488 %6.4 %8.06 %Из таблицы видно, как разные языковые модели влияют на ошибку распознавания (WER) на различных тестовых множествах. Самые лучшие результаты для Crowd (3.318 %) и Farfield (11.488 %) достигаются с языковой моделью, построенной на Common Crawl и Golos вместе. Пример скрипта для инференса доступен в репозитории Golos.Буду рад увидеть использование датасета Golos и предобученных моделей (акустической и языковой) в ваших экспериментах. Больше деталей можно найти в полном тексте статьи.',\n 'Привет, Хабр! Сегодня расскажем о том, почему и как мы решили мигрировать на Greenplum шестой версии с Greenplum пятой версии. Сразу скажем, что мы каждый день обрабатываем огромное количество данных — шутка ли, у одного из наших клиентов 80 млн пользователей, из которых каждый день активны до 90 тысяч из 178 стран.Естественно, все эти пользователи генерируют терабайты информации, поэтому обработка этих данных, построение аналитических отчетов и прочие задачи — тот еще челлендж. Сегодня поговорим об аналитике и решении аналитических задач при помощи Greenplum, с описанием перехода с одной версии на другую со всеми сопутствующими задачами, вроде перестройки инфраструктуры.Зачем нам вообще понадобился Greenplum? Как и любая более-менее крупная компания, мы многое анализируем. Результаты анализа нужны для оптимизации работы, оценки развития разных продуктов и других задач. В большинстве компаний паттерн построения аналитики строится на получении данных из продуктовых сервисов. Данные нужно не просто получить, а еще и загрузить в хранилище, где проводить анализ данных. Аналитические запросы весьма ресурсоемкие, так что всегда есть риск что-то \"уронить\" из-за неправильно составленного запроса.Мы решили отделить аналитические задачи от продуктовых сервисов, с чем отлично справляется Greenplum. Это аналитически распределенная OLAP-СУБД, к тому же, open-source, которая на рынке уже более 16 лет. Ее относительно просто адаптировать под собственную экосистему.Достоинством Greenplum является еще и то, что продукт использует MPP-архитектуру с полным отсутствием разделяемых данных. Так, есть основной хост, на котором размещается мастер — это инстанс Postgres для обращения пользователей. Кроме того, есть несколько хостов, на каждом из которых располагаются сегменты, тоже инстансы Postgres, но к ним может обращаться лишь мастер.Поскольку нет разделяемых данных — можно говорить о повышенной отказоустойчивости за счет создания зеркал как мастер-хоста, так и отдельных сегментов. Архитектура GreenplumЕще один огромный плюс Greenplum — совместимость с PostgreSQL. Нам Greenplum понравился как раз благодаря этому, плюс важны были еще несколько моментов:Кластер (MPP);Полиморфное хранилище;Собственный сетевой протокол взаимодействия;Разграничение ресурсов;Движок хранения для аналитической нагрузки (OLAP);AD-HOC аналитика;Построение отчетов в BI Tableau;Аудит качества данных платформы;Онлайн отчеты.Вот статистические данные по нашему Greenplum:886 таблиц (не включая партиции);15 ТБ сжатых данных, 40 TB несжатых данных без индексов;Индексов в системе на 7 ТБ;Сбор данных с 40 сервисов/микросервисов компании;В пике загрузка 27k событий в секунду.Какое-то время мы работали с пятой версией Greenplum с PostgreSQL 8.3., но затем решили мигрировать на новую версию. Миграция — почему и зачемУ нас было сразу несколько проблем, связанных с использованием устаревшей версии Greenplum:Постоянно увеличивалась емкость кластера, ведь платформа растет, количество клиентов увеличивается, растет и объем данных.В нашей сборке мы не использовали изоляцию ресурсов, такую как ресурсные очереди и ресурсные группы. То есть любой пользователь мог подключиться к нашему кластеру, выполнить любой запрос. Выполнение неоптимизированного запроса занимало большой объем ресурсов, что наносило ущерб работе других пользователей. При переезде на новый кластер стоит спланировать внедрение такого функционала.Еще один аргумент за миграцию — отсутствие ограничений на размер схемы/таблиц.И последнее — мы отставали от stable-сборки пятой версии Greenplum на целый год. Тому было несколько причин, но когда назрела необходимость апгрейда, мы решили не \"догонять\" stable-сборку предыдущей версии, а проапгрейдиться сразу на Greenplum 6. Схема переездаНам удалось совершить бесшовную миграцию, если так можно выразиться. Схема ее выглядит следующим образом:Сначала устанавливается новый кластер 6 версии Greenplum, выполняется миграция баз данных.Посредством Greenplum выполняется бэкап таблиц, а также восстановление данных в новый Greenplum.Запускаются дополнительные инстансы Kafka и запускается загрузка из kafka топиков в Staging таблицы Greenplum.Пользователей переключают на новый кластер.Здесь было важно предусмотреть один момент — у нас оставались пользователи, которые выполняли запросы в старой версии Greenplum. Для них мы оставили возможность переключения на старый кластер (временно). Если никаких проблем не возникало, аномалии не замечались, то старый кластер спустя определенное время отключался.Важнейшей задачей компании был сбор данных для аналитиков из разных источников, включая базы данных микросервисов, приложения, внутреннего транспорта и других источников. Был разработан внутренний механизм, который доставляет данные из источников посредством стриминга. Кроме того, с другой стороны был размещен самописный компонент, который подключается к топикам kafka в виде стрима, берет данные и записывает их при помощи таблиц. Такая схема позволяет масштабироваться горизонтально.Ну а теперь — о проблемахВсе, что выше, звучит неплохо, правда? Но, естественно, миграция такого уровня не может пройти совсем гладко. Возникло несколько проблем, о которых сейчас и поговорим. Миграция схемы с 8.3 PG до 9.4 PG требовала ручного вмешательства. Нельзя просто взять и забэкапить все с пятой версии Greenplum и Postgres 8.3 на шестую с Postgress 9.4. Проблема в том, что меняются сами запросы к базе данных, поэтому многое приходилось реализовывать вручную.Штатные инструменты бэкапирования и восстановления с S3 хранилища давали сбои (10% данных не грузились). На момент миграции у нас было около 800 таблиц, из которых 20-30 — весьма объемные, занимающие несколько терабайт каждая. Их бэкап мог занять 4-5 часов, и уже в конце процесса иногда возникала ошибка с сообщением невалидности данных. В этом случае приходилось либо использовать сторонние (т.е. не штатные) инструменты, либо обращаться к партнерам, которые все дополнительно фиксили.Как оказалось, шестая версия Greenplum была сыроватой. Поэтому у нас периодически возникали неожиданные ошибки, причину появления которых мы не понимали. Скорее всего, сейчас этой проблемы уже нет. Запросы отваливались по segfault, primary сегменты падали и происходило переключение. Примеры таких багов, которые к моменту написания статьи либо решены, либо в стадии принятия PR.https://github.com/greenplum-db/gpdb/pull/10273 https://github.com/greenplum-db/gpdb/pull/11538https://github.com/greenplum-db/gpdb/pull/11894https://github.com/greenplum-db/gpdb/pull/11988Консультации с аналитиками. Раньше аналитический отдел выполнял сложнейшие запросы без всякой изоляции ресурсов. Сейчас, после того, как изоляция была внедрена, многие запросы перестали работать, поскольку не были оптимизированы. Первые несколько недель нам пришлось регулярно консультировать аналитиков, разбирая каждый проблемный запрос. Что в итоге?В течение нескольких месяцев мы решали возникающие проблемы, переписывали запросы и проводили массу консультаций. Но в итоге добились того, чего хотели изначально:Стабилизировали работу кластера, теперь не страшно спать по ночам.Большинство запросов выполняются без проблем, клиенты с разных команд гарантированно получают свой пул ресурсов, практически не влияя на работу друг друга.Имеем последнюю актуальную версию сборки Greenplum.Заключили соглашение с консалтинговой компанией, специализирующейся на Greenplum, имеем DBA as Service услугу поддержки, освободили ресурсы для решения других задач.При миграции на новый кластер и, особенно, на новый мажорный релиз, имеем план Б, чтобы не остаться с единственным нерабочим кластером и злыми аналитиками).2 базовых совета, если хотите повторить этот путь: Заложите время на то, что все пойдет не по плану и вы потратите больше времени, чем планировали).При таких глобальных изменениях в продукте, скорее всего вам придется переписывать запросы, так как план запросов сильно меняется, и это надо тоже учитывать.',\n 'В двух последних постах Что такое тестирование? и Организация тестирования я поделилась своими соображениями об испытаниях. Хотя между понятиями «тестирование» и «качество» есть тесная связь, одно из них не обязательно подразумевает второе. Тестирование лишь дает нам какое-то представление о качестве.Я хотела завершить эту серию публикаций постом, полностью посвященным качеству и управлению качеством, но у меня оказалось так много мыслей на эту тему, что его пришлось разделить на две части. В первой из них я расскажу о своем взгляде на качество.Я предпочитаю использовать в отношении него термин «управление», а не «обеспечение» (QA), потому что информация, полученная в ходе тестирования, может помочь улучшить качество продукта, но не может «обеспечить» его. Проходя сертификацию на менеджера по качеству в Американском обществе качества (ASQ), я узнала, что QA подразумевает прежде всего правильную организацию процессов. Сейчас значение термина QA искажено, и зачастую он употребляется неверно, поэтому я не думаю, что он по-прежнему применим, — по крайней мере, в сфере разработки ПО.Существует множество разных контекстов, каждый из которых требует отдельного взгляда на качество. Но в любом случае оно, как характеристика, должно быть «встроено» в продукт с самого начала, и мне нравится, когда клиенты становятся участниками этого процесса.Самое популярное определение, которое я слышала, принадлежит Джеральду Вайнбергу: «Качество — это ценность для определенного человека». Оно мне нравится, но кажется слишком упрощенным и оставляющим за скобками некоторые параметры, которые следует иметь в виду.Вот уже несколько лет Алан Пейдж (Alan Page) и Брент Дженсен (Brent Jensen) обсуждают современные принципы проведения тестирования. Пятый из этих принципов звучит так: «Клиент — единственный, кто может судить о качестве нашего продукта и оценивать его». На мой взгляд, как и в случае с определением Джеральда, это чересчур простая концепция. Мне хотелось чего-то более содержательного, поэтому я решила сосредоточиться на том, какой должна быть стратегия качества для продукта, над которым я работаю.Вернемся в прошлое. Уильям Эдвардс Деминг определял хорошее качество как «предсказуемый уровень единообразия, надежности и соответствия стандартам клиента». И здесь мы также видим ориентацию на клиента. Многие из 14 принципов Деминга, описанных в его книге Выход из кризиса (Out of Crisis), по-прежнему применимы и, вероятно, останутся актуальными в будущем. Например, он говорил о «встраивании» качества и уменьшении зависимости от проверок.Дэн Эшби (Dan Ashby) в своем посте о четырех абсолютах качества Кросби показывает, что эти абсолюты, с поправкой на контекст, все еще можно применять к программному обеспечению.Несколько лет назад, когда я готовилась к беседе о процессах качества, Изабель Эванз (Isabel Evans) показала мне статью Дэвида Гарвина (David A. Garvin), которая снова заставила меня задуматься, что же в действительности представляет собой качество. Ниже я приведу несколько мыслей оттуда, но лучше прочитайте ее целиком. А в этой статье автор дополнительно раскрывает значение восьми параметров качества.Люди обычно говорят о качестве, как об одной общей категории. Большая часть того, что мы можем измерить, касается выполнения процессов — я называю это «качество процесса». В свою очередь, «качество продукта» связано с конечным продуктом и опытом наших клиентов. Подходы к качествуКачество продукта можно рассматривать с разных сторон. Точка зрения клиента — лишь одна из них. Она показывает, насколько продукт пригоден к использованию или удовлетворяет потребностям, которые у разных клиентов могут отличаться. Мы также должны учитывать мнение других ключевых лиц, заинтересованных в качестве продукта.Дэвид Гарвин говорит о пяти подходах к качеству. Ниже вы можете увидеть их в форме диаграммы.Все мы смотрим на качество под разными углами, которые могут меняться в зависимости от обстоятельств. Я начну с внутреннего круга, как с самого простого, и буду постепенно двигаться наружу. Ориентация на производствоРассматривая цикл разработки как производственный процесс, мы должны сосредоточиться на методах, которые используем, и соблюдении требований. Наивысший уровень качества в данном случае достигается путем выполнения спецификаций, а основное внимание уделяется предотвращению неполадок и переделок. Вот почему многие разработчики тратят время на выстраивание правильного производственного процесса и оценку его качества, а также проводят тестирование, направленное на эти цели.Однако нужно понимать, что разработка ПО отличается от производства одного и того же продукта раз за разом. Мы постоянно развиваем то, что у нас есть, меняем и адаптируем.Вот несколько мероприятий, которые позволяют поддерживать качество разработки (производства): разработка через тестирование (TDD), написание кода с учетом удобства отладки, рецензирование кода, непрерывная интеграция, исследовательское тестирование и даже обеспечение соответствия критериям готовности. Мы оцениваем качество процесса и отвечаем на вопрос: «Правильно ли он организован?».  Ориентация на продуктГарвин определяет этот подход как обеспечение качества отдельных компонентов, составляющих целое. Из лучших ингредиентов получается лучший продукт.При таком подходе производство компонентов стоит недешево, поэтому товары, сделанные из них, тоже имеют более высокую цену. Повышение качества продукта, который является качественным по умолчанию, подразумевает улучшение производительности и дополнительные возможности — что также увеличивает стоимость.Обсуждая этот вопрос, мы с Полом Симаном (Paul Seaman) придумали интересную метафору: обычный повар не обязательно приготовит из хороших ингредиентов вкусное блюдо, но умелый шеф даже из обычных продуктов может сделать нечто волшебное. Мы должны понимать, из чего состоит наш продукт и как эти части соединяются друг с другом.Вот несколько мероприятий, которые помогают поддерживать качество продукта: разработка на основе приемочных испытаний (ATDD) или разработка на основе поведения (BDD), а также тестирование параметров качества, таких как безопасность, производительность и эксплуатационная надежность. В этом случае мы отвечаем на вопрос: «Работает ли продукт так, как должен (или как нам хотелось)?». Ориентация на потребителяВзгляд на качество с точки зрения пользователя — один из самых часто используемых, но при этом очень субъективных и индивидуальных критериев.Он подразумевает, что потребители имеют достаточно информации для оценки качества продукта. А если это не так, они могут ориентироваться на некие признаки, чтобы произвести такую оценку. Возьмем, например, чашку кофе. Я предпочитаю простой черный кофе из зерен средней обжарки, а моя сестра всегда заказывает капучино. Что это значит для вас? Кто ваш потребитель? Кто сможет оценить качество вашего продукта? Должны ли вы удовлетворить потребности большинства потребителей или ориентироваться на конкретную целевую группу? Вот процессы, которые, по моему мнению, могут помочь поддерживать потребительское качество: разработка дизайна взаимодействия с клиентом для понимания его потребностей, ATDD, испытания с использованием тест-персон, A/B-тестирование, тестирование доступности и наблюдаемости, проведение аналитики использования продукта. Здесь мы пытаемся ответить на вопрос: «Этого ли хочет потребитель?». Ценностный подходЦенность продукта определяется его стоимостью и затратами на производство. Может ли он обеспечить нужную производительность по приемлемой цене?  Возьмем все ту же чашку кофе: одни люди хотели бы купить ее за 50 центов, а другие могут заплатить 5 долларов. Разница в цене составляет 1000 %. Неужели бобы, из которых готовится этот кофе, так сильно отличаются друг от друга? А может, дело в обжарке? Или в чем-то другом?Специалисты по маркетингу часто задаются этими вопросами. Чтобы найти ответы, они исследуют ценовые точки, проводят опросы среди покупателей, анализируют количество продаж и определяют, какую прибыль может принести та или иная специфическая характеристика продукта. Такие исследования позволяют подтвердить предположения и помогают организациям понять, как именно потребители используют их товары и какую ценность в них находят.Менеджеры по продукту также заинтересованы в поддержании этой ценности. Они должны найти такие ценовые точки, которые помогут сделать предложение привлекательным для клиента и в то же время позволят им получить прибыль, чтобы не выпасть из бизнеса.  Вопрос, на который нам нужно ответить, звучит так: «Представляет ли наш продукт достаточную ценность для клиента?». Абсолютный подходЯ намеренно оставила абсолютное качество напоследок. Гарвин описывает его как «изначальное совершенство», которое способен распознать кто угодно, признак высочайших стандартов и отличных показателей. Оно с трудом поддается количественному определению, но вы узнаете его, когда увидите. Ваши чувства подскажут вам. Возможно, это вкус и аромат того великолепного капучино, который вы заказываете снова и снова.Когда организация собирает команду специалистов для создания чего-то особенного, выходящего за рамки обыденности, получается продукт абсолютного качества. Это может быть, например, неожиданно прекрасная графика для новой игры. Однажды я работала над системой, которая должна была заменить другую — ту, что больше не поддерживалась. Когда я спросила, нравится ли новая система пользователям, мне ответили: «Да, очень!» — все потому, что она подстраивалась под их рабочие процессы, а не наоборот. Небольшой пример абсолютного качества. ЗаключениеЯ бы хотела, чтобы сейчас вы еще раз подумали о том, что значит качество лично для вас. Это непростая тема, поэтому ее редко обсуждают. На одном из своих семинаров я попросила участников дать собственное определение качеству конкретного продукта. Задайте этот же вопрос своим специалистам и посмотрите, о чем говорят их ответы — о качестве разработки (процесса) или все же самого продукта.Мой следующий пост будет посвящен управлению качеством: я расскажу о тех, кто его осуществляет, о проведении измерений и о том, как создать стратегию качества.Вы также можете прочитать пост о качестве и его стоимости, опубликованный в моем старом блоге в 2010 году. P.S.  Я все еще езжу на «Мини Купере» — правда, на новой модели, которая еще шикарнее, чем я описывала в том посте. Если вы это читаете, я надеюсь, вы прочтете и комментарии тоже.В преддверии старта курса QA Lead приглашаем всех желающих на бесплатный двухдневный интенсив в рамках которого изучим теоретические основы методов тестирования требований. Рассмотрим использование User Story и критериев приемки для тестирования бизнес-требований. Изучим Example Mapping как способ протестировать технические требования. А также попрактикуемся в построении Example Mapping.ЗАПИСАТЬСЯ НА ИНТЕНСИВ. День 1ЗАПИСАТЬСЯ НА ИНТЕНСИВ. День 2',\n 'SaaS платформа по управлению контентом дополненной реальностиВ данной статье хочу поделиться опытом с теми, кто начинает развивать свои B2B SaaS продукты и может столкнуться с вопросом переноса своего решения на внутренние ИТ мощности корпоративных заказчиков. Опишу, не столько технические нюансы, сколько организационные и архитектурные. Хочу акцентировать внимание на объеме дополнительных работ, которые необходимо учитывать при продаже внедрения Enterprise клиенту. Тем, кто работает изначально по модели внедрения ПО, читать будет не особо интересно, а проектам с SaaS моделью надеюсь, что будет полезно. Заказчика не называю.Статья вышла не очень маленькая, по этому привожу краткое содержаниеО проектеОсновной вызов - кратное масштабированиеЗадача №1. Понять реальный объем бедствия. Задача №2. Провести нагрузочные тестирования. Задача №3. Проектирование новой архитектуры, балансировка.Задача №4. Соответствие системы нефункциональным требованиям. Задача №5. Адаптация платформы под программные решения заказчика. Задача №6. Перенос на IT мощности заказчика. Задача №7. Внутреннее тестирование. Задача №8. Настройка отказоустойчивости. Задача №9. Подготовка документации. Задача №10. Настройка логгирования и 9Мы развиваем российскую платформу по управлению контентом дополненной реальности: в web-редакторе совмещается digital контент и маркер (как привило это фотография реального объекта), а с помощью мобильного приложения, в режиме камеры digital контент накладывается на маркер. В базовой модели платформа ARGUMENT работает как SaaS сервис: редактор в он-лайне и чтобы им пользоваться необходимо самостоятельно зарегистрировать аккаунт и начать создание AR контента. У нас B2B модель, которая дает компаниям возможность использовать AR в первую очередь, для обучения персонала.  Однако, ряд компаний по соображениям ИТ безопасности не хотят или не могут использовать облачные решения. Все сервисы должны быть On-Premise расположены на внутренних ИТ мощностях.Мы не первый и не последний SaaS сервис, который сталкивается с вопросом, внедрять ли решение заказчику или отказаться от возможности заработать и спать спокойно )). Технически и организационно гораздо проще развивать решение, размещенное в одном месте, нежели поддерживать работоспособность и обновления у всех клиентов. Да, для решения таких задач придуманы специальные технологии: и докеры и Kubernetes, но мы на момент разворачивания первых внутренних инсталляций контейнеризацией не владели. А когда клиент пришел – решили внедрять.С чем же мы столкнулись.Кратное масштабирование. Это практически основная задача из который вытекает все остальное. На момент внедрения наш SaaS обслуживал 2000-3000 активных пользователей мобильного приложения, которое по API забирает данные с сервера. Соответственно, вся архитектура была развёрнута под такие нужды: на одной виртуальной машине (под управлением Debian) в крупном ЦОД у нас была развернута и БД (MySQL) и Web-интерфейс редактора (PHPFpm +Phalcon), и API (PHPFpm) обработки запросов и файл-сервер и все прочие мелочи.Потенциальный размер аудитории у заказчика, кто может использовать наше приложение – 200 000 сотрудников. А такие масштабы — это совсем другая архитектура. Задача №1. Понять реальный объем бедствия. Нам цифра 200 000 ни о чем особо не говорит, для оценки данных для ИТ- инфраструктуры и проектирования новой архитектуры работы системы, необходимо понять количество обращений к серверам в единицу времени, желательно в секунду. Опыта использования таких систем в компании не было, поэтому, решили прогнозировать. Сколько сотрудников в день может сканировать маркеры и сколько раз за день. Чтобы это понять, расписывали по мелочам бизнес-кейс использования AR на рабочем месте: если это обучение нового сотрудника, сколько раз в день новый сотрудник будет что сканировать и как долго после принятия на работу; а если это работа с ассортиментом и нужно сканировать ассортимент в процессе работы; а если это сканирование стеллажей для работы с панорамами или склад и т.д.Задача №2. Провести нагрузочные тестирования.  Когда определились с количеством возможных запросов к серверам в секунду, необходимо было организовать нагрузочные тестирования, чтобы понять требования определения требований к архитектуре и инфраструктуре.Увы, у начинающего стартапа в штате специалиста по нагрузочным тестированиям не нашлось, пришлось обращаться к внешним специалистам. Эксперта нашли довольно быстро, развернули тестовый стенд (сначала один, скопировав нашу базовую архитектуру), развернули сервер нагрузочного тестирования, установили на него набор ПО для организации нагрузки и логгирования: influxdb, grafana, mavenz и пр. И начали нагрузку и тестирование. Задача №3. Проектирование новой архитектуры, балансировка.Сразу стало понятно, что архитектура «всё в одном» это «не серьёзно и так далеко не уедешь. Шагом первым мы выделили на отдельные сервера: СУБД, Web-редактор, Файл-сервер и сервер API.  Плавно увеличивали количество обращений (запросов в секунду) к серверу до запланированный значений. Делали это помощью специального ПО и отмечали, когда и в каком месте начинает сбоить: не хватает процессорных мощностей – не беда, увеличиваем в облаке мощности; оперативка – тоже самое; БД – правили SQL запросы. В определённый момент времени уткнулиcь в возможности веб-обработки API запросов, за которые отвечал nginx, тут стало понятно - на одном API сервере далеко не уедешь, нужно распределять запросы. Подключили второй сервер для обработки входящих от мобильного приложения обращений, а для распределения запросов подняли сервер балансировки с установленным на него haproxy. Он помог нам раскидывать входящие обращения то на один, то на другой сервер, равномерно распределяя нагрузку. В итоге, казалось, что двух API серверов нам вполне достаточно.Точно также определили, что файл-сервер - тоже узкое место при большом количестве запросов в секунду, пришлось продублировать и его.Подкрутили там (настройки nginx и php), подкрутили здесь (sql запросы и haproxy) и пришли к такой упрощенной схеме:На этом этапе стало понятно, какие мощности у заказчика нужны для поддержки такого решения: сколько серверов, сколько процессорных мощностей, сколько памяти и жестких дисков.Подготовили схемку в ArchiMate (пришлось попутно освоить), согласовали, пошли дальше.Задача №4. Соответствие системы нефункциональным требованиям. Помимо того, что именно делает наша система и как она будет работать на 200 000 сотрудниках, она во внутреннем IT контуре компании должна соответствовать серии нефункциональных требований.Нам прислали эксельку в 142 строки с таким же количество требований. Часть из них - общие технические требования, часть - касающаяся интеграции, часть по безопасности.  При этом, некоторые требования отсекающие, не выполнив которые, нельзя запускать систему в эксплуатацию, некоторые – критические, некоторые рабочие. Для каждого критерия определен вес и эталонные значения, при наборе определенного количества несоответствий опять же систему запускать нельзя.Но оказалось все не так страшно. Основному количеству критериев мы соответствовали, там где нет – пришлось перенастраивать или дорабатывать компоненты системы.Задача №5. Адаптация платформы под программные решения заказчика. Эта часть проекта была не самой сложной, но всё же пришлось повозиться.  Она же имела отношение к вышеупомянутым НФТ.В базовом варианте у нас сервера были на Debian, а unix заказчика это только лицензируемое ПО - Linux CentOS,  Red Hat Enterprise Linux (RHEL). Ок, сделали адаптацию, переписали некоторые компоненты системы, под специфичные версии пакетов этой системы.СУБД. Изначально мы использовали MySQL, у заказчика - много всякого другого, но корпоративного. Путём переговоров договорились на использование СУБД Percona, так как это форк от MySQL и переехать на нее было вполне реалистично.Задача №6. Перенос на IT мощности заказчика. Вот вроде то самое, с чего мы и хотели начать ). Но и тут не все просто. Как правило во внутренний IT контур компании по сети просто так не попадешь, это специальные доступы по VPN, внутренние виртуальные рабочие столы с протоколированием совершаемых действий и т.д. Потом внутри системы подключение по ssh к развернутым виртуальным серверам и т.д. По соображения безопасности, в крупных компаниях сервера не имеют доступа в интернет, а кто знает, установка unix окружения для серверного ПО тянет за собой необходимость установки множества различных пакетов из внешних репозиториев и заранее описать связи всех пакетов не представляется возможным, какие файлы понадобятся для работы компонент системы предугадать сложно. Так и получилось, зашли мы на голые сервера, а установить туда ничего не можем. Даже просто пробросить файлы на сервер нет возможности, всё через корпоративный файлообменник через специального сотрудника. Мы тут приуныли немного, но заказчику нужно было решение и нам выделили грамотного unix специалиста из внутренних кадров компании, мы быстро нашли общий язык и потихоньку все нужные компоненты окружения были установлены.Дело оставалось за малым – перетащить все компоненты нашей системы, настроить структуру БД и всего нужного окружения.Задача №7. Внутреннее тестирование. Когда перенесли все компоненты системы и настроили доступы к балансировщикам и API через интернет (если помните, у нас приложение на смартфоне в мобильной сети оператора связи считывает данные с внутренних серверов) то только тут приступили к внутреннему тестированию. На данном этапе выявили ряд багов, что-то по работе приложения, что-то по нагрузкам, а что-то по отказоустойчивости. Ничего из ряда вон выходящего. Задача №8. Настройка отказоустойчивости. Тут все для нас было просто, так как практически вся данная задача легла на плечи внутренних специалистов заказчика. Настроена система регулярного бекапирования. Почти все сервера из схемы выше были продублированы с целью сохранности данных (БД и файл-сервера) и стабильности работы. Была настроена система автоматического разворачивания резервных серверов по мере загрузки мощностей существующих серверовЗадача №9. Подготовка документации. Тут, наверное, все по стандарту, но всё же чтобы не было сюрпризом. В нашем случае комплект документации был таким:Соответствие нефункциональным требованиямОписание архитектурного решения Схема компонент системы, в формате ArchiMateРасчет нагрузочного тестирования с учетом горизонтального масштабированияИнструкция пользователя (он же редактор контента)Инструкция конечного пользователя (владелец смартфона)Инструкция администратораИ это мы еще легко отделалисьЗадача №10. Настройка логгирования и системы мониторинга.Когда система была передана на внутреннее ИТ сопровождение, то понадобились дополнительные телодвижения для настройки мониторинга и оповещений. Система должна сигнализировать в нужных случаях – у меня сбой: место заканчивается, запросы не правильно обрабатываются, данные не считываются и т.д. Всё автоматизировать и предусмотреть не возможно, но сделали, что могли. Настроили систему логов и передачу данных в систему мониторинга при критичных значениях работы компонент платформы.Вот вроде и всё. Хочу отдельно отметить, что весь объем работ занял у нас около полугода. Система функционирует стабильно, находится под нашим наблюдением м техническим сопровождением и приносит прибыль заказчику, чего и всем желаем.Хоть статья получилась и большая, но рассмотрены тут только основные моменты, так как весь объем работ в статью не засунешь: обошли стороной вопросы интеграции с внутренней LMS и ActiveDirectory, разворачивание тестового стенда во внутреннем контуре и пр Надеюсь, не усыпил и кому-то было полезно.',\n 'Представляю текстовую версию моего недавнего разговора с коллегами из Data Egret — компании, которая специализируется на поддержке PostgreSQL. Я пообщался с Ильей Космодемьянским (CEO) и Алексеем Лесовским (senior DBA). Обсудили, как мониторить PostgreSQL, какие бывают ошибки при выборе и настройке систем мониторинга, кто такие DBA и какие soft skills для них важны, а также затронули более хардкорные темы. Пост объемный, но он того стоит.Илья: Всем привет. Меня зовут Илья Космодемьянский. Я CEO компании Data Egret и один из ее основателей. Мы занимаемся поддержкой баз данных PostgreSQL. Нам часто приходится решать вопросы вроде «Что произошло?», «Кто бросил валенок на пульт?» и т. п. Клиентов у нас много, базы огромные. Без нормального мониторинга мы бы, конечно, всё это не осилили. Поэтому тема сегодняшнего разговора — мониторинг.Мои собеседники — это коллега Алексей Лесовский, большой специалист по Postgres, и Владимир Гурьянов, который занимается разработкой системы мониторинга Okmeter. Система много чего умеет мониторить, в том числе Postgres.Кто такие DBA, и чем они занимаютсяВладимир: Я буду сегодня выступать не как эксперт, а скорее как ведущий. Коллеги, расскажите, чем вы занимаетесь.Илья: Мы поддерживаем всё, что касается базы данных Postgres. Мы давно работаем на российском и зарубежном рынках. У некоторых компаний есть администраторы баз данных — DBA (database administrator). Мы можем такого DBA заменить или дополнить. То есть следим за тем, чтобы базы данных в компании были под присмотром, все рутинные задачи решены, траблшутинг оттраблшучен.Чем мы отличаемся от обычного DBA? У большинства администраторов в Data Egret гораздо больше опыта эксплуатации самых разных баз, больше сложных ситуаций, с которыми приходилось сталкиваться. Как правило, стрессовые штуки встречаются нам гораздо раньше: мы раньше нащупываем баги, раньше встреваем в какие-то проблемы.Владимир: Я в свое время достаточно долго работал инженером. Знаю, что во всех компаниях есть база данных (БД), но не у всех есть выделенные DBA. Поддержкой БД занимаются там обычно инженеры эксплуатации или разработчики. В какой момент у компании появляется необходимость в выделенном DBA, когда они к вам приходят? Кто вообще ваши клиенты?Илья: У нас очень разные клиенты. Отсутствие у них выделенного DBA может быть связано с кучей разных особенностей. Например, компания еще не доросла до этого, и БД проще поручить сисадмину. Иногда просто нет регулярных задач для DBA. Многое зависит от того, какая специфика бизнеса, как разработан бэкенд, как он ходит в базу и т. д.Среди наших клиентов и маленькие, и большие компании. Есть большие компании, у которых целый отдел опытных DBA, но поскольку они постоянно работают с плюс-минус одним проектом, им бывает нужна внешняя экспертиза — помочь разобраться с чем-то специфическим, какой-то подземный стук диагностировать. Бывает, что для небольшой компании мы периодически выступаем в роли DBA, потому что постоянно он им не нужен.Владимир: Мне раньше казалось, что DBA — это такие специалисты, которых очень сложно найти. При этом в последние несколько лет большинство компаний осознали, что все они — ИТ-компании: что магазин одежды, что ритейл, что ресторан. Существенно вырос спрос на ИТ-специалистов. И это очень заметно по разработчикам, по DevOps-инженерам. А как обстоят дела с DBA?Илья: Точно так же... Кстати, мы ищем DBA. Если кто интересуется, обязательно пишите. (Смеется.) Илья КосмодемьянскийДело в том, что DBA исторически был не очень популярной профессией. Многие разработчики считают, что это какая-то скучная старперская штука. Почему-то это не популярно. Прикольнее быть каким-нибудь разработчиком Haskell, стоять на переднем крае технологий. Поэтому дефицит DBA был всегда. Это довольно дорогие специалисты, с вредным характером. DBA знает себе цену, он разбирается, куда пойти работать, хочет много денег, приятного графика.К тому же, сейчас появились две тенденции. Во-первых, количество БД растет — в связи с любовью к миросервисам, со всеми этими подходами, которые приносят много пользы бизнесу. Из-за этого масса всяческих издержек. Когда у тебя, скажем, 20 баз, начинаются гораздо более интересные истории, чем когда у тебя одна большая база, за которой могут следить двое DBA. То есть микросервисы тоже влияют на рост спроса.Во-вторых, многие компании считают, что им не нужен целый DBA, что хорошо бы иметь отдел на подхвате, который в случае чего может прийти на помощь.Владимир: Как вообще попадают в эту профессию? Можно ли перейти откуда-то? И с чего стоит начать?Илья: На эту тему можно долго рассуждать. Я когда-то даже специальный доклад делал, чтобы рассказать, с чего начать, какие книжки читать. Но сейчас я могу точно сказать: было бы желание. Один из профилей людей, которых мы обычно ищем, это уверенный пользователь Linux. Тот, кто не боится командной строки, не боится читать документацию. Это программа-минимум. И я бы сказал, что при должной въедливости остальному научим. Дальше мы начинаем рассуждать о том, что нужно иметь какое-то представление о математической логике, о базах данных, о том, что такое транзакция. Но, по моему опыту, этому можно научиться по ходу дела.Владимир: Сейчас ИТ-рынок активно развивается. Каждый день появляются сотни новых технологий и направлений. Насколько это влияет на БД? Или это все еще такой укромный, консервативный уголок, где рост не столь стремительный и бурный? Илья: Рост, конечно, не столь бурный и стремительный, как много где. Но при этом понятно, что БД — это не уголок. БД — это такая штуковина, которая есть практически в любой компании. Она находится в центре ИТ-инфраструктуры, хочешь ты этого или нет. Все системы с ней взаимодействуют. Если, условно говоря, активно развивается Java, сообщество придумывает какие-то новые технологии, появляются какие-то принципиально новые языки, то неминуемо уголок DBA перестает быть уютным. Потому что эти новые языки и технологии должны взаимодействовать с базой. И DBA нужно вникать, что там происходит — на уровне решения проблем, которые вызывает конкретное приложение, написанное на конкретном языке.Могу по своему опыту сказать, что мне приходилось работать с языками, на которых я в жизни ничего не мог написать. Потому что когда я что-то кодил, их еще не было или они еще не набрали популярность. Но все эти технологии надо учить, разбираться в них.На самом деле здесь зеркальная ситуация. Если ты работаешь с БД, у тебя с одной стороны консерватизм, а с другой — постоянно происходит накопление новых знаний. Потому что все эти новые знания в базу ломятся, их периодически надо траблшутить. Это очень расширяет кругозор.Мониторинг БД и популярные системыВладимир: Я прекрасно понимаю, о чем ты говоришь. Вроде бы Linux остался тот же самый, а языков и технологий появилось такое количество… Но перейдем к мониторингу.Мониторинг — такая же важная штука для БД, как и для любой другой системы в ИТ-ландшафте?Илья: Иногда даже еще более важная. Владимир: А насколько принципиально отличается мониторинг БД от мониторинга того же Linux?Илья: Linux долгое время был немного вещью в себе в плане диагностики. Такие технологии, как eBPF, стали появляться относительно недавно. Долгое время средства диагностики были довольно куцые: какой-нибудь мониторинг ввода-вывода в виде iostat и примерно всё.Что касается БД, то здесь всегда было нужно разобраться с тем, что происходит с запросом, почему именно он тормозит, где узкие места, какой тренд в нагрузке, трассировка, сэмплинг и т. д. То есть базы начали тщательно мониторить немножко раньше, это базам имманентно.Владимир: А бывает так, что к вам кто-нибудь приходит с просьбой о помощи и оказывается, что у них совсем нет системы мониторинга?Илья: Бывает, конечно, но очень редко. Обычно с мониторингом такая же ситуация, как с бэкапами: часто люди снимают бэкапы, а восстановление с них не тестируют. Это такая массовая большая беда. С мониторингом бывает так, что он вроде как есть, но в реальном траблшутинге задействован редко. Снимаются какие-то метрики, которые при возникновении проблем просто не помогают. Еще бывает, что в мониторинге нет чего-то важного, зато какая-нибудь левая хрень есть.Владимир: Какая система самая распространенная среди ваших клиентов?Алексей: Давайте я попробую ответить, но начну с предыдущего вопроса. Илья говорил, что к нам приходят клиенты, и у них, бывает, мониторинга нет. На самом деле Илья немного приукрашивает ситуацию. Да, приходят клиенты, у которых действительно нет мониторинга...Илья: Я политкорректен.Алексей: Да, ты политкорректен, но всё намного страшнее, чем кажется. (Смеется.)Люди приходят без мониторинга. Либо с мониторингом, но они в него не смотрят. Иногда мониторинг есть, но мониторится только операционная система, а остальные компоненты инфраструктуры, включая БД, — нет. Используется какой-то дефолтный мониторинг, который шел в комплекте.По поводу того, какая система — самая распространенная. Здесь можно поделить мониторинг на опенсорсный и на закрытый. Если брать именно опенсорсный, то на первом месте Zabbix. Он появился раньше, чем Prometheus, более распространен. Для него больше есть всяких рецептов, плагинов. Он до сих пор хорошо развивается, с каждым релизом много интересных функций выходит. Он постоянно в движении. На втором месте Prometheus и, как правило, Grafana для отрисовки графиков. Это два основных мониторинга, которые мы чаще всего встречаем.Алексей ЛесовскийС Zabbix довольно легко начать, потому что у него свой пользовательский интерфейс, своя база знаний внутри, где довольно много решений описано: как что ставить, что мониторить. С Prometheus чуть сложнее. У него большая экосистема экспортеров. Очень часто экспортеры предлагают базовую функциональность, и расширять ее нужно самому пользователю. Чтобы ее расширить, приходится глубже погружаться в предметную область того объекта, который нужно замониторить. И вот здесь начинаются сложности.Можно, конечно, довериться кому-то из экспертов по Prometheus. Либо можно самому из интернета скачать какие-то дашборды, какие-то темплейты конфигов и их использовать. Но здесь тоже могут быть сюрпризы: скачиваешь что-нибудь этакое, открываешь дашборд в Grafana, но там чего-то не хватает. Потом приходится самому это добавлять, кастомизировать эти дашборды. С Zabbix в этом плане попроще.Ошибки при мониторинге баз данныхВладимир: Не сталкивались ли с такой ситуацией, когда система мониторинга вместо того, чтобы приносить пользу, давала какую-то большую часть нагрузки на БД?Илья: Нет ли здесь скрытого наезда на Zabbix? (Смеются.)Алексей: Бывало, но такое по пальцам можно пересчитать. Я с таким сталкивался, может, один-два раза.Илья: У нас иногда бывали на поддержке просто базы с Zabbix, которым требовался траблшутинг.Владимир: Я понимаю, о чем идет речь, я видел пару инсталляций, которые собирали метрики с 5-6 тысяч хостов, и Zabbix’у в этом случае было достаточно тяжко.На самом деле это был наезд не на Zabbix. В моей практике мне попадалась достаточно нагруженная БД. Там были экспортеры от Prometheus, один из них немножко аффектил базу. При определенном стечении обстоятельств бывало печально.Алексей: Могу привести пример из своего опыта. В PostgreSQL есть модуль, который называется pg_buffercache. Он показывает утилизацию шаренных буферов. Запросы, которые используют pg_buffercache, раньше были довольно тяжелыми. (Сейчас — не знаю, может ситуация улучшилась.) И когда в мониторинг добавляешь запросы, которые используются в pg_buffercache, это начинает создавать довольно-таки хорошую нагрузку и может привести к проблемам. Я сам наступал на такие грабли, когда делал плагины для Zabbix.Владимир: А есть какой-то топ распространенных ошибок, с которыми вы встречаетесь при построении мониторинга у клиентов?Илья: Наиболее частая проблема — это отсутствие должного мониторинга. То есть мониторинг какой-то стоит, а на график, например, ничего не выведено. Бывает, что вроде бы какая-то информация в системе есть, можно посмотреть, просемплить. Но когда встает вопрос о том, что было вчера в 2 часа ночи, ничего не видно. И главное — не видно тренда, куда ситуация развивается.Очень часто берут дефолтные дашборды и считают, что у них теперь есть мониторинг. При этом зачастую там достаточно бесполезная информация. Бывает, что не сбрасывают статистику. У Postgres есть есть довольно странный, по историческим причинам, просмотрщик — pg_stat_bgwriter. В силу того, как он сделан, в нем надо периодически скидывать статистику. Если статистика ведется от начала инсталляции базы, смысла в ней довольно мало. Тем не менее бывает, что вроде бы дашборд сделан, выведены графики этих циферок, но они абсолютно неинформативны, потому что статистика просто не скидывалась.Скажу так: мониторинг часто носит на себе следы его неиспользования.Сотрудничество Data Egret и OkmeterВладимир: Расскажите про взаимодействие с Okmeter еще до того, как он стал частью «Фланта». Как вы вообще познакомились, как началось ваше сотрудничество?Илья: Во-первых, мы были знакомы: когда-то давно с одним из основателей Okmeter мы сисадминили в одной конторе. Во-вторых, на самом деле это было очень органичное взаимодействие. Ребята, которые начали делать Okmeter, работали в службе эксплуатации одного из наших клиентов. Их бизнес родился из того, что они просто начали делать свой мониторинг, который бы мониторил всё более комфортно. Поскольку там большая часть инфраструктуры была Postgres’овой, мы им помогали сделать эту часть. Ну, и постепенно это вылилось в такое взаимодействие, когда мы говорили ребятам: мы хотим вот это мониторить. А те говорили: окей, хорошая идея, давайте сделаем.Владимир: Чего, на ваш взгляд, не хватает Okmeter на данный момент — не только в разрезе БД, а в целом?Алексей: В плане развития мониторинговых вещей, которые в Postgres появляются, они появляются с каждым релизом [Okmeter]. Другое дело, что клиенты не всегда используют самый свежий Postgres. Бывает даже так, что есть базы Postgres, которые вышли из поддержки, для них патчи не выпускаются и т. д. Поэтому, конечно, в мониторинг можно добавлять новые фишки, связанные с Postgres, но это не самое основное. Некоторые можно и потом добавить, чрез несколько лет. Более насущная проблема Okmeter — это то, что, как правило, не хватает чего-то по мелочи в плане интерфейса или в плане каких-то функций, связанных с самим мониторингом. Но в целом, если говорить о функциях, когда мы делали техзадание на мониторинг PostgreSQL, мы заложили все основные вещи. Okmeter сделал их, и нам до сих пор хватает этого с лихвой. Это и топ-графики, и графики по запросам — то, чего не хватает в том же Zabbix.Примеры графиков Okmeter для PostgreSQLЕсли же говорить про интерфейс, то там бывают всякие мелочи — например, работа с датами не всегда удобная. Приходится лишнего накликивать, чтобы получить желаемое. Или, например, с алертами бывает неудобно работать: есть группа DBA, серверы баз данных, и нам нужно алерты только по этим серверам получать. Но мы получаем алерты по всем инстансам. Приходят, скажем, алерты, которые связаны с тем, что контейнера больше не существует.Ну, и бывает, что не хватает документации. Язык свой, его нужно знать, там много функций, операторов. В документации есть примеры использования, примеры запросов. Но какие-то угловые кейсы, использование каких-то экзотических функций — ты их просто не знаешь, а они могли бы где-то облегчить жизнь. Приходится как-то по наитию все это изучать. Есть всякие неочевидные тонкости типа копирование элемента легенды по тройному клику. Для нас в свое время это было открытием. (Смеются.) Не знали, что такое есть, в документации тоже, по-моему, этот момент не описан. Вполне допускаю, что есть и какие-то другие неочевидные вещи. То, что я перечислил относится, как правило, к работе с мониторингом, с пользовательским интерфейсом. А в плане мониторинга базы данных, в общем-то, всего хватает. Владимир: Есть хорошая новость, что в обозримом будущем интерфейс будет существенно улучшен. Мы уже активно делаем новый интерфейс, прямо с нуля. Надеюсь, уже скоро все его смогут попробовать.Алексей: Главное, не отключать старый. Потому что знаете, если пользователям подсовываешь что-то новое, они такие: «У-у, не то. Верните нам старое».Илья: На самом деле, когда Okmeter появился, его интерфейс после Zabbix был вау.Как Data Egret делятся знаниями и опытомВладимир: В процессе работы у вас накапливается какой-то большой объем знаний, опыта. Понятно, что часть своего опыта вы вложили в Okmeter. А все остальное вы как-то внутри агрегируете, делитесь этим с общественностью?Илья: У нас много способов. Во-первых есть свое внутреннее хранилище инцидентов, опыта, набитых шишек, инструкций и так далее — как у всех. Во-вторых, мы стараемся все это активно объяснять людям на всех основных конференциях Postgres-коммьюнити, на коммерческих конференциях, где Postgres — одна из тем. Алексей тоже довольно много делает для того, чтобы этот опыт передать. Принципиальная такая позиция: мы не боимся раскрывать наш опыт. При этом я должен честно сказать, что идеального способа куда-то эти знания дампить, а потом ресторить кому-нибудь в голову, я пока не придумал. Сами по себе знания можно сейчас получить откуда угодно. У Postgres хорошая документация, масса справочных материалов. Можно пойти на «Планету», почитать, что там пишет куча умных людей, почитать рассылку «Хакера». Правда это занимает много времени. А для DBA очень важная часть — это опыт, который, в общем-то, не пропьешь. Знать много про PostgreSQL — это одно, а быть хорошим траблшутером Postgres’а — это принципиально другая история. Владимир: А у вас есть какие-нибудь внутренние тренажеры для тех, кто приходит в компанию? Может быть, какой-то тестовый контур, где вы ломаете базу и просите починить?Илья: Мы несколько раз такие вещи делали. Но с БД в  этом плане сложно. Можно сделать тренировочную БД, но будет понятно, что она тренировочная. Какие-то задачи можно выполнять только на реальных клиентских кейсах, когда что-то происходит. К сожалению, тут очень сложно потренироваться сначала на кошках, а потом делать что-то свое. Хотя в принципе у каждого есть какие-то тестовые места для упражнений.Об участии в Open Source-проектахВладимир: Кстати, о траблшутинге… Бывает же, наверное, в процессе вы находите какие-то неприятные баги в самой базе или в каких-нибудь Open Souce-утилитах. Как часто приходится коммитить исправления? И насколько активно компания участвует в жизни Open Source-проектов?Илья: Если говорить о PostgreSQL, то это удивительно стабильный проект, с хорошим качеством кода. Что-то критичное, что надо срочно исправлять, появляется крайне редко. За всё время, пожалуй, всего один раз был очень серьезный баг, который затронул клиента. Баг был зарепорчен и очень быстро исправлен. По-моему, в версии 9.3 поломали репликацию, и при определенных случаях восстановление не происходило. Из бэкапа, соответственно, тоже. По мелочи же это постоянно происходит, потому что на подводные грабли с таким объемом клиентов мы наступаем регулярно. Коммитить что-то самим и баги находить — это несколько разные вещи, и у нас часто этим занимаются разные люди. Мы очень много репортим багов в баг-рассылку. Потому что репортинг бага — это немножко другая задача, чем просто разработка. Я думаю, что Максим Богук, небезызвестный наш коллега, зарепортил багов больше, чем иные коммитеры за всю историю проекта PostgreSQL. Даже был анекдот, что это не реальный человек, а какая-то группа анонимных хакеров под псевдонимом шлет в баг-рассылку истории.При этом надо понимать, что у нас сложившийся workflow. То есть мы знаем, как описать баг, как его воспроизвести. Примерно знаем, кто обычно работает с этой тематикой в Postgres, кого пнуть на этот счет персонально, если баг завис.Но помимо это мы контрибьютим — как кодинг, так и некодинг. У нас есть ребята, которые постоянно попадают в списки контрибьюторов, потому что что-то делают. Например, Сергей Корнилов и Виктор Егоров. Я со своей стороны решаю некоторые менеджерские задачи сообщества. Поскольку сообщество — большое, таких задач тоже довольно много. Там надо и с людьми взаимодействовать, и организационные вопросы решать — то, чем, например, занимается европейская ассоциация PostgreSQL, где и я сижу. Там огромный список разных вещей, начиная с проведения конференций, заканчивая юридическим сопровождением коммьюнити. Это очень комплексная тема.Владимир: Я даже немного растерялся: никогда не думал, что Open Source-сообщество вокруг баз данных настолько развито и настолько многогранно. В целом для меня БД всегда были немного темным ящиком. Оказывается, здесь гораздо более яркая и активная жизнь, чем кажется со стороны.Владимир Гурьянов, ведущий инженер проекта OkmeterАутсорсинг DBA и мониторингаВладимир: В моем коротком списке остался один вопрос, который я бы хотел обсудить. Не так давно Дима Столяров выступал с докладом «Перестаньте делать Kubernetes». Основная идея в том, что Куб — это очень сложная штука, и каждой компании делать ее очень дорого и сложно. Поэтому намного выгоднее и интереснее отдать ее кому-то на обслуживание, а самому заниматься более интересными задачами. Как вы думаете, насколько этот тезис справедлив по отношению к базам данных и к мониторингу —  в том, чтобы отдать все эти составные кубики куда-то на аутсорсинг?Илья: Сложный вопрос. Старорежимные админы раньше ругались на всякие микросервисы, контейнеры и прочее. Например, у вас есть проблема монолита. Вы разбиваете монолит на 50 маленьких монолитиков, и они должны взаимодействовать между собой. Но вы убрали одну проблему, перенесли ее на другой уровень и добавили новых. В принципе, это не решение первоначальной проблемы. То есть — да, прикольно сосредоточиться на своем основном бизнесе; например, продавать мороженое на весь мир, его логистику обеспечивать. А ИТ-инфраструктуру такой компании, которая гипотетически будет играть огромную роль, отдать на Managed Kubernetes. Но очень быстро в таких компаниях мы что наблюдаем: что админам и техническим менеджерам, которые там работают, внезапно нужны совершенно другие скиллы.Можно даже взять такой вульгарный пример. Например, мы, не сильно заморачиваясь с тем, как это должно работать, накликиваем в Amazon инфраструктуру компании. Что нам нужно: PostgreSQL, DynamoDB, какие-нибудь файловые хранилища, серверы для приложений и т. д. Во-первых, это будет очень дорого: не зная, можно очень легко пробить любой бюджет, особенно при масштабировании. С другой стороны, мы гарантированно сделаем это неправильно. В лучшем случае накликаем все это от рутового амазоновского пользователя, без правильного понимания политик, без правильного понимания менеджмент-ресурсов. То есть уже нужен специальный человек, который из кубиков что-то умеет составлять. Мы просто сдвигаем то, где у нас админ. Теперь он занимается немного другими вещами, но все равно он нужен.Владимир: Да, все так. В этой ситуации по сути есть два таких рабочих решения. Это либо нанимать в штат специалистов, которые будут хорошо разбираться в предметной области, либо отдавать ее компаниям на аутсорсинг. Раньше компании очень боялись отдавать что-то во вне. И у меня такое ощущение, что в последнее время это очень сильно изменилось. Компании стали нормально к этому относиться, потому что сложность возросла настолько, что невозможно держать такое количество специалистов, просто невыгодно.Илья: С одной стороны, это действительно так. С другой, я бы сказал, что компании научились работать с аутсорсингом, в хорошем смысле. Потому что одна из суровых проблем, с которыми мы сталкиваемся, это когда приходит к тебе, например, клиент — компания со своими поставленными процессами. Эти процессы, чего греха таить, неоптимальны. (Ни у кого нет оптимальных: ни у нас, ни у вас, ни у кого из наших слушателей. Всегда есть что-то индивидуальное, что в процессе изменений, что-то не совсем корректное.) И часто бывает так, что сложность процессов компании несоизмерима с тем, чтобы взять аутсорсинг. Ты хочешь взять DBA на аутсорсинг, а выясняется, что у тебя процедура согласования любого процесса, связанного с БД, включает 50 сотрудников из 20 департаментов. Никакой DBA этим заниматься не будет. А на самом деле тебе нужен DBA с большими скиллами менеджмента и с пониманием процессов именно этой компании. Многие компании научились с этим работать. То есть, когда стало понятно, что проще держать внешнюю экспертизу, и стали целенаправленно учиться, как с такой экспертизой работать. В то же время многие поняли, что обнести все колючей проволокой и переживать за свою безопасность без внешних подрядчиков — тоже не вариант. Все равно что-то когда-то рискует утечь. Нужно наращивать серьезную безопасность вместо того, чтобы считать, что мы просто не пускаем никого к себе в контур и поэтому якобы безопасны. Это тоже поменялось.О важности soft skills для DBAВладимир: Еще один вопрос про современные тенденции. Я все чаще вижу и слышу от коллег-инженеров, что при приеме на работу все больше учитываются не только hard skills, но еще и soft skills. Я знаю, что ряд крупных российских компаний при наличии двух примерно сопоставимых по силам кандидатов выбирают того, у кого сильнее soft skills, даже если слабее hard skills. А в случае с DBA это как-то проявляется?Илья: Проявляется, конечно. Леша не даст соврать, мы иногда шутим, что нам нужно уметь не столько даже базу починить, сколько задушевную беседу с клиентом провести. Огромное количество проблем с БД связаны не с ними как таковыми, а с тем, что там какая-то ошибочная настройка, или плохой запрос приехал. Запросы пишут люди, настройки делают тоже люди. Люди делали что-нибудь, как им привычно, может быть, не совсем оптимально, и у них появилась какая-то проблема. Им надо объяснить, что виновата не база, а виноват подход — даже если он 10 лет назад был правильный, даже если вчера был правильный. Просто ситуация изменилась. Поэтому для DBA умение объяснять на самом деле всегда было очень важным скиллом, а сейчас — особенно, поскольку все усложнилось. С БД работают разные люди из разных технологий. Люди часто гораздо больше ориентированы на продукт. И мне кажется, что soft skills просто всегда были недооцененными у базистов, потому что, дескать, они такие вот совсем системщики, в своей базе сидят и ничего вокруг не замечают. Но это на самом деле не так, и никогда так не было.Алексей: Согласен с Ильей. Soft skills нужны, Всегда нужна способность договариваться с клиентом, самыми разными способами объяснять свою, скажем так, не претензию, а причину проблем. Если он не понимает, нужно с другой стороны зайти и снова как-то объяснить. И это нужно делать оперативно. Эти скиллы очень нужны и без них никуда. Просто потому что в какой-то момент можно зайти в тупик с клиентом: он будет стоять на своей позиции, DBA — на своей. Это будет контрпродуктивно для всех.Илья: Это вообще давняя проблема эксплуатации и разработки. Очень часто бывает, что они сидят по разным окопам и друг с другом враждуют. А по идее, должны делать одно и то же дело. Для DBA, который ответственен за работу БД и считает что, это единственное, за что он ответственен, разработчики — это перманентная головная боль, потому что все время рискуют положить БД.Если заниматься тем, что просто отбиваться от разработчиков, не пытаясь проявить эмпатию, понять, почему они так делают, как они замотивированы, то это путь в никуда.Ты очень быстро начнешь посылать всех, как человек за прилавком. Это неконструктивно и для работы, и для отношений, и для всего прочего.Разработчик придумывал месяцами какую-то фичу, вынашивал разрабатывал, пробовал, ошибался. Тут он приносит тебе ее на ревью перед выкатыванием в бой (и хорошо, если приносит, а не так, что выкатил — и она упала). А ты ему говоришь: «Ну Вася, ты и дурак. Кто ж так делает...» Какой результат у такого диалога будет? (Смеются.)Вопросы слушателейВопрос из чата: «Что Okmeter использует для хранения метрик?»Владимир: Это отличный вопрос. У Okmeter изначально был разработан свой формат хранения и передачи метрик. Сейчас он использует Cassandra как долгосрочное хранилище. Также у него есть Kafka в качестве буфера, и кэши, которые держат в памяти метрики за 1 час и за 4 часа. Но поскольку, повторюсь, там свой формат передачи метрик и их группировки, это позволяет потреблять не очень много ресурсов при большом объеме данных.Андрей: Планируются ли в Okmeter какие-то интеграции с облачными метриками?Владимир: Да, планируются. Если у вас есть какие-то конкретные пожелания, предложения, напишите мне, мы это зафиксируем. Сейчас мы определились с направлениями, которые мы хотим сделать в первую очередь. У нас точно будет интеграция с большинством облаков. Пока четкого понимания, какие метрики и откуда мы будем забирать, нет, но в планах это есть. А также — автодетекты того, в каком облаке запущен агент, уведомления, если не хватает каких-то доступов, и так далее. Вячеслав: Привет. Хотел бы поблагодарить Алексея Лесовского за его доклад на предыдущей конференции, которая была в этом чате. С удовольствием послушал. Я один из тех людей, которые сделали свой мониторинг — на базе InfluxDB со сборщиком Telegraf. Причина, по которой я сделал такой мониторинг: InfluxDB был хранилищем метрик, который был под рукой. Почему я продолжаю его использовать: попробовал Prometheus и InfluxDB. В Influx у меня получилось сделать различные retention policies с разным шагом хранения метрик. Метрики хранятся несколько месяцев и потом стираются. А в Prometheus не было такого встроенного механизма, и там они хранятся неделю у нас на стенде. Мы храним сразу в двух местах: неделю в Prometheus и много месяцев в другом хранилище. По поводу Okmeter хотел узнать какой-то, может быть, секрет, как там всё хранится. Может, какой-то ClickHouse или еще что-то более компактное, производительное.Владимир: Если вам интересно посмотреть, как это реализовано под капотом, на YouTube есть большой доклад Николая Сивко. Он рассказывает как раз о том, как устроено хранилище Okmeter и почему именно такое решение было выбрано. В рамках текущей встречи в двух словах не рассказать, там много нюансов. К слову, небольшой анонс. В ближайшие полгода, может быть, чуть больше, мы планируем запустить новый вариант хранилища для Okmeter. Как только мы получим первые результаты, мы обязательно ими поделимся. Но нам кажется, мы придумали достаточно интересную историю. Расскажем-покажем, как это будет работать.Дмитрий: Всем привет. Учитывая, что здесь две компании, у меня вопросы к обеим. Okmeter — достаточно большой комбайн, который мониторит много чего, насколько активно он смотрит в сторону развития мониторинга PostgreSQL, о котором в том числе Алексей Лесовский рассказывал в свое время. Появляются какие-то достаточно интересные дополнения к Postgres, которые, может быть, тоже было бы интересно мониторить — TimescaleDB и что-то еще. Владимир: По поводу развития мониторинга PostgreSQL мы продолжаем взаимодействовать с коллегами [из Data Egret]. Всё то, что появляется нового в Postgres, и то, что нужно добавлять, мы стараемся дорабатывать. В целом мы сейчас формируем обширный бэклог, который включает как текущие продукты и список того, что нужно доработать, так и новые. Потому что сейчас есть ряд новых, модных баз данных и продуктов, которые набирают популярность, и которые у нас недостаточно еще покрыты. И мы активно занимаемся развитием в этом направлении. В любом случае система мониторинга — это очень живая вещь, которую нельзя сделать один раз так, чтобы она работала потом 5-10 лет. Ее нужно все время дорабатывать. В частности, поэтому у нас есть отдельная команда, которая занимается исключительно тем, что смотрит, какие метрики нужно еще собирать, как эти метрики формировать в полезные дашборды. Помимо этого, она взаимодействует с большим количеством разных компаний, как внутренних, так и внешних, разбирает реальные кейсы, вытаскивая из них информацию: какие данные еще нужны, как сформировать дашборды таким образом, чтобы можно было еще быстрее находить проблемы; как сформировать алерты таким образом, чтобы в следующий раз не допустить эти проблемы, узнавать о потенциальных проблемах заранее и успеть их предотвратить. Дмитрий: Будут ли рассматриваться какие-то изменения, кроме технических — например, по коммерческой модели предоставления услуг?Владимир: Это более сложный вопрос. Прошло еще немного времени с момента приобретения Okmeter. У нас есть идеи по тому, как мы изменим коммерческую модель. На данный момент это точно не будет связано с подорожанием [тарифов]. Детальной информации у меня сейчас нет. Но в ближайшие полгода-год позитивные изменения будут.Дмитрий: У меня еще вопрос к Илье. Действительно вы правы по поводу того, что многие приходят с достаточно слабым мониторингом. И мы, в принципе, относимся к таким же компаниям. Ваша команда очень часто рекламирует Okmeter. А вы не думали его сами продавать в комплекте со своими услугами, чтобы клиенту не приходилось куда-то ходить, заключать с кем-то договор?Илья: На самом деле мы так делали: продавали нашим клиентами Okmeter «в коробке» для простоты. Но часто был обратный запрос, и связан он с очень простой технической историей. Как уже прозвучало, Okmeter — это такой комбайн, который много чего другого обслуживает. Мы обслуживаем только БД, а есть еще инфраструктура, с которой работают админы. Довольно неудобно получалось, когда серверы в мониторинг добавлялись через нашу команду. Чисто технически это получалось довольно странно. Как правило, тем, кто это пробовал, нужно было мониторить и что-то, совсем не касающееся PostgreSQL. Поэтому мы теперь клиентов сразу перенаправляем к коллегам.Вячеслав: У меня есть вопрос на коммерческую тему. Раньше у Okmeter было две недели триального срока, что в принципе хорошо, чтобы оценить систему. А сейчас — неделя, и мне кажется, это маловато. Почему такие изменения произошли?Владимир: Изменения произошли чуть раньше того момента, когда Okmeter перешел к «Фланту». И про причины, почему случилось именно так, я вам не смогу ответить. Но я записал себе этот вопрос, мы с коллегами обсудим и подумаем. Возможно, вернутся старые две недели.Илья: У нас есть вопрос в чате по поводу обработки статистики pg_stat_statements: «Как лучше группировать статистику — по queryid, query text, по обработанному query text? И почему?»Алексей: Я пришел к тому, что лучше всего группировать по трем полям: имя БД, имя пользователя и queryid. Потому что queryid не уникальный, и появляются дубликаты. Чтобы эти дубликаты отсечь, нужно группировать еще по userid и dbid.Второй момент — по query text. С ним возникает та же проблема. Понятно, что в production, скорее всего, одинаковые запросы в разных базах данных не появятся. Но если какая-то инсталляция, где и production, и stage, и тестовая базы живут на одном оборудовании, или, например, собираются в одно хранилище, то можно получить одинаковые запросы. Причем одинаковые запросы, которые выполняются в разных базах, с разными пользователями. Нужно это учитывать.И третий пункт — по обработанному query text. Я так подозреваю, что здесь вопрос в том, что нужно как-то предварительно нормализовать этот текст, и уже от него отталкиваться. Тут тоже есть нюанс. Когда мы обрабатываем этот query, получаем какую-то новую сущность. И если раньше у нас была возможность группировки по имени пользователя, имени базы и по queryid, то теперь появляется еще четвертая сущность. И всё становится немного сложнее. Допустим, в мониторинге мы увидели этот обработанный query, и это какой-то идентификатор. Мы должны его сопоставить с оригинальным запросом, который находится в pg_stat_statements. И вот тут могут быть сложности для обратной трансляции. Поэтому на мой взгляд самый лучший вариант — это связка из трех полей: имени пользователя, имени базы и queryid.Вячеслав: Спасибо, Алексей. Я тоже сейчас стал использовать queryid. И когда общался с коллегами из Okmeter на стенде конференции HighLoad, мне рассказывали про специальную пост-обработку, которая позволяет получать статистику по двум похожим запросам, как-то объединить.Алексей: Да, они нормализуют запрос, и потом уже по этому нормализованному запросу отображают статистику. Вообще, нормализация pg_stat_statements не очень оптимальна. Например, запросы с разным количеством параметров в списке будут считаться отдельно. Хотя по факту это один и тот же запрос, просто количество параметров отличается. Они решили эту проблему через свою собственную нормализацию. Владимир: Хочу добавить, что у Okmeter появился свой чат. Если у кого-то есть вопросы по мониторингу, рады будем ответить. Дмитрий: У меня еще один вопрос. Есть такая клевая штука в Okmeter, которая называется как-то Prometheus-like-API. В моем случае она мне очень пригодилась. Там, где я сейчас работаю, довольно много разных мониторингов. И это удобно собирать с разных систем: что-то одна система лучше мониторит, что-то — другая. А у Grafana единый интерфейс, и там уже права проще раздать: этот может смотреть это, другой — то. Эта штука как-то будет поддерживаться и развиваться?Владимир: Это хороший вопрос, спасибо. Та функциональность, которая есть, точно будет сохранена, во всяком случае в ближайшее время. Во-вторых, то, о чем идет речь, если я правильно понимаю, — да, будет развиваться. В глобальном роадмапе есть планы настроить совместимость Okmeter с Prometheus. Чтобы его можно было использовать не только как полноценную систему мониторинга, но и как облачное хранилище для метрик, очень быстрое и очень дешевое. Оттуда можно будет читать и туда можно будет писать. По индустриальным стандартам это, скорее всего, будет Prometheus read и Prometheus write.Илья: Всем спасибо. И до новых встреч в эфире.P.S.Читайте также в нашем блоге:«Postgres-вторник №5: PostgreSQL и Kubernetes. CI/CD. Автоматизация тестирования»;«Как будет развиваться сервис мониторинга после его покупки „Флантом“»;«Обзор операторов PostgreSQL для Kubernetes. Часть 1: наш выбор и опыт».',\n 'Привет, Хабр! Мы — Тимофей Василевский, Сергей Дымашевский и Максим Чайка — только что окончили первый курс бакалавриата «Прикладная математика и информатика» в Питерской Вышке. В качестве семестрового проекта по C++ мы написали симулятор всем известной настольной игры Ticket to ride. Что у нас получилось, а что нет, читайте под катом.Наша версия игры. Снизу карты игрока, колода и дополнительные маршруты, справа — карты на столе и информация об игроках.Правила игрыСуть игры заключается в том, чтобы проходить железнодорожные перегоны между городами и выполнять маршруты. Каждый перегон имеет свой цвет, и, чтобы его построить, необходимо иметь достаточно карточек вагончиков такого же цвета. Построенный перегон приносит вам в конце игры какое-то количество очков, а сам маршрут считается выполненным, если существует непрерывный путь из ваших вагончиков из одного города в другой.Вот, что происходит у нас в игре при постройке перегона: красный игрок построил перегон между верхними станциями, а желтый — между нижней левой  и верхней правой:Более подробные правила игрыВ Ticket to ride могут играть от двух до пяти игроков. Каждый ход игрок выполняет одно из четырех действий: он может построить перегон, взять карты вагончиков со стола или из колоды, построить станцию или взять новые маршруты. Существуют специальные вагончики — локомотивы, их можно использовать вместо вагончика любого цвета, а также они требуются для построения некоторых перегонов. Станции строятся в городах и используются так: вы можете считать, что один из смежных к станции путей ваших соперников построен вами, и учитывать его, когда прокладываете маршрут. В конце игры подсчитываются очки: за каждый построенный маршрут вы получаете столько очков, сколько на нем написано, за каждый непостроенный — теряете столько же. Игрок, построивший самый длинный непрерывный маршрут, получает дополнительные очки.Полные правила смотри тут.Варианты двух маршрутов между Варшавой и ПарижемИдея написать именно такой проект казалась нам хорошей, потому что он вмещает в себя сложную логику и архитектуру, взаимодействие с интерфейсом, серверное взаимодействие. Собственно, так мы и поделили обязанности внутри проекта.Архитектура приложенияМы пользовались шаблоном проектирования Model-View-Controller (MVC), как он работает, можно почитать по этой ссылке. В нашем случае это было реализовано так:Есть графический интерфейс, который реагирует на нажатия пользователем на конкретные места на игровом поле, после чего передает эти действия в контроллер (вызывает функции контроллера).Действия контроллера зависят от того, играют ли пользователи по сети или локально. Если локально, то контроллер просто передает из графического интерфейса действие в удобном для модели игры формате. Если же игра осуществляется по сети, то контроллер посылает запрос через клиента. Он в свою очередь через сервер говорит игре, которая хранится только в одном экземпляре, информацию о действии, которое игрок хотел совершить.Модель игры принимает действие от контроллера. Почти всегда контроллер передает ей полиморфный класс Turn:struct Turn {\\npublic:\\n\\tstatic inline int num = 0;\\n\\tstatic void increase_num();\\n\\tvirtual ~Turn() = default;\\n};Он  имеет четырех наследников, каждый из которых соответствует своему типу хода:  struct DrawCardFromDeck final : virtual Turn {\\npublic:\\n\\texplicit DrawCardFromDeck();\\n\\t~DrawCardFromDeck() override = default;\\n};В модель игры передается один из наследников Turn:  void Game::make_move(Turn *t);\\nДальше происходит обработка, используя удобную конструкцию языка C++:\\nif (auto *p = dynamic_cast<DrawCardFromActive *>(t); p) {\\n\\tget_wagon_card_from_active_cards(p->number);\\n}Модель игрыСама модель игры устроена так, что там есть множество вспомогательных классов: Board, Deck, Player, Algo и т.д., каждый из которых отвечают за свою смысловую часть, а также главный класс Game, который связывает их между собой. Также модель содержит в себе бота, с которым при желании можно поиграть. Бот, как ни странно, делает ходы автоматически. Он высчитывает наилучшую стратегию, используя графовые алгоритмы, например, алгоритм Дейкстры. В будущем мы планируем обучать бот с помощью алгоритмов машинного обучения — так мы сможем значительно повысить уровень его игры.СерверМы реализовывали серверную часть, используя библиотеку gRPC, которая позволяет компоновать запросы с помощью «protocol buffers» и после этого передавать их буквально за пару команд. Конечно, на каком-то этапе у нас возникла проблема, потому что классы, которые нужны для передачи запросов, часто похожи на классы самой игры. Возможно, нужно было использовать именно эти классы, избежав парсинга одних классов в других. Однако мы все же решили оставить свои классы и переводить их в классы gRPC, потому что у наших собственных классов интерфейс гораздо более понятен и приспособлен к обработке логики, а перевести один класс в другой не так уж сложно. ttr::Route parse_to_grpc_route(const Route &route) {\\n\\tttr::Route n_route;\\n\\tn_route.set_begin(route.city1);\\n\\tn_route.set_end(route.city2);\\n\\tn_route.set_points(route.points_for_passing);\\n\\treturn n_route;\\n}Как работает наш сервер и клиентСервер внутри себя имеет указатель на контроллер, который является главным в текущей игровой сессии. Именно он обрабатывает запросы напрямую, не передавая их дальше. У сервера есть несколько методов, которые вызываются у клиента для получения какой-либо информации: make_turn, get_board_state, get_player_state, start_game и get_score — и еще несколько вспомогательных методов, которые в основном нужны для реализации этих. Также есть конструктор, который запускает сервер.Сервер ответственен за то, чтобы каждый игрок понимал, кто он есть: при подключении игрок получает себе уникальный номер и цвет, после чего, посылая запросы, указывает в них этот номер, из чего сервер делает вывод, может ли сейчас ходить данный игрок.Клиент, в свою очередь, тоже имеет несколько методов, которые можно вызывать из контроллера, чтобы удобно получать информацию и делать ходы. Внутри себя он конвертирует необходимую информацию в запрос, посылает его на сервер, после конвертирует ответ обратно и возвращает. Таким образом, внутри контроллера все выглядит очень просто и удобно, а вся неприятная часть с компоновкой запросов остается спрятана внутри кода клиента и сервера.std::vector<Path> TTRController::get_paths() {\\n   if (typeOfGame != type_of_game::LOCAL_CLIENT) {\\n       return game->board.paths;\\n   } else {\\n       throw_exception_if_server_disconnected();\\n       return client->get_paths();\\n   }\\n}Чтобы обрабатывать случаи, когда сервер неожиданно отключается по какой-либо причине, мы добавили исключения. Тогда пользователь не просто получит упавшее приложение, но и сообщение об ошибке и возможность вернуться в главное меню.  В то же время мы, к сожалению, не успели добавить обработку случая, когда неожиданно отключается клиент. Наш сервер не может спрашивать у клиентов никакой информации — он просто отвечает на запросы и не знает, кто к нему подключен. В целом проблема отключения игрока встает во многих играх, и с этим можно бороться по-разному:Заканчивать игру при отключении игрока;Подсвечивать игрока отключившимся и пропускать его ход;Сделать ограничение на ход по времени: если игрок не делает ход, то он его пропускает.Мы не могли реализовать первый и второй варианты в нашей архитектуре, поэтому думали насчет третьего. В итоге решили, что он будет очень неудобен для остальных игроков. Каждый ход может занимать достаточно много времени: надо продумать, какой перегон строить, как помешать соперникам и т.д., поэтому таймер пришлось бы ставить на слишком большое время, что будет сильно тормозить игру.В текущем состоянии в наш Ticket to ride можно играть по локальной сети или с одного компьютера нескольким игрокам. К сожалению, по глобальной сети поиграть не получится. Подключиться через VPN возможно, но из-за того, что запросы достаточно объемные, а VPN чаще всего работает через протокол UDP, игра сильно тормозит, и каждый ход делается по 5-10 секунд.Идеальным решением было бы поменять архитектуру. Например, сделать один глобальный сервер, который хранит игру, но сам не является игроком, а также имеет возможность делать запросы к подключившимся для проверки, что они в игре. Это позволило бы решить сразу несколько проблем:Возможность делать хорошую проверку на отключение игрока.Возможность сделать игру по глобальной сети. Для этого установим скрипт, создающий контроллеры на каком-то выделенном глобальном сервере и выдающий им их ip-адреса.Возможность делать обновление игры более удобным: главный контроллер просто посылал бы игрокам информацию об изменении в игровом состоянии, что позволило бы рисовать только эти изменения, а не перерисовывать всю доску каждый раз. Это, вероятно, решило бы и проблему с vpn: информации передавалось бы в разы меньше, и все стало бы работать гораздо быстрее.Но, к сожалению, продумать это вначале у нас не получилось, а изменять архитектуру после было слишком сложно, и времени нам просто бы не хватило.Графический интерфейсМы выбрали библиотеку QT как наиболее распространенную и имеющую обширную документацию. Кнопки сделали при помощи класса QGraphicsRectItem:Большинство объектов на поле — это вагончики типа QGraphicsPolygonItem, которые заданы координатами в специальном файле, что дает возможность сделать другую карту без изменения кода.Остальные элементы интерфейса — это кнопки.Весь текст принадлежит классу QGraphicsTextItem.Станции — это эллипс с совпадающими центрами и радиусами. К сожалению, они находятся на сцене не как объект, а как картинка. Чтобы построить станцию, на них нужно кликнуть дважды, так как проверяются все станции посредством определения координат даблклика.В основном интерфейс просто вызывает определенные функции у контроллера. Например, можно посмотреть свои маршруты, нажав на блоки слева от карты, при этом маршруты других игроков будут видны лишь по окончании партии. На картинке ход красного игрока, и он может увидеть свои маршрутыСправа от них на столе расположены карты, которые можно взять в руку. При нажатии на них будет видна небольшая анимация перемещения карты вниз экрана.Во время работы офлайн-игры код выполняется более-менее линейно. Когда же мы начали реализовывать часть для взаимодействия по сети, оказалось, что нужно получать изменения с сервера. Это не очень удобно, поскольку сервер не умеет просто сигнализировать всем клиентам, что произошли изменения. Решением этой проблемы стало обновление всего игрового поля у всех клиентов:void View::timed_redraw() {\\n   draw_board();\\n      redrawble = true;\\n   QTimer *timer = new QTimer();\\n   timer->setSingleShot(false);\\n   timer->setInterval(5000);\\n   connect(timer, &QTimer::timeout, [=]() {\\n             if(redrawble) {\\n                    draw_board();\\n                    // timed_redraw();\\n                    timer->start();\\n             }\\n   });\\n   timer->start();\\n}ЗаключениеПроекты на первом курсе — одна из самых содержательных и полезных вещей. Мы смогли ощутить на себе все прелести командной разработки (и попрактиковаться в использовании git’a), попробовать себя в интересной для области и, конечно же, дополнить свое резюме реальным проектом.Исходный код можно посмотреть здесь.  Другие материалы из нашего блога о проектах студентов младших курсов:Плагин для ранжирования кода по важности или как я пыталась облегчить жизнь программистамАнализатор C++ на первом курсе: миф, иллюзия или выдумка?Красиво? Очень! Как мы написали приложение для визуализации аттракторов',\n 'Всем привет! Меня зовут Андрей, я работаю системным архитектором в Arenadata. В этой статье расскажу, как и зачем мы сделали свой инструмент для обмена данными между Arenadata DB (аналитическая MPP-СУБД на базе Greenplum) и фреймворком для распределенной обработки данных Apache Spark (входит в экосистему Arenadata Hadoop). Поскольку Greenplum и Hadoop — это базовые элементы нашего стека, нам часто приходится сталкиваться с задачами обработки и передачи информации между ними. Ранее мы решали эту задачу частично с помощью Platform Extension Framework (PXF), но это лишает нас всех преимуществ, которые предлагает Spark, а они довольно существенны.Мы решили написать ADB-Spark Connector на базе HTTP-сервера, реализующего протокол gpfdist. Почему выбрали именно такой путь и на какой архитектуре в итоге остановились, расскажу ниже в статье. Немного теорииArenadata DB — это распределенная СУБД, построенная на базе Greenplum, которая относится к классу MPP-систем. MPP (massively parallel processing, массово-параллельные вычисления) — класс СУБД, распределяющих хранимые данные на множестве вычислительных узлов для параллельной обработки больших объемов данных. Каждый узел имеет собственное хранилище и вычислительные ресурсы, позволяющие выполнять часть общего запроса к базе. Логика разбиения таблицы на сегменты задается ключом (полем) дистрибуции. Для каждой отдельной колонки в таблице можно задать свой тип и уровень сжатия.MPP DB хорошо подходят в качестве аналитического хранилища данных, потому что:масштабируются горизонтально;хорошо держат OLAP нагрузку;позволяют загружать данные с большой скоростью.Рассмотрим способы, с помощью которых можно обмениваться данными с ADB:1.  C помощью JDBC-драйвера через мастер ADB. Этот способ доступен «из коробки» с помощью стандартного Spark JDBC-коннектора:2.  C помощью COPY на мастере:3.  C помощью распределенного COPY:4.  С помощью gpfdist — HTTP-сервера для параллельного обмена данными с сегментами GP:5.  С помощью HTTP-сервера, реализующего протокол gpfdist (1,2):Выбор основы для коннектораМы решили написать коннектор на основе HTTP-сервера, реализующего протокол gpfdist-a. Разработали две реализации протокола на базе:Akka-HTTP;Finagle.Версия с Finagle показала себя лучше при наличии множества одновременных сессий от сегментов ADB, поэтому остановились на ней.Теперь взглянем на эту задачу со стороны Apache Spark. Высокоуровневая архитектура процесса компиляции запросов Spark соответствует традиционному подходу СУБД, который заключается в преобразовании запроса в логический план, оптимизации логического плана в физический и его выполнении на рабочих узлах.Рассмотрим средства, предоставляемые Spark-ом для написания коннекторов к различным источникам данных.Data Source API был представлен в Spark 1.3 вместе с абстракцией Dataframe. Со времени этого релиза Spark претерпел большие изменения. С версией 2.0 пришла новая абстракция DataSet, и в связи с этим API был переосмыслен. Ограничения API V1:смешанное из API низкого и высокого уровня;cложно делать Push down операторов;сложно передавать информацию о партициях;нет поддержки транзакционной записи;нет поддержки колоночного режима;нет поддержки стриминга.С версией 2.3 выпустили и новый API, известный как Data Source V2. В ней нет вышеперечисленных ограничений, и её характерной особенностью является переход Scala-трейтов к Java-интерфейсам для лучшей совместимости.Рассмотрим подробней, что происходит с точки зрения Data Source API v2 на каждом из представленных этапов. Источник данных представлен в логическом плане экземпляром DataSourceV2Relation. Во время планирования DataSourceV2Strategy преобразует отношение в экземпляр DataSourceV2ScanExec. Последний создает экземпляр DataSourceRDD, который используется для фактического параллельного вычисления результатов из нескольких разделов.Логический план — на этом этапе цель состоит в том, чтобы создать DataSourceV2Relation из опций, предоставленных клиентом. Процесс инициируется загрузкой Dataset-а.Физический план — на этом этапе цель состоит в том, чтобы преобразовать DataSourceV2Relation в DataSourceV2ScanExec. Процесс инициируется выполнением действия с Dataset-ом, созданным на предыдущем этапе.Выполнение запроса — на этом этапе цель состоит в том, чтобы получить данные из партиций RDD, созданного DataSourceV2ScanExec, и собрать строки из всех разделов.На основании вышеизложенного, было принято решение реализовать коннектор с помощью Datasource API v2.АрхитектураКаждое Spark-приложение состоит из управляющего процесса-драйвера и набора распределённых рабочих процессов-исполнителей. Общая компонентная диаграмма взаимодействия Spark-приложения и Greenplum кластера:Чтение данныхЗагрузка данных в Spark из Greenplum состоит из нескольких этапов:Инициализация Spark-драйвера.Spark-драйвер при помощи JDBC устанавливает соединение с мастером Greenplum для получения необходимых метаданных о кластере и таблице:количество активных сегментов;схема и тип таблицы;ключ распределения таблицы;план запроса.В зависимости от выбранной стратегии партиционирования (речь о которых пойдет далее) происходит вычисление количества партиций, а также генерация условий, используемых при загрузке данных с сегментов в партиции.На каждый Spark-исполнитель назначается задача по обработке данных  и соответствующая ей партиция.Обмен данными происходит с помощью механизма writable-внешних таблиц, одновременно для каждого сегмента. На текущий момент реализованы следующие стратегии партиционирования:по gp_segment_id: данные считываются и распределяются по партициям Spark в соответствии с их распределением по сегментам в Greenplum; по указанной колонке и указанному количеству партиций: для указанной колонки запрашиваются минимальное и максимальное значения и генерируются условия считывания данных из сегментов. Таким образом в Spark-партиции загружаются данные соответствующего диапазона;только по указанной колонке: данные разбиваются в соответствии с уникальными значениями указанной колонки. В таком случае количество Spark-партиций соответствует количеству уникальных значений указанной колонки;только по указанному количеству партиций: данные разбиваются согласно некоторой хеш-функции (по умолчанию) на указанное количество партиций;по указанной хеш-функции и указанному количеству партиций: аналогично предыдущему пункту, с указанием желаемой хеш-функции. Запись данныхВыгрузка данных из Spark в Greenplum происходит в несколько этапов:Инициализация Spark-драйвера.Spark-драйвер при помощи JDBC устанавливает соединение с мастером Greenplum.В зависимости от режима записи Spark-драйвер производит инициализирующие действия (создание или очистка таблиц) в Greenplum-е для загрузки.Каждый из Spark-исполнителей выгружает данные из назначенной ему партиции в Greenplum.Обмен данными происходит с помощью механизма readable-внешних таблиц, одновременно для каждого сегмента. На текущий момент поддерживаются следующие режимы записи:перезапись (overwrite): либо целевая таблица удаляется полностью и пересоздается, либо происходит truncate;добавление (append): данные добавляются в целевую таблицу;запись в новую таблицу: завершение с ошибкой, если целевая таблица уже существует (errorIfExists).Резюмируем функциональные возможности:чтение данных из Greenplum;запись данных в Greenplum с помощью различных режимов записи:overwrite;append;errorIfExists;автоматическое формирование схемы данных;гибкое партиционирование;дополнительные опции при создании целевой таблицы в Greenplum;поддержка push-down операторов:отсекание колонок;push-down фильтров.извлечение дополнительных метаданных из Greenplum:схема распределения данных;статистика.оптимизация выполнения count-выражений в запросе;выполнение произвольного SQL через мастер Greenplum;поддержка пакетного (batch) режима при загрузке в Spark.АналогиНа текущий момент существует Spark Greenplum-коннектор от Pivotal. Мы провели сравнительный анализ функциональных возможностей и производительности:Таким образом, нам удалось создать высокопроизводительный двунаправленный Spark-ADB коннектор, который, в свою очередь, обладает рядом функциональных преимуществ относительно аналога от Pivotal. Подробное описание коннектора с примерами использования можно посмотреть в официальной документации. ',\n 'ПроблематикаДля того чтобы управлять командой сотрудников необходимы чёткие KPI показатели, которые будут служить точным ориентиром в оценке эффективности их работы. Если в продажах все просто (лиды, сделки, конверсия, ROI и пр. показатели), то в процессной работе программистов, маркетологов, аналитиков, дизайнеров так четко измерить эффективность будет сложно т.к.:Если в вашей команде более 1 человека - встаёт проблема, связанная со сложностью того, чтобы собрать в кучу всю информацию, в рамках оцениваемой продуктивности сотрудников.Их результаты работы часто трудно оцифровать, а если и выбраны kpi то обработка данных, их учёт и ежедневный контроль может отнимать огромное количество рабочих часов.Как решить первую задачу, не покупая лицензии agile платформ, я рассказал в своей предыдущей статье, а вот о том, как это все считать и наглядно визуализировать для оценки в общей рабочей среде мы поговорим ниже.Решение задачиВ своей работе мы будем использовать таскменеджер Trello и пакет trelloR на языке программирования R. Почему именно такой \"стек\" можно ознакомиться тут>>Перед тем как начать оценивать продуктивность выберем для себя основные kpi. Я буду оценивать продуктивность команды аналитиков и ключевыми показателями будут служить:Объем решаемых задач за периодСкорость решения задачЧисло задач закрытых в день постановкиЧисло закрытия \"сложных\" задач за период *Это индивидуально мои метрики, которые важны в моей работе. Оценивать продуктивность, описываемым в этой статье методом, можно в любом разрезе, который только может прийти вам в голову и исходя из индивидуально вашего бизнес-процесса.И так, сначала устанавливаем среду разработки и ключевые пакеты необходимые для работы. Для решения вышеописанной задачи нам потребуется установить пакеты для:работы с API Trello — «trelloR»работы со временем и временными периодами — «lubridate»работы с таблицами и агрегации данных — «dplyr»работы с визуализацией — «ggplot2»сбор нескольких визуализаций в одну — «ggpubr»Чтобы установить пакеты из основного репозитория CRAN примените базовую функцию install.packages, а для загрузки пакетов с github функцию install_github:remotes::install_github(\"jchrom/trelloR\")\\ninstall.packages(\"lubridate\", dependencies = TRUE)\\ninstall.packages(\"dplyr \", dependencies = TRUE)\\nremotes::install_github(\"jchrom/trelloR\") \\ninstall.packages(\"ggplot2\", dependencies = TRUE) \\ninstall.packages(\"ggpubr \", dependencies = TRUE)Для отправки запросов к API Trello нам потребуется токен, как его получить можно прочесть тут>>*Еще одним важным шагом перед визуализацией является стандартизация всех досок сотрудников в рамках ваших бизнес-процессов. В своей работы мы внедрили:Единый набор меток для каждой из задач в Trello, разделив их по \"срочности-важности\" и по направлениям деятельности в рамках задачиМетки в карточках TrelloОбязательное заполнение даты начала и даты окончания выполнения каждой из задачНастройка даты в карточке TrelloПеренос всех выполненных задач в столбцы с единым форматом и указания месяца выполнения задачиК сожалению, без предварительной стандартизации всех досок выполнить дальнейший анализ будет невозможноПолучаем данные для визуализации# Подключаем все необходимые пакеты для работы\\nlibrary(trelloR)\\nlibrary(lubridate)\\nlibrary(dplyr)\\nlibrary(ggplot2)\\nlibrary(ggpubr)\\n\\n# Указываем путь к папке где лежит полученный ранее token\\nsetwd(\"C:\\\\\\\\*********\\\\\\\\R_script\\\\\\\\trello\")\\n\\n# Собираем в список доски сотрудников, которых мы будем анализировать\\nboard_id <- c(\"https://trello.com/b/wNV1HMse/максим-м\",\\n             \"https://trello.com/b/tXrO68ix/максим-я\",\\n             \"https://trello.com/b/fCZrgJYg/арсений\",\\n             \"https://trello.com/b/HeLC3RVE/анатолий\",\\n             \"https://trello.com/b/xweaki49/софия\")\\n\\n# Собираем циклом статистику со всех досок в общую таблицу\\n\\nstattrello <- data.frame() # будущая сводная таблица\\n\\nfor (xi in 1:length(board_id)) {\\n\\n  cardList <- get_board_lists(board_id[xi], query = list(customFieldItems = \"true\"))\\n  cardList$zadach <- NA # число задач за каждый период\\n  for (i in 1:length(cardList$id)) {\\n    cardList$zadach[i] <- nrow(get_list_cards(cardList$id[i]))\\n  }\\n  cardList$spec <- stringr::str_to_title(basename(board_id[xi]))\\n  cardList <- cardList %>% select(id, spec, name, zadach)\\n  stattrello <- bind_rows(stattrello,cardList)\\n}Собираем списки карточек всех сотрудников и сводим их в единую таблицу для получения ID столбцов по периодам и числа выполненных задач за данные периоды:# Фильтруем сводные данные только по задачам за последние 3 месяца.\\n# (эффективность по основным kpi будет оцениваться в разрезе этого периода)\\nlid <- stattrello %>% filter(grepl(\"20\",stattrello$name)==T)\\nlid$name <- parse_date_time(lid$name, \"my\")\\nlidvse <- lid\\nlid <- lid %>% filter(name > floor_date(today() - months(3), \"month\"))\\n\\n# Используя ID списков с карточками собираем задачи по каждому списку\\nallvcard <- data.frame() # Все задачи всех сотрудников\\ntype_zadach <- data.frame() # Типы задач по направлениям\\n\\n#Запускаем цикл для сбора\\nfor (i in 1:nrow(lid)) {\\ncards_v <- get_list_cards(lid$id[i])\\ncards_v$spec <- lid$spec[i]\\ncards_v$datezad <- lid$name[i]\\n\\nif(length(bind_rows(cards_v$labels))!=0){\\n\\ntmp <- bind_rows(cards_v$labels) %>% group_by(name) %>% summarise(ЧислоЗадач = n())\\ntmp$spec <- max(cards_v$spec)\\ntmp$datezad <- max(cards_v$datezad)\\ntype_zadach <- bind_rows(type_zadach,tmp)\\n\\ncards_v <- tidyr :: unnest_longer(cards_v,labels)\\ncards_v <-  rename(cards_v$labels, Naprav=3) %>% select(Naprav) %>%\\n  bind_cols(cards_v) %>% select(id, due, Naprav,spec)\\n\\n# В поле ID зашифрована дата создания карточки. Проводим расшифровку для всех задач\\ncardID <- cards_v$id[i]\\ndateList <- data.frame(dateadd = NA)\\ndateList[1,1] <- strtoi(strtrim(cardID, 8), 16L)\\ndateList$dateadd <- as.POSIXct(dateList$dateadd, origin = \"1970-01-01\")\\n\\ndateList <- data.frame(dateadd = NA)\\nfor (i in 1:length(cards_v$id)) {\\n  cardID <- cards_v$id[i]\\n  dateList[i,1] <- strtoi(strtrim(cardID, 8), 16L)\\n}\\ndateList$dateadd <- as.POSIXct(dateList$dateadd, origin = \"1970-01-01\")\\n\\ncards_v[\"dateadd\"] <- dateList$dateadd\\n\\n# Вычисляем время от создания задачи до плановой даты её завершения\\ncards_v <- cards_v %>% bind_cols() %>% data.frame(\\n  raznica = trunc(as.numeric(as.POSIXct(cards_v$due) - cards_v$dateadd)/24)) %>%\\n  select(-due)\\n\\nallvcard <- bind_rows(allvcard,cards_v)\\n}else{print(\"no\")\\n}}После выполнения всех предварительных манипуляций с данными, мы можем приступить к оформлениюВизуализацияВыводим цифры в среду визуализации используя пакет ggplot2 и строим графики по каждому из показателей:# Задаем единый стандарт отображения данных (стиль графиков)\\ntema <- theme(legend.position=\"right\",axis.text=element_text(size=7),\\n              axis.title=element_text(size=10,face=\"bold\"),\\n              legend.text=element_text(size=10),\\n              legend.title = element_text(size=9),\\n              plot.title = element_text(size=10),\\n              plot.margin = margin(5, 20, 5.5, 5))\\n\\n# Строим график по общему числу решенных задач за период\\n# Переводим даты в стандартный вид из того что были в списках Trello\\nstattrello$id <- as.character(as.Date(floor_date(as.POSIXct(strtoi(strtrim(stattrello$id, 8), 16L), origin = \"1970-01-01\"),\"month\"))+1)\\n\\n# Подготавливаем столбцы к отображению на графике\\nstattrello <- stattrello %>% rename(date=id) %>% group_by(spec,date) %>% summarise(zadach = sum(zadach))\\n\\n# Строим график\\ngrafik_vse <- ggplot(data=stattrello, aes(x=as.character(date), y=zadach)) +\\n  scale_fill_discrete(name = \"Сотрудники\" ) +\\n  geom_bar(aes(fill=spec), position = position_dodge(), stat=\"identity\") +\\n  geom_text(aes(label=zadach, y = zadach +1, group = spec),\\n            position = position_dodge(0.9), size=2.5) + \\n  theme(legend.position=\"bottom\",axis.text=element_text(size=7),\\n        axis.title=element_text(size=10,face=\"bold\"),\\n        legend.text=element_text(size=10),\\n        legend.title = element_text(size=9),\\n        plot.title = element_text(size=10),\\n        plot.margin = margin(10, 50, 20, 10)) +\\n  labs(title = \"Закрыто задач в прошлом периоде\",\\n       x = \"Дата\",\\n       y = \"Задач, шт.\")\\nВ результате получаем график по числу выполненных задач на каждого из сотрудниковОтчет по числу задач на сотрудника по месяцамПолучаем графики и таблицы для остальных интересующих нас показателей:# Формируем график по задачам для направлений деятельности\\ntype_zad <- type_zadach %>% filter(grepl(\"ВАЖНО|Пауза\", name)!=T) %>%\\n  mutate(date = format(datezad, \"%m-%Y\")) %>%\\n  group_by(spec,name) %>%\\n  summarise(nwork= sum(ЧислоЗадач))\\n\\ntyp_p <- ggplot(data=type_zad, aes(x=name, y=nwork, group=spec)) +\\n  scale_fill_discrete(name = \"Сотрудники\" ) +\\n  geom_bar(aes(fill=spec), position = position_dodge(), stat=\"identity\") +\\n  geom_text(aes(label=nwork, y = nwork+1, group = spec), \\n            position = position_dodge(0.9), size=2.5) +\\n  tema +\\n  labs(title = \"Задачи по направлениям за 3 мес.\",\\n       x = \"Дата\",\\n       y = \"Задач, шт.\") График по \"фокусу\" в направлениях деятельности для каждого из сотрудниковОтчет по направлениям работы*Получение графиков и таблиц остальных kpi показателей происходит аналогично выше описанному методу визуализации данных. Дабы не мучать вас кодом, ограничимся на этих примерах=)После того как мы собрали нужные нам графики их необходимо соединить в один файл - дашборд:# Объединение визуализации\\nggarrange(grafik_vse,\\n          ggarrange(typ_p, p1,\\n          ncol = 2, nrow = 1,\\n          heights = c(1, 1),common.legend = T,legend = \"bottom\"),\\n          nrow = 2,legend = \"right\")\\n\\n# Сохраняем полученный файл для загрузки в Trello\\nggsave(\"sotrudniki.png\", width =25, height = 20, units = \"cm\", device=\"png\")Итог сведения графиков:Сводный отчет по группе показателейТеперь визуализацию передаем в 1-ый список на нашем общем дашборде, который мы создавали в прошлой статье.#Сначала удаляем старые визуалы если они там были\\nbid = get_id_board(\"o****W\")\\n# 1ый лист в доске куда пишем наши графики\\nlid <- get_board_lists(bid)$id[1]\\ncid <- get_list_cards(lid)\\n\\n# Удаление\\ndelete_resource(resource = \"card\", id = cid$id[1])\\ndelete_resource(resource = \"card\", id = cid$id[2])\\n\\n# Создаем новые карточки в которые после добавим графики\\npayload = list(\\n  idList = lid,\\n  name = \"Показатели работы\",\\n  pos = \"bottom\"\\n)\\ncreate_resource(\"card\", body = payload)\\n\\npayload = list(\\n  idList = lid,\\n  name = \"Сводка\",\\n  pos = \"bottom\"\\n)\\ncreate_resource(\"card\", body = payload)\\n\\n# Снова обращаемся к этому листу с уже новыми но пока пустыми карточками\\nbid = get_id_board(\"o****W\")\\nlid <- get_board_lists(bid)$id[1]\\ncid <- get_list_cards(lid)\\n\\n# Добавляем наши сводные графики\\nadd_card_attachment(cid$id[1], file = \"sotrudniki.png\", cover = TRUE)\\nadd_card_attachment(cid$id[2], file = \"zadachi.png\", cover = TRUE)Дашборд департаментаПо итогу на общей доске появились сводные графики по тем KPI которые важны для ежедневного контроля. При нажатии на изображения в Trello можно сразу раскрыть превью и оперативно оценить как идут дела в рамках необходимых показателей:Скрин из Trello карточки с графикамиДалее настраиваем автоматический запуск скрипта для актуализации дашборда и задаём нужное количество раз обновлений в день.  ИтогПолученные данные, карточки и задачи расположены на одном листе и для оценки эффективности департамента не нужно открывать +100500 разных досок. Такой подход экономит кучу времени и делает контроль за исполнением задач оперативным, а работу с сотрудниками эффективнее.При желании, можно анализировать и делать выводы о том, кто с какой эффективностью решает задачи с детализацией до каждого сотрудника, создавая им свои мини отчеты.Используя оценочные коэффициенты по каждому из KPI, можно формировать стандарты работы, в рамках любого периода, наблюдать динамику и принимать решение о дополнительном премировании или депремирование сотрудников на основе близких к объективным* данных.*Для повышения точности оценки сотрудников, перед построением автоматизации необходимо как следует продумать критерии \"сложных\" задач, чтобы присвоение и закрытие таких карточек контролировалось руководителем и не происходила чрезмерная \"переоценка\" важности-сложности работ.Но это уже совсем другая история =)П.С.: Буду признателен комментариям и конструктивной критике кода. ',\n 'Примеры FPGA-проектов на базе Nvidia Jetson, Xilinx KRIA и Zynq UltraScale+Давайте попробуем оптимизировать самый времязатратный этап разработки устройств на базе ПЛИС — отладку прошивки. В этой статье мы расскажем о принципе 20/80 при планировании времени, рассмотрим инструменты для отладки FPGA, вспомним Гордона Мура и Уинстона Черчилля (да-да), затроним отладку сложных распределенных систем и внешних интерфейсов, а в конце — разберемся с типичными ошибками и поделимся полезными практическими советами.Для начала рассмотрим типовой цикл программирования и моделирования FPGA-прошивки:Все знакомы с классическим итерационным циклом разработки прошивки:Разработка FPGA-прошивки, а по факту — использование стандартных блоков и собственно разработка одного-двух специфических блоков.Моделирование — проверка базовой функциональности устройства. Синтез.Имплементация. Прошивка ПЛИС. Тестирование, которое чаще всего превращается в отладку. После каждого из первых трех этапов, в том числе и после синтеза, в классическом цикле разработки предусмотрен этап моделирования. На практике моделирование синтезированного проекта — это по сути проверка корректности работы синтезатора, то есть на этом этапе мы не можем выявить для себя что-то новое. В целом результат поведенческого моделирования не должен отличаться от моделирования после этапа синтеза, если синтезатор правильно понял код на одном из языков описания аппаратуры. Моделирование после стадии имплементации позволит проверить, насколько дизайн выполняет заданную функцию с учетом реальных таймингов и раскладки внутри кристалла. Если уделить достаточно времени моделированию RTL-описания, то можно будет сэкономить времени на этапе отладки. Принцип 20/80 для разработки и отладки FPGA-проектаКак показывает наш опыт и опыт коллег по FPGA-разработке, моделирование и отладка обычно занимают бо́льшую часть времени на проекте. Можно ориентироваться на закон Парето: порядка 20% времени уходит на саму разработку, написание кода, реализацию верхнего уровня дизайна в виде блок-диаграммы и порядка 80% — на тестирование, отладку и поддержку. Диаграмма «Распределение времени на разработку и отладку FPGA-проекта»С чем связано такое суровое распределение на 20/80? Отчасти это обусловлено самим итерационным процессом разработки. Каждая итерация занимает много времени, большие проекты требуют много времени на этапе пересборки. Например, час-полтора минимум для типового проекта для ПЛИС Xilinx. Есть и более крупные проекты, которые могут собираться гораздо дольше. Для успешного завершения процесса отладки от FPGA-разработчика требуется не только навык написания прошивки на одном из HDL-языков или создания его блок-дизайна, но еще и навыки программирования на других языках. Например, Си и Python для написания некоторых тестовых скриптов. Также может понадобиться и такой скриптовый язык как Tcl.А теперь давайте разбираться с тем, как сократить этап отладки, который стоит в нашем рабочем процессе на последнем месте, но занимает основную часть времени проекта.Типы инструментов для отладки FPGAУ программистов — высокоуровневых или embedded — для отладки есть большой инструментарий: дебаггеры, возможность пошагово исполнять программный код, широкие возможности по логированию работы программы. А вот у FPGA-разработчиков для отладки своих ПЛИС набор инструментов ограничен. Итак, что у нас есть: Измерительное оборудование: осциллографы и логические анализаторы. Эти инструменты позволяют понять, что происходит за пределами ПЛИС, но не дают заглянуть внутрь. С их помощью можно отладить внешний интерфейс и посмотреть, как ПЛИС взаимодействует с внешним миром. Встроенные средства отладки позволяют заглянуть внутрь ПЛИС. Такие инструменты есть у всех популярных производителей FPGA и называются они по-разному: у самого известного производителя, Xilinx, – это ChipScope, у Intel (ex. Altera) – SignalTap, у Microchip (ex. Microsemi) – продукт от Synopsys, который называется Identify RTL Debugger. Встроенные отладчики схожи по функциям. Примеры встроенных отладчиков компаний Xilinx, Intel (ex. Altera) и Microchip (ex. Microsemi)Ограничения встроенных отладчиковРазрушающий метод контроляПри использовании встроенных отладчиков важно помнить, что использование ChipScope, SignalTap и Identify RTL Debugger  — это разрушающий метод контроля. При добавлении отладчика внутрь FPGA-прошивки мы нарушаем логику ее работы: занимаем часть ресурсов ПЛИС, за счет этого меняется раскладка проекта внутри кристалла, меняются временные параметры, и в результате мы получаем не на тот продукт, который был до момента добавления отладчика. Выбор «Отсчеты vs сигналы» Встроенный отладчик использует для своей работы ресурсы ПЛИС — блочную память, ресурсы памяти, поэтому перед нами возникает необходимость выбора: мы можем посмотреть много отсчетов, но при этом будем видеть мало сигналов, ИЛИ мы хотим просмотреть много сигналов, но при этом сократится количество отсчетов. Перед разработчиком FPGA всегда стоит проблема выбора: отсчеты или сигналы. Приходится определять оптимальный набор наблюдаемых сигналов и оптимальное время наблюдения. Простой пример: мы хотим посмотреть 4096 отсчетов на частоте 200 МГц, это всего 20 мкс реального времени. А за 20 мкс реального времени иногда можно увидеть часть процесса, который мы пытаемся отладить. И если мы при этом хотим пронаблюдать пару 512 разрядных шин, это потребует 4 Мбита памяти ПЛИС, что для некоторых кристаллов составляет большой процент от ее общего доступного объема.Пошаговое исполнение Есть еще одна интересная фишка, но она доступна только в отладчике Synopsys для ПЛИС Microchip (ex. Microsemi) — так называемое пошаговое исполнение, к которому привыкли разработчики высокоуровневого ПО. Эта фича заявлена как прогрессивное решение, хотя сам отладчик далеко не новый. Отладчик Synopsys позволяет остановить исполнение прошивки в ПЛИС в определенный момент и дальше продолжать исполнение кода такт за тактом. Интересное решение, но у отлаженного проекта в этом случае появляется масса ограничений:На практике более-менее качественно можно отлаживать проект, состоящий из одного домена синхронизации. Для большого проекта такая фишка уже не актуальна. Если в проекте есть внешние интерфейсы, которые привязаны к реальному времени (даже самый банальный UART), то в момент остановки исполнения, естественно, мы нарушаем всю коммуникацию с внешним устройством и после одной такой остановки дальнейшее пошаговое исполнение теряет смысл.Вывод: любопытный инструмент, но нам пока не удалось найти ему качественное применение на практике. Мур, Черчилль и правила отладки FPGAПрежде чем отправиться дальше, хотим поделиться двумя полезными правилами, которые помогут спланировать этап отладки FPGA и не отчаяться в процессе. :-) Первое эмпирическое правило напоминает всем знакомый закон Мура: «Каждая последующая ошибка находится в два раза дольше предыдущей». Эта информация полезна для оценки времени, которое необходимо заложить для отладки и развития проекта. Чем обусловлен такой рост времязатрат? Каждая последующая ошибка возникает при гораздо более сложных начальных условиях, а для повторения этих условий требуется гораздо больше времени. Наш опыт показывает, что этот принцип работает: одну ошибку ищем в течение дня, следующую ошибку будем искать как минимум в течение двух дней.Второе эмпирическое правило — это видоизмененное высказывание, которое приписывают Уинстону Черчиллю. Он якобы говорил, что успех — это умение двигаться от одной неудачи к другой, не теряя энтузиазма. Для FPGA-разработчика отладка – это умение двигаться от одной ошибки к другой, не теряя энтузиазма.  Последняя часть высказывания про энтузиазм — самая важная, потому что отладка устройства на базе ПЛИС — это постоянная эмоциональная борьба с прошивкой. После каждой твоей победы прошивка как бы говорит: «Нет, товарищ, ты рано расслабился, у меня есть еще одна ошибка, и ты будешь ее искать в два раза дольше, чем предыдущую». Отладка распределенных FPGA-системПри отладке простых систем с одной платой и одной ПЛИС сложности есть, но они решаемы, а вот отладка распределенных систем — это другое дело. Под распределенными системами мы понимаем два типа систем: гетерогенные и гомогенные. Для гетерогенных систем правила отладки примерно такие же: каждая плата выполняет свою функцию, мы отлаживаем каждую в отдельности, а затем — в составе системы. А теперь пример с гомогенными системами: рассмотрим VPX-модуль на 12 слотов. Давайте представим, что туда вставлены 10 слотов с FPGA, 1 слот с коммутатором PCI Express и 1 слот — с CPU, который все эти данные агрегирует, обрабатывает и передает дальше. Такая система по своей структуре является гомогенной, потому что каждый модуль выполняет одинаковую функцию. На фото слева — VPX-система на 12 слотов, справа — ПЛИС-модуль, который в нее вставляетсяВ случае отладки гомогенных системы мы сталкиваемся с эффектом масштабирования: если мы отладили одну плату, прогнали все тесты и видим, что она работает, то с учетом эффекта масштабирования это не гарантирует, что так будет работать система на двух платах. Когда мы отладили систему на двух, это не гарантирует, что так будут работать все три платы и т. д. — пока мы не заставим систему корректно работать на всем массиве нужных плат. Сложности при отладке распределенных системНевозможность подключиться ко всем ПЛИС одновременно. Если у нас большое число FPGA-устройств, мы не можем одновременно подключиться к встроенному отладчику через JTAG. Первая проблема: у вас может не оказаться 10 компьютеров для подключения к каждому модулю. Вторая проблема характерна для VPX-систем, в них FPGA-устройства соединены в одну JTAG-цепочку и, соответственно, вы просто технически не можете подключиться ко всем ПЛИС одновременно. Недетерминированность возникновения ошибки, т. е. ошибка может равновероятно возникнуть в любом из 10 или N модулей. Это существенно осложняет процесс отладки. Во-первых, необходимо повторить ситуацию, вызвавшую ошибку. Во-вторых, мы должны взвести встроенный отладчик на ту плату, на которой надеемся увидеть ту самую ошибку, т. е. здесь у нас нет никаких инструментов, кроме везения и многократного запуска тестов, пока ошибка не возникнет там, где мы ее ждем. Сложный критерий детектирования ошибки — третья сложность, с которой мы сталкиваемся при отладке сложных распределенных FPGA-систем и не только. Часто ошибку невозможно детектировать внутри ПЛИС (с точки зрения ПЛИС данные идут корректные), но когда мы эти данные передаем дальше на обработку, например, сигнальному процессору, он может детектировать в них ошибку. Даже если он с помощью каких-то сигналов обратной связи сообщит об этом ПЛИС (запустим в этот момент встроенный отладчик), то мы не увидим ошибки, потому что событие произошло достаточно давно на временной шкале в прошлом, и причину возникновения ошибки мы определить не сможем.Как быть? В этом случае при отладке можно добавить внутрь основного устройства дополнительные блоки, которые контролируют входные данные. И порой сложность этих контролирующих блоков может сравниться со сложностью блоков, которые мы разрабатываем для имплементации в FPGA.Отладка внешних интерфейсовЛюбое FPGA-устройство так или иначе взаимодействует с внешним миром, поэтому мы не можем обойти стороной отладку внешних интерфейсов. Блоки прошивки FPGA взаимодействуют между собой и со сторонними устройствами, ведь если блок не имеет внешних связей, то на этапе синтеза он будет абсолютно резонно удален из проекта — для оптимизации. Анализатор PCIe протокола LeCroy (очень дорогое оборудование, облегчает отладку в разы)В рамках идеологии блочного дизайна блоки взаимодействуют с помощью стандартных интерфейсов: для Xilinx — это AXI Memory mapped и AXI Stream, для Intel FPGA это будут похожие интерфейсы Avalon MM и Avalon Stream, также используется стандартная шина APB. С точки зрения отладки, это достаточно простые интерфейсы с ограниченным числом состояний и управляющих сигналов, поэтому серьезных проблем возникнуть не должно. Тем более у некоторых производителей для нее есть готовые решения: например, у Xilinx есть специализированные блоки, которые позволяют на лету проверять корректность протокола и детектировать ошибки на шине. Благодаря такому блоку можно зафиксировать момент ошибки, а затем при помощи встроенного отладчика посмотреть, что к этому привело. Совсем иначе дело обстоит, когда один из интерфейсов используется в качестве транспорта для более высокоуровневого протокола. Например, низкоуровневое ядро PCI Express, которое работает на уровне TLP-пакетов и в качестве транспорта для передачи этих пакетов использует стримовый интерфейс AXI. AXI Stream — это стандартный интерфейс, мы понимаем, как он работает, но у нас нет возможности его отлаживать и следить за корректным формированием TLP-пакетов, которые мы передаем через AXI Stream в ядро PCI Express.При программировании и последующей отладке таких блоков разработчик ПЛИС сталкивается с проблемами, которые невозможно решить без применения специализированного дорогостоящего оборудования (в случае с PCI Express это будет анализатор протокола PCI Express). Без такого оборудования отладка превращается в работу с черным ящиком. Максимум, что вам доступно – это проверка соблюдения требований интерфейса AXI Stream. Поэтому если в проекте предстоит работа с такими интерфейсами, как PCI Express, 10/100G Ethernet или боле специфическими интерфейсами типа Infiniband, то при планировании проекта нужно учитывать, что на этапе отладки понадобятся соответствующие анализаторы, которые существенно упростят процесс работы. В результате тот же контроллер PCI Express перестанет быть черным ящиком, потому что мы сможем проанализировать поведение шины PCIe. Если у нас нет доступа к специальному оборудованию для анализа и отладки, можно использовать поведенческие модели. Например, модель для контроллера PCI Express. Однако такая модель позволит нам получить лишь малый процент покрытия всех возможных ситуаций, с которыми ваше разрабатываемое устройство столкнется в реальной жизни. При попытке обеспечить больший процент покрытия моделирование может затянуться на неопределенное время, а проект не будет реализован.Итак, вот три проблемы, с которыми сталкиваются FPGA-разработчики при отладке внешних интерфейсов:Невозможность корректировки внешних интерфейсовНеобходимость использовать дорогостоящее оборудование для отладки: Анализаторы PCIe и 10/100G EthernetОсциллографы 10—25 Гб/с с функцией анализа трафикаМодели черных ящиков не покроют 100% функциональности, особенно в части real-timeТри типичные ошибки процесса отладки FPGAА теперь рассмотрим типичные ошибки на этапе отладки. Если вам удастся их избежать, вы сэкономите массу проектного времени. Ошибка №1. Внесение двух и более исправлений за одну сборкуУчитывая, что сборка проекта может занимать несколько часов (а то и сутки), то возникает соблазн исправить сразу несколько замеченных ошибок. Но, как показывает практика, особенно в части сложных проектов, попытка сделать два и более исправления за раз чаще приводит к тому, что поведение отлаживаемой системы становиться еще более непредсказуемым, чем до внесения исправлений. Это происходит из-за сложной логической связанности — как между блоками, так и внутри блока. В результате приходится откатываться назад и последовательно проверять одно исправление за другим. Решение этой проблемы достаточно простое: несмотря на длительное время пересборки проекта необходимо двигаться step by step – одно исправление, затем сборка и проверка, следующее исправление — снова сборка и проверка и т. д. Так вы сэкономите больше времени и усилий.Ошибка №2. Ложная уверенность в верности алгоритма и работоспособности блока на основе анализа кодаКак это происходит? Разработчик смотрит на код, понимает, как он должен исполняться, и даже не пытается в это место добавлять встроенные отладчики, чтобы смотреть, что там происходит, «ведь по коду же видно, что все должно работать».Возьмем классическую ошибку: различная разрядность шин. Если два сигнала разной разрядности попытаться сравнить в логическом операторе типа if в языке VHDL, то с точки зрения синтаксиса и визуально все будет выглядеть нормально, однако синтезатор это сравнение преобразует в вечное false и, соответственно, алгоритм работать не будет.С аналогичной ошибкой мы столкнемся в языке Verilog, где можно вольно назначать один сигнал другому без контроля разрядности. При назначении сигнала большей разрядности сигналу меньшей разрядности мы потеряем часть значащих битов, хотя с точки зрения анализа кода будет корректно записанное логическое условие в первом случае и не менее корректное присвоение сигналов во втором случае. Решение: при отладке, если есть возможность и ресурсы памяти позволяют, необходимо выводить в отладчик все сигналы, которые так или иначе участвуют в логическом условии, потому что даже, казалось бы, очевидные вещи могут приводить к некорректному поведению блока. Ошибка №3. Перекладывание проблемы на блок коллеги или внешние микросхемыТак могут поступать не только новички, но и опытные разработчики, которые исходят из предположения, что ошибка возникла не у них, а в блоке коллеги или даже в стандартном ядра или внешней микросхеме, с которой FPGA обменивается данными. Надо признать, что ошибки случаются и в стандартных блоках, и даже в серийных микросхемах, и большая их часть задокументирована. Решение: несмотря на то, что даже в стандартном ядре Xilinx может быть ошибка, нужно руководствоваться правилом «презумпции виновности», то есть исходить из предположения, что ошибка все-таки возникла в нашем блоке, а не в стандартном IP-ядре, например, AXI4 Interconnect, который по какой-то причине заблокировал транзакцию. Конечно, дополнительное письмо в службу техподдержки вендора блока лишним не будет. Практические рекомендацииНу и, наконец, рассмотрим список из восьми практических рекомендаций, который сделает процесс программирования ПЛИС более-менее управляемым.1. Счетчики ошибок. Добавляйте их везде, где это возможно. Ошибки контрольных сумм, ошибки доступа, протокольные ошибки. Например: видим ошибку CRC — заводим счетчик ошибок CRC, замечаем появление некорректных данных в пакете — учитываем их тоже.2. Счетчики статистики: счетчики данных, счетчики пакетов, счетчики запросов. Все блоки так или иначе обрабатывают данные — входные или выходные. Очень полезно поставить счетчик, чтобы видеть, сколько пришло на вход и сколько передано на выход.Если это пакетные данные, то считаем количество пакетов, если пакетам предшествует какой-то запрос (handshake) — считаем, сколько этих запросов было. На этапе отладки счетчики помогут определить, где потерялись данные в длинной цепочке из нескольких блоков.3. Детектирование «невозможных» состояний и ситуаций. Почему невозможность взята в кавычки? Потому что в увлекательном мире ПЛИС нет ничего невозможного. :-) Может возникнуть любая ситуация, и к ней нужно всегда быть готовым, т.е. детектировать ее. Пусть это будет флаг, который показывает, что ваше устройство было в том состоянии, которое не предусмотрено спецификацией. И, когда мы получим готовое устройство, которое внезапно зависло, считывая эти флаги, увидим, что такая ситуация все-таки возникла и ее нужно корректно обработать.4. Обработка всех сигналов ошибок. Если от сторонних блоков или интерфейсов в ваш блок приходят сигналы ошибки, их нужно обрабатывать, даже если они кажутся маловероятными или невозможными. Как минимум – все сигналы ошибок нужно фиксировать, чтобы определить причину проблемы, когда устройство перестанет работать или его поведение станет непредсказуемым.Для примера вернемся к нашему контроллеру PCI Express: почему нужно обрабатывать такую ошибку, как потеря линка? Вероятность того, что endpoint, который коммуницирует с root-комплексом, вдруг потеряет линк, стремится к нулю, но если такая ситуация возникнет, мы должны знать, что она была. 5. Анализ флагов FULL / EMPTY и ситуаций UNDERFLOW / OVERFLOWЭта рекомендация касается всеми любимого компонента в мире ПЛИС – FIFO (first in, first out). Это базовый элемент цифровой схемотехники, который используется для передачи данных из одного домена синхронизации в другой, для сглаживания разницы в скоростях между приемником и передатчиком, ну и по прямому назначению – для буферизации. Для отладки полезно добавить логику, которая будет детектировать ситуации чтения из пустого буфера (underflow) и записи в переполненный (overflow). Так мы оперативно обнаружим место, где теряются данные, или, наоборот, — появляются из ниоткуда.6. Обеспечение доступа к состояниям FSMШестой пункт рекомендаций касается другого важного элемента цифровой схемотехники ПЛИС – конечных автоматов. Именно они позволяют в максимально наглядной форме описать логику устройства, например, парсеры пакетов, контроллеры интерфейсов AXI MM и многое другое. Доступ к состояниям автоматов позволяет понять, в каком состоянии зависло устройство и какого сигнала ждет, чтобы перейти из одного состояния в другое. LTSSM — link training and status state machineИзображённый на этой схеме конечный автомат – это автомат инициализации линка PCI express (LTSSM). И практически все ядра PCIe позволяют вывести состояние этого автомата за пределы ядра, так что этим требованием пользуются даже крупные вендоры, в частности, Xilinx и Altera.7. Чтение документации «от корки до корки»Это очень скучная рекомендация, но крайне полезная. Если на интерфейс или ядро, с которым вы работаете, есть документация (а она должна быть), читайте ее целиком. Да, совет может показаться странным, потому что на некоторые ядра документация занимает не одну сотню страниц. Вы потратите на это время, но с большой вероятностью это сэкономит гораздо больше времени, потому что даже стандартные интерфейсы, которые используются в тех или иных ядрах, не всегда работают так, как мы себе представляем. Например, возьмем ядро PCI Express, работающее на уровне TLP-пакетов. Читаем документацию и видим, что в качестве интерфейса взаимодействия с пользователем задействован AXI Stream. Хочется пролистать этот раздел, ведь мы и так знаем, как работает и за что отвечают сигналы интерфейса AXI Stream. Но, в случае с ядром PCIe логика hand-shake-сигналов отличается от стандарта AXI Stream (в частности, у компании Xilinx). Если это не учесть, можно очень долго искать ошибку — особенно, не зная, где ее искать.8. Применение автоматических синтезаторов HLS, Simulink (где возможно)Ну и, наконец, последнее: применяйте по возможности автоматические синтезаторы. Например, среду HLS, которая позволяет реализовать вашу логику на языке С / С++. Или продукт Simulink, который позволяет реализовать логику FPGA на базе огромного набора стандартных библиотечных компонентов, а потом нажатием одной кнопки синтезировать из этого описание — HDL-код. Да, этот код будет нечитаемым с точки зрения разработчика. Очевидно, что написанное на HLS или реализованное в Simulink требует высокоуровневой поведенческой верификации и моделирования, но можно быть уверенным в том, что сгенерированный HDL-код, несмотря на свою нечитаемость, будет работать в ПЛИС. В заключение отметим важный аспект: рекомендация «обеспечить доступ» к различным счетчикам и состояниям внутри ПЛИС, подразумевает разные варианты реализации: Можно добавить к заведенным переменным встроенный отладчик типа Chipscope. Но бывает, что таких сигналов набирается слишком много, а оставлять Chipscope в устройстве, которое мы передаем пользователю в промышленную эксплуатацию и тестирование — не совсем корректно. Обеспечить доступ через один из доступных интерфейсов ко всем регистрам, которые можно завести на этапе описания кода. Примеры реализации:Отображение регистров в адресное пространство устройства, если оно взаимодействует с внешним миром, например, через PCI Express. Обеспечение доступа через тот же UART. В идеале это должен быть максимально неубиваемый интерфейс. Не забываем про светодиоды: более наглядного способа индикации работоспособности устройства с разработанной прошивкой еще не придумано. Вот мы и рассмотрели самые распространенные ошибки и решения проблем при отладке устройств на базе FPGA. Надеемся, что осознанное написание кода и внимательное чтение документации позволит вам менять принцип 20/80 в свою пользу и ощутимо сократить время отладки и тестирования в общем плане проекта.Конечно, в рамках одной статьи на Хабре невозможно рассказать обо всех тонкостях отладки ПЛИС, отсюда и слово «искусство» в заголовке. Каждая ситуация требует осмысленного подхода и своего метода отладки. Так что если нужна конкретика — можем пообщаться в комментариях.',\n 'Если вы находитесь на этом сайте, то наверняка разбираетесь в компьютерах. Я никакой не компьютерный эксперт (далек от этого), я — обычный пользователь, и хотел бы поделиться первыми и самыми теплыми воспоминаниями о компьютерах и интернете.  Конкретно на этой фотографии запечатлен не мой компьютер, но именно такая модель была у меня, и я уверен, что многие из вас обладали таким же Packard Bell. Журнал PC World включил Packard Bell в список десяти худших компьютеров всех времен, но, когда он появился у меня дома в 1994 году, это была самая крутая вещь, какую я когда-либо видел. Все говорят о своей любви к старым консолям, таким как NES и Genesis, но что насчет ваших первых ПК? Мы покупаем разные вещи, от спортивных курток до пряжек ремней, с изображениями геймпадов Nintendo и тому подобным, но кто-нибудь носит футболки с тем же Packard Bell или Windows 95? Многие из нас рассказывают о сотнях игр и фильмов, так любимых в детстве; но что насчет всего софта из прошлого? Чем дальше мы продвигаемся в технологиях, тем больше таких воспоминаний уходят в небытие.  Вот такой диск от Microsoft шел в комплекте с каждым компьютером, на котором стояла Windows 95. На нем было пару случайных игр и видео, демонстрирующих способности Windows. Ничего особенного для современного пользователя; но меня это невероятно поразило, буквально как пламя для пещерных людей.  Одним из музыкальных видео на диске был клип на песню Buddy Holly от Weezer — ныне одна из моих самых любимых песен. Качество видео было очень плохим по сравнению с тем же MTV, но факт того, что я могу посмотреть клип Weezer на компьютере, когда моей душе будет угодно, был революционным. Я, должно быть, посмотрел этот клип тысячу раз.Честно говоря, я вообще не помню этот фильм. Но я запомнил, что трейлер этого фильма находился на диске Windows 95. Сейчас интернет наполнен буквально миллионами трейлеров, но трейлер Rob Roy стал, наверное, первым трейлером на компьютере. Забавно, что фильм так и не добился успеха, несмотря на то что диск с его трейлером шел в комплекте с каждым проданным тогда компьютером. Также на диске была действительно прикольная игра, но, к сожалению, я не смог найти ни одного её изображения в интернете (что случается не так уж и часто). Это ещё больше подтверждает вымирание таких воспоминаний.  Другой бесплатной программой была Spiderman Cartoon Maker. Это очень примитивная версия flash-программы, где можно создать собственные ролики с человеком пауком. Я до сих не могу осознать, насколько это повлияло на мою жизнь. Сейчас я работаю в рекламной компании и занимаюсь обработкой видео; ясно, что проведенное в детстве время в этом приложении, по итогу вылилось в мою профессиональную сферу интересов. Последнее бесплатное приложение, которое я хотел бы упомянуть, это Encarta 95. В то время приложений-энциклопедий не было. Это сейчас интернет превратился в бесконечный источник информации. На любой вопрос можно найти ответ всего в несколько кликов. До выхода Encarta, в поиске ответов я обращался к гигантской семейной энциклопедии восьмидесятых годов. Конечно, в Encarta имелось не так много информации, поскольку, честно говоря, она была довольно ограниченной; но это приложение заложило основу того, какими должны быть интернет-энциклопедии.  Пожалуй, самым крутым открытием для меня, сделанным с помощью Encarta, стал Дэвид Боуи. Будучи ребенком, я никогда о нем не слышал, но в Encarta имелась различная информация о нем и его творчестве: фотографии, музыкальные сэмплы и биография. Фотография выше была в Encarta, и я долгое время думал, что он всегда так выглядел. Я сильно удивился, когда увидел его в образе Ziggy Stardust.Для меня компьютер был чем-то большим, чем просто инструментом для учебы, и как мы все знаем, на ПК можно играть в игры. Выше картинка SKIFREE - игры, о которой я совсем позабыл. Это очень простая игра - игрок просто спускается на лыжах по склону бесконечной горы, иногда прыгая с трамплинов и стараясь ни во что не врезаться. Но, честно говоря, гора не такая и бесконечная; при достижении определенного момента выскакивал Бигфут и съедал лыжника.Страшно?Первой моей купленной игрой на ПК была Wolfenstein 3D, и с ней в комплекте шел Doom. После этого я играл в Doom также много, как и другие ребята; но мне кажется, что Doom заслуживает собственной статьи, поэтому я опущу подробности. Вместо это, расскажу немного о Wolfenstein. Многие не знают, но Wolfenstein 3D - это сиквел игры Castle Wolfenstein, выпущенной на Apple 2 в 1981 году. Я никогда в неё не играл, но на сколько я понимаю, это стелс-игра, вроде Metal Gear.Дизайн уровней ничем особенным не отличался, да и назвать это “дизайном уровней” я могу только с натяжкой, учитывая, что каждый уровень представлял из себя просто набор квадратных комнат на одном этаже. Но это моя первая игра, в которой я ощутил эффект погружения: я буквально чувствовал, что нахожусь в эпицентре событий. Игра погрузила меня в мир нацистов и немецких овчарок. Что мне нравилось больше всего? Убивать Гитлера снова и снова! Только вот на этот раз фюреру достался гигантский роботизированный костюм.РобоГитлерМы очень часто с теплотой вспоминаем о Doom и Wolfenstein. Но как по мне, есть ряд тем, о которых сказано так много, что о них уже лучше промолчать. Например, «чувак, я так люблю Марио, это суперкрутая игра». Да, я знаю, все знают, это само собой разумеется. Что думаю я? Я хотел бы воззвать к таким вещам, которые вы когда-то любили, но уже забыли об этом. Например, к Chex Quest.Да, этот чувак внутри гигантской подушечки бренда «Chex« мочит зеленых пришельцев. Выглядит безумно? Да, но это также безумно весело. Игрок брал на себя роль воина Chex и с помощью разного оружия, например вращающейся вилки, уничтожал зеленых «флемоидов». Возможно, вы никогда не слышали об этой игре, но у неё имелась огромная фанбаза; настолько большая, что фанаты выпустили неофициальные сиквелы.  “Хавай вилку, флемоид!”Вместе с игрой поставлялись 50 бесплатных часов AOL(прим. в тот период AOL — крупный интернет-провайдер с возможностью почасовой оплаты интернет-соединения. Дальше пойдет речь о раздаче дисков с бесплатными часами)! Вы можете вспомнить, когда бесплатные часы AOL на самом деле хоть что-то значили? Это было безумие! Они начали с рекламы с раздачей 10 бесплатных часов и в конце концов раздали 789039 часов (или что-то вроде того). Мы с друзьями обычно крали их (даже если они были бесплатными) из магазинов, просто чтобы побросать их в друг друга. Когда я работал в магазине, который раздавал их, я обычно тратил свободное время на поиск новых путей их уничтожения … бесполезная вещь.Говоря о бесполезных вещах… что насчет самого AOL? Хоть кто-нибудь пользуется AOL сейчас? У меня есть старое мыло (email) от AOL, которым я пользуюсь, но что касается входа в систему и проверки писем - этого не происходило годами. Думаю, когда я в последний раз проверял чаты, то они были мертвы. Удивительно, что AOL настолько устарела, хотя когда-то это был единственный способ попасть в сеть, но с высокоскоростным интернетом AOL постигла судьба динозавра.Самым репрезентативным примером того, что компьютерные технологии полны исчезающих воспоминаний, является сам интернет. Как я могу окунуться в прошлое и насладиться старостью wwf.com примерно 1996 года? Ответ: никак, только если кто-нибудь не создаст клон этого сайта (что крайне сомнительно). Только взгляните на эту рекламу! У них даже было что-то с названием “gallery”, ну не шутка ли? Они даже считали необходимым продемонстрировать “главную страницу”. Я рад, что запечатлел момент, когда веб-сайты можно было пересчитать на пальцах.  Всем известно, что интернет навсегда изменил музыкальный бизнес, но помните ли вы, когда увидели такой CD-R впервые? Я даже не знал, для чего он нужен. Я думал, что он необходим для того, чтобы собрать все треки в одном месте, что-то типа микс-кассеты, только диск. Я понятия не имел, что mp3, которые я скачиваю, можно поместить на него. Вообще, говоря о mp3, помните, как в свое время скачивали их задолго до того, как появились napster или limewire? Раньше приходилось загружать их с разных сайтов, где было от десяти до ста различных mp3. Сайт, которым я пользовался - mp3can.com - уже много лет не работает.А как вы переносили mp3 с одного компьютера на другой? Чертовски уверен, что диковинкой на фотографии многие современные юзеры не пользовались. Я помню, как разозлился, когда моя семья купила компьютер без дисковода для дискет. “Как мы без него обойдемся?!”. Но, как оказалось, он нам больше никогда не понадобился. Если, конечно, речь не заходила о необходимости проверить половинку фотографии.Если вы помните этот скриншот, то получаете 25 дополнительных баллов за игру, о которой даже не подозревали. Как уже сказано, я не компьютерный специалист, и моя память очень ограничена, но именно так запомнился прогресс. Надеюсь, статья вам понравилась.…\"if you can\\'t say something nice, don\\'t nothing at all\"-ThumperДата-центр ITSOFT — размещение и аренда серверов и стоек в двух дата-центрах в Москве. За последние годы UPTIME 100%. Размещение GPU-ферм и ASIC-майнеров, аренда GPU-серверов, лицензии связи, SSL-сертификаты, администрирование серверов и поддержка сайтов.  ',\n 'Рынок ИТ- продуктов переполнен предложениями платформенных решений для анализа больших данных: их обсуждают, рекомендуют и внедряют, но всем ли они необходимы? Алексей Ершов, эксперт по продуктам Factory5 (входит в группу Ctrl2GO), ответил на главные вопросы об аналитических платформах для ИТ-директоров, менеджеров проектов и других участников data science инициатив на предприятиях. Далеко не все такие инициативы успешны: глобальные исследования Gartner и других компаний показывают, что до 85% проектов не приносят бизнесу результатов. Например, аналитические модели не получается интегрировать в бизнес-процессы. Это происходит по разным причинам, в том числе техническим. Компании работают со множеством сервисов — иногда от разных производителей. Это и инструменты business intelligence, и реляционные и NoSQL-базы данных, и инструменты для big data и data science. Возникают проблемы с интеграцией, передачей данных и их согласованной обработкой. Часть информации может просто потеряться. Эти трудности решает такой класс продуктов, как платформы для анализа больших данных, или, как еще называют, data science платформы.  Платформы анализа больших данных: что это такое и зачем они нужныПлатформа для обработки больших данных — это решение, которое объединяет различные инструменты, необходимые специалистам по data science. Такие платформы существенно упрощают их работу, охватывая весь жизненный цикл data science проектов: от идеи и исследования данных до построения и развертывания аналитических моделей. Они позволяют решить так называемую проблему «последней мили»: интегрировать результаты анализа данных в операционную деятельность, чтобы они влияли на принятие решений и трансформировали бизнес-процессы. Это может быть реализовано в виде API предиктивной модели, к которой обращаются другие системы, веб-приложения, которым могут пользоваться сотрудники, или просто ежедневного отчета, отправляемого на почту. Объяснить, зачем нужны платформы, можно с помощью простой аналогии. Представьте, что на промышленном предприятии конструкторский отдел разработал новый продукт. Принесет ли один опытный образец пользу бизнесу? Нет — прибыль даст только серийное производство. А для этого потребуется не только оборудование, но и регулярные поставки комплектующих, технологические карты, настроенные процессы контроля качества, обслуживания, модернизации продукта. Чтобы поставить производство на поток, нужны дополнительные ресурсы и компетенции. Аналогичная ситуация возникает и в data science проектах. Ключевой результат работы дата сайентиста — аналитическая модель — это и есть тот самый опытный образец. Она работает, ее можно запустить, показать в действии. Но если сделать только модель, то на бизнес это не повлияет. Чтобы разрабатывать модели и превращать их из пилотных проектов в работающие бизнес-приложения, чтобы модели работали с потоками данных и не «падали», чтобы выдавали результат за разумное время, нужна соответствующая технологическая оснастка — data science платформы.Такие решения делают работу data science специалистов прозрачной и масштабируемой. Платформы могут использовать и системные интеграторы, и конечные заказчики, у которых есть специалисты по обработке данных и аналитике. Какие функции есть у платформ анализа больших данныхКаждый data science-проект проходит жизненный цикл, состоящий из трех этапов:сбор данных и исследованиеэкспериментирование и разработка моделиразвертывание и интеграция.На каждом этапе специфические задачи, которые помогает выполнять платформа. И есть более общие задачи, включающие управление данными, управление процессами обработки и масштабирования.Для решения всех этих задач платформы обработки данных предлагают такой технический функционал: прием, подготовка и исследование данных, генерация признаков, создание, обучение, тестирование и деплой моделей, мониторинг и обслуживание системы. Также платформа должна обеспечивать безопасность данных и их хранение, каталогизацию источников, предоставлять инструменты для визуализации и формирования отчетов. Облачные платформы дополнительно дают большой объем хранилища и вычислительных мощностей. Все перечисленные функции платформ нужны, чтобы:Ускорять работу специалистов.Публиковать модели и интегрировать их в бизнес-процессы.Делиться понятными, читаемыми результатами анализа с сотрудниками всех подразделений.Сохранять прошлые наработки, включая метаданные, код, датасеты и обсуждения, и использовать их в новых проектах.Создать общую базу знаний и собирать лучшие практики, на которых будут учиться новые сотрудники.Безопасно внедрять новые инструменты, не ломая текущие процессы и не вмешиваясь в работу коллег.Масштабировать вычислительные мощности.Контролировать доступы к каждому проекту, чтобы его видели только определенные сотрудники.Зачем Data Science, если есть системы Business Intelligence  Иногда платформы Data Science воспринимают как аналоги систем Business Intelligence (BI), так как они тоже содержат инструменты для визуализации результатов анализа данных. Важно понимать отличия между ними, чтобы выбирать область применения.Традиционно BI-решения используются для статических отчетов о текущем или прошлом состоянии бизнеса. Они отвечают на такие вопросы, как: «Какая динамика объема продаж в прошедшем квартале? За счет чего произошел рост или падение продаж? Какой тип продукции произвели больше всего за месяц?». Это так называемый дескриптивный или описательный анализ. Кроме того, BI системы работают со структурированными данными, извлеченными из хранилищ данных и представляют результат анализа в виде интерактивных информационных панелей — дашбордов или отчетов.Платформы анализа больших данных — это уже инструмент для прогностического и динамического анализа. Они позволяют делать прогнозы по развитию любой сферы бизнеса и на их основе принимать более точные решения. Типовые вопросы: «Какой оптимальный сценарий развития бизнеса? Что будет, если продолжатся текущие тренды? Что случится, если принять новое управленческое решение?». Платформы могут использовать как структурированные, так и неструктурированные данные из множества источников, и умеют обрабатывать большие данные. Так как предиктивный анализ связан нацелен на прогнозирование какого-то параметра или события, то он фокусируется на конкретной задаче, в отличие от business intelligence. Дескриптивный же анализ должен позволять пользователям гибко создавать отчет в том разрезе,  который им потребуется.Современные BI-системы, например, Tableau или PowerBI, имеют большой набор средства визуализации: от линейных графиков и круговых диаграмм до тепловых карт и диаграмм санкей. Поэтому хотя BI-системы и data science платформы предназначены для разных задач, но они могут дополнять друг друга. Например, существующая BI-система может в удобном виде представить результаты анализа данных, которые поступают из платформы.Платформы или open sourceВ некоторых компаниях специалисты по big data по-прежнему работают с open source-инструментами. Дата сайентисты чаще всего учатся на них и продолжают использовать их уже на работе. Это подтверждает исследование Normal Research и агенства New.HR при поддержке портала GeekJOB среди аналитиков, в котором респонденты чаще всего упоминают языки Python, R и соответствующие библиотеки (NumPy, Pandas и другие). Это объяснимо, ведь у таких инструментов низкий технический порог входа: ими легко пользоваться на личном ноутбуке. Но в реальном бизнесе, когда растет и объем данных, и сложность вычислений, когда нужно обеспечить процессинг и масштабирование, объем сопутствующих работ резко увеличивается. Эти задачи лежат уже в инженерной плоскости, а не в аналитической. Специалисты сталкиваются с необходимостью «подружить» разные решения, поделиться кодом и моделями с другими сотрудниками, а также с вопросами безопасности. На интеграцию уходит дополнительное время, а зачастую это требует и дополнительных расходов. Поддержка разных инструментов тоже закономерно требует больших усилий, чем единого решения. И даже когда open source-инструменты покрывают потребности в обработке и анализе данных, они не интегрированы с другими сервисами компании — в итоге специалистам сложно встроить ML-модели в существующее ИТ-окружение.Современные платформы анализа больших данных не заменяют, а дополняют известные дата сайентистам open source-инструменты. Они по-прежнему могут разрабатывать модели с помощью привычных фреймворков и библиотек, а платформы предоставляют необходимый технический функционал для продуктивной работы и реализации полного цикла data science проектов. Такой подход позволяет специалистам не переучиваться и быстрее разрабатывать аналитические продукты для бизнеса.Когда стоит внедрять платформы для анализа больших данныхОбычно в компании у каждой команды есть инструмент для упорядочивания процессов и совместной работы над задачами. Платформы обработки данных — такой же нужный инструмент для дата сайентистов, как система контроля исходного кода для разработчиков, CRM для отдела продаж и helpdesk для технической поддержки. Например, в небольших компаниях вместо CRM используются excel файлы  или облачные kanban сервисы. Разные менеджеры могут использовать разные инструменты. На определенном этапе возникают проблемы: информация не хранится в одном месте, нет единого доступа к ней у руководителей, файлы увеличиваются в объеме, в них долго искать информацию и трудно масштабировать такую систему. Схожие трудности возникают и в data science проектах. Вот признаки того, что вам пора начать использовать платформу анализа больших данных:Снижение продуктивностиЕсли вы замечаете, что инструментов становится слишком много, вы тратите много времени на рутинные задачи, а время на обработку данных и обслуживание разных инструментов только растет — вероятнее всего, вам пора переходить на DS-платформу.Трудности масштабированияКогда вам необходимо тиражировать разработанное приложение на другие направления бизнеса, или один из этапов обработки данных потребовал больше вычислительных ресурсов, то возникает потребность в масштабировании. Если вам нужно масштабироваться, но вы не очень хорошо представляете, как именно это сделать, то платформа просто необходима. Нехватка прозрачностиЕсли в команде дата сайентистов начинаются проблемы с коммуникацией, то это говорит об отсутствии централизованных знаний, которыми можно легко делиться между собой. Современные платформы предоставляют доступ разным сотрудникам, вовлеченным в проект.Рынок data science платформ растет вместе с рынком искусственного интеллекта и углубленной аналитики данных. По оценкам агентства Markets and Markets, рынок платформ для анализа больших данных растет в среднем на 30% в год. Появляются новые продукты, которые разработчики называют «платформами для анализа данных»   или “data science platform”, и неподготовленному человеку может быть сложно в них разобраться. ',\n 'Ссылка на GitHubВступлениеПривет всем. Представим, что вы часто работаете с canvas в html. И многие вещи в канвасе делаются не просто. Например чтобы загрузить картинку вам нужно создать объект new Image(), подождать пока прогрузиться, затем только добавить на холст. Или же чтобы нарисовать треугольник вам нужно подбирать координаты и т.п. Очень много строк кода. По этому, если вы потратите 5 минут на изучение данной библиотеки, вы очень сильно сможете упростить отрисовку вашего холста.Для чего создана это библиотекаЭта очень простая, абсолютно бесплатная, но в то же время сильная библиотека, которая создана специально, чтобы облегчить использование canvas в html. Почему стоит использовать ее вы узнаете ниже, я расскажу вам основные преимущества.Как подключитьПодключить очень просто, достаточно вписать строчку в ваш html файл<script src=\"https://gaidadei.ru/easycanvas/easyc.js\"></script>Или же вы можете скачать файл на сайте ПреимуществаОЧЕНЬ ПРОСТО НАРИСОВАТЬ ЧТО-ТОПо сути библиотека состоит из объекта new EasyC(). Который имеет два свойства: .canvas и .objects . Первый это DOM элемент канваса, на котором мы будем рисовать. Второй, это массив элементов, которые мы будем рисовать, затем вызываем функцию .draw() и все автоматически рисуется. Например нарисуем картинку:new EasyC(document.getElementById(\"canvas\"), [{\\n    type: \"image\",\\n    x: 250,\\n    y: 100,\\n    src: \"2.png\"\\n }]).draw();И сразу получаете готовое изображение на вашем канвасе.И даже не нужно создавать объект new Image(), все это библиотека делает сама.ЛЕГКО ЗАГРУЖАТЬ СТОРОННИЕ ФАЙЛЫКак это уже было показано на примере выше с изображением, но также еще очень просто загружать и шрифты, например создадим текст с собственным шрифтомnew EasyC(document.getElementById(\"canvas\"), [{\\n  type: \"text\",\\n  x: 250,\\n  y: 150,\\n  value: \"Hello, world\\\\nSecond stroke\\\\nThird stroke\",\\n  font: \"url(myownfont.ttf)\",\\n  size: 26,\\n  align: \"left\",\\n  fill: \"#999\",\\n}]).draw();Вот что имеемКак вы видите, вы можете даже писать несколько строк разделяя их знаком \\\\n . Еще также покажу на примере как вы можете заполнять фигуры любой картинкой. Тут я не буду писать полностью код, укажу только свойство которое присвоено всем объектам на этом примере.fill: \"url(2.png, 0, 1, repeat)\"Первое это ссылка на файл, второе координаты изображение, третье это размеры изображение и четвёртое это указываем будет ли  изображение повторяться. Получаем такие фигурыВЫ МОЖЕТЕ УКАЗЫВАТЬ Z КООРДИНАТУПросто указав z координату вы можете выбирать какой объект будет рисоваться поверх другого. Например new Easyc(document.getElementById(\"canvas\"), [{\\n  type: \"triangle\",\\n  x: 300,\\n  y: 105,\\n  angleLeft: 1.5*Math.PI/4,\\n  angleRight: 1*Math.PI/4,\\n  base: 150,\\n  fill: \"#000\",\\n  z: 12\\n}, {\\n  type: \"rectangle\",\\n  x: 200,\\n  y: 150,\\n  width: 100,\\n  height: 100,\\n  fill: \"#999\",\\n  z: 3\\n}]).draw();ПолучаемВЫ МОЖЕТЕ РАБОТАТЬ С ОТНОСИТЕЛЬНЫМИ КООРДИНАТАМИДо этого мы все указывали в абсолютных координатах в пикселях, но также вы можете работать с относительными, относительно ширины и высоты канваса. Достаточно установить relative: truenew EasyC(document.getElementById(\"canvas\"), [{\\n    type: \"rectangle\",\\n    relative: true,\\n    x: 0.5,\\n    y: 0.25,\\n    width: 0.3,\\n    height: 0.3,\\n    fill: \"#999\"\\n }]).draw();ПолучаемЗаключениеЭто конечно еще не все свойства, например я не рассказал как создаётся градиент, как делается поворот объектов, прозрачность, масштабирование и т.д. Но основная задача была рассказать о главных преимуществах данной библиотеки. Она очень проста и сильно упрощает вам жизнь. Буду рад, если вам действительно это поможет. Вот, кстати, есть видеоурок, как создаётся мини игра на этой библиотеке. ',\n 'Helm — незаменимый инструмент для развертывания приложений в кластерах Kubernetes. Но только следуя передовому опыту, вы действительно ощутите преимущества Helm. Вот 13 рекомендаций, которые помогут вам создавать, использовать и обновлять приложения с помощью Helm.\\nКоллеги, пишите, если что-то переведено неправильно.\\nПоднимите свои Helm Charts на новый уровень\\nHelm — менеджер пакетов для Kubernetes. Он сокращает усилия по развертыванию сложных приложений благодаря шаблонному подходу и богатой экосистеме многоразовых и готовых к производству пакетов, также известных как диаграммы Helm. С помощью Helm вы можете развертывать упакованные приложения как набор предварительно настроенных ресурсов Kubernetes с установленными версиями.\\nПредположим, вы развертываете базу данных с Kubernetes, включая несколько развертываний, контейнеров, секретов, томов и служб. Helm позволяет установить одну и ту же базу данных с помощью одной команды и одного набора значений. Его декларативные и идемпотентные команды делают Helm идеальным инструментом для непрерывной доставки (CD).\\nHelm — это проект Cloud Native Computing Foundation (CNCF), созданный в 2015 году и завершенный в апреле 2020 года. С последней версией Helm 3 он стал еще более интегрированным в экосистему Kubernetes.\\nВ этой статье представлены 13 рекомендаций по созданию диаграмм Helm для управления вашими приложениями, работающими в Kubernetes.\\n1. Использование преимуществ экосистемы Helm\\nHelm дает вам доступ к обширному опыту сообщества — возможно, самое большое преимущество инструмента. Он собирает чарты от разработчиков со всего мира, которые затем передаются через репозитории диаграмм. Вы можете проверить в Artifact Hub доступные репозитории диаграмм Helm.\\nНайдя репозиторий диаграмм, вы можете добавить его в свою локальную настройку следующим образом:\\n$ helm repo add bitnami https://charts.bitnami.com/bitnami\\nЗатем вы можете искать диаграммы, например, MySQL:\\n$ helm search hub mysql\\n\\nURL                                                     CHART VERSION   APP VERSION     DESCRIPTION                                       \\nhttps://hub.helm.sh/charts/bitnami/mysql                8.6.3           8.0.25          Chart to create a Highly available MySQL cluster\\n2. Использование subcharts для управления своими зависимостями\\nПоскольку приложения, развернутые в Kubernetes, состоят из детализированных, взаимозависимых частей, их диаграммы Helm имеют различные шаблоны ресурсов и зависимости. Например, предположим, что ваш бэкэнд полагается на базу данных и очередь сообщений. База данных и очередь сообщений уже являются автономными приложениями (например, PostgreSQL и RabbitMQ). Поэтому рекомендуется создавать или использовать отдельные чарты для автономных приложений и добавлять их в родительские чарты. Зависимые приложения названы здесь в виде поддиаграмм.\\nЕсть три основных элемента для создания и настройки subcharts:\\n\\nСтруктура чарта. Структура папок должна быть в следующем порядке:\\n\\nbackend-chart\\n  - Chart.yaml\\n  - charts\\n      - message-queue\\n          - Chart.yaml\\n          - templates\\n          - values.yaml\\n      - database\\n          - Chart.yaml\\n          - templates\\n          - values.yaml\\n  - values.yaml\\n\\nChart.yaml\\n\\nКроме того, в файле chart.yaml родительском чарте должны быть перечислены все зависимости и условия:\\napiVersion: v2\\nname: backend-chart\\ndescription: A Helm chart for backend\\n...\\ndependencies:\\n  - name: message-queue\\n    condition: message-queue.enabled\\n  - name: database\\n    condition: database.enabled\\n\\nvalues.yaml\\n\\nНаконец, вы можете установить или переопределить значения вложенных диаграмм в родительской диаграмме с помощью следующего файла values.yaml:\\nmessage-queue:\\n  enabled: true\\n  image:\\n    repository: acme/rabbitmq\\n    tag: latest\\ndatabase:\\n  enabled: false\\nСоздание и использование subcharts устанавливает уровень абстракции между родительским и зависимым приложениями. Эти отдельные чарты упрощают развертывание, отладку и обновление приложений в Kubernetes с их отдельными значениями и жизненными циклами обновления. Вы можете просмотреть структуру папок, зависимости и файлы значений в образце диаграммы, например bitnami / wordpress.\\n3. Использование ярлыков для простого нахождения ресурсов\\nLabels имеют решающее значение для внутренних операций Kubernetes и повседневной работы операторов Kubernetes. Почти каждый ресурс в Kubernetes предлагает метки для разных целей, таких как группировка, распределение ресурсов, балансировка нагрузки или планирование.\\nОдна команда Helm позволит вам установить несколько ресурсов. Но очень важно знать, откуда берутся эти ресурсы. Labels позволяют быстро находить ресурсы, созданные релизами Helm.\\nСамый распространенный метод — определить метки в helpers.tpl, например:\\n{{/*\\nCommon labels\\n*/}}\\n\\n{{- define \"common.labels\" -}} \\napp.kubernetes.io/instance: {{ .Release.Name }}\\napp.kubernetes.io/managed-by: {{ .Release.Service }}\\n{{- end -}}\\nЗатем вам нужно использовать функцию include с метками в шаблонах ресурсов:\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: my-queue\\n  labels:\\n{{ include \"common.labels\" . | indent 4 }}\\n...\\nТеперь вы должны иметь возможность перечислить все ресурсы с помощью селекторов меток. Например, вы можете перечислить все модули развертывания my-queue с помощью команды kubectl get pods -l app.kubernetes.io/instance=[Name of the Helm Release]. Этот шаг важен для поиска и отладки тех ресурсов, которыми управляет Helm.\\n4. Документирование чартов\\nДокументация необходима для обеспечения поддерживаемости диаграмм Helm. Добавление комментариев в шаблоны ресурсов и README помогает командам в разработке и использовании диаграмм Helm.\\nВы должны использовать следующие три варианта для документирования ваших диаграмм:\\n\\nКомментарии: Файлы шаблонов и значений являются файлами YAML. Вы можете добавлять комментарии и предоставлять полезную информацию о полях в файлах YAML.\\nREADME: README диаграммы — это файл уценки, в котором объясняется, как использовать диаграммы. Вы можете проверить содержимое файла README с помощью следующей команды: helm show readme [Name of the Chart]\\nNOTES.txt: это специальный файл, расположенный по адресу templates/NOTES.txt, который предоставляет полезную информацию о развертывании выпусков. Для содержимого файла NOTES.txt также можно использовать шаблоны функций и значений, аналогичные шаблонам ресурсов:\\n\\nYou have deployed the following release: {{ .Release.Name }}.\\nTo get further information, you can run the commands:\\n  $ helm status {{ .Release.Name }}\\n  $ helm get all {{ .Release.Name }}\\nВ конце команды helm install или helm upgrade Helm распечатывает содержимое NOTES.txt следующим образом:\\nRESOURCES:\\n==> v1/Secret\\nNAME        TYPE      DATA      AGE\\nmy-secret   Opaque    1         0s\\n\\n==> v1/ConfigMap\\nNAME           DATA      AGE\\ndb-configmap   3         0s\\n\\nNOTES:\\nYou have deployed the following release: precious-db.\\nTo get further information, you can run the commands:\\n  $ helm status precious-db\\n  $ helm get all precious-db\\n5. Проверка чартов\\nЧарты Helm состоят из нескольких ресурсов, которые должны быть развернуты в кластере. Важно убедиться, что все ресурсы созданы в кластере с правильными значениями. Например, при развертывании базы данных вы должны убедиться, что пароли базы данных установлены правильно.\\nК счастью, Helm предлагает функцию тестирования для запуска некоторых контейнеров в кластере для проверки приложений. Например, шаблоны ресурсов, помеченные как \"helm.sh/hook\": test-success, запускаются Helm как тестовые примеры.\\nПредположим, вы развертываете WordPress с базой данных MariaDB. Диаграмма Helm, поддерживаемая Bitnami, имеет модуль для проверки соединения с базой данных со следующим определением:\\n...\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: \"{{ .Release.Name }}-credentials-test\"\\n  annotations:\\n    \"helm.sh/hook\": test-success\\n...\\n      env:\\n        - name: MARIADB\\\\_HOST\\n          value: {{ include \"wordpress.databaseHost\" . | quote }}\\n        - name: MARIADB\\\\_PORT\\n          value: \"3306\"\\n        - name: WORDPRESS\\\\_DATABASE\\\\_NAME\\n          value: {{ default \"\" .Values.mariadb.auth.database | quote }}\\n        - name: WORDPRESS\\\\_DATABASE\\\\_USER\\n          value: {{ default \"\" .Values.mariadb.auth.username | quote }}\\n        - name: WORDPRESS\\\\_DATABASE\\\\_PASSWORD\\n          valueFrom:\\n            secretKeyRef:\\n              name: {{ include \"wordpress.databaseSecretName\" . }}\\n              key: mariadb-password\\n      command:\\n        - /bin/bash\\n        - -ec\\n        - |\\n          mysql --host=$MARIADB\\\\_HOST --port=$MARIADB\\\\_PORT --user=$WORDPRESS\\\\_DATABASE\\\\_USER --password=$WORDPRESS\\\\_DATABASE\\\\_PASSWORD\\n  restartPolicy: Never\\n{{- end }}\\n...\\nРекомендуется писать тесты для ваших графиков и запускать их после установки. Например, вы можете использовать команду helm test <RELEASE_NAME> для запуска тестов. Тесты являются ценным активом для проверки и поиска проблем в приложениях, установленных вместе с Helm.\\n6. Обеспечение безопасности секретов\\nКонфиденциальные данные, такие как ключи или пароли, хранятся в Kubernetes как секреты. Хотя на стороне Kubernetes можно защитить секреты, они в основном хранятся в виде текстовых файлов как часть шаблонов и значений Helm.\\nПлагин helm-secrets предлагает секретное управление и защиту вашей важной информации. Он делегирует секретное шифрование Mozilla SOPS, который поддерживает AWS KMS, Cloud KMS на GCP, Azure Key Vault и PGP.\\nПредположим, вы собрали конфиденциальные данные в файл с именем secrets.yaml следующим образом:\\n$ helm secrets enc secrets.yaml\\nEncrypting secrets.yaml\\nEncrypted secrets.yaml\\nВы можете зашифровать файл с помощью плагина:\\npostgresql:\\n    postgresqlUsername: ENC\\\\[AES256\\\\_GCM,data:D14/CcA3WjY=,iv...==,type:str\\\\]\\n    postgresqlPassword: ENC\\\\[AES256\\\\_GCM,data:Wd7VEKSoqV...,type:str\\\\]\\n    postgresqlDatabase: ENC\\\\[AES256\\\\_GCM,data:8ur9pqDxUA==,iv:R...,type:str\\\\]\\nsops:\\n  ...\\nТеперь файл будет обновлен, и все значения будут зашифрованы:\\npostgresql:\\n    postgresqlUsername: ENC\\\\[AES256\\\\_GCM,data:D14/CcA3WjY=,iv...==,type:str\\\\]\\n    postgresqlPassword: ENC\\\\[AES256\\\\_GCM,data:Wd7VEKSoqV...,type:str\\\\]\\n    postgresqlDatabase: ENC\\\\[AES256\\\\_GCM,data:8ur9pqDxUA==,iv:R...,type:str\\\\]\\nsops:\\n  ...\\nПриведенные выше данные в файле secrets.yaml не были безопасными, а helm-secrets решает проблему хранения конфиденциальных данных как части диаграмм Helm.\\n7. Создание многоразовой диаграммы с помощью шаблонных функций\\nHelm поддерживает более 60 функций, которые можно использовать внутри шаблонов. Функции определены на языке шаблонов Go и библиотеке шаблонов Sprig. Функции в файлах шаблонов значительно упрощают работу Helm.\\nДавайте посмотрим на следующий файл шаблона в качестве примера:\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: {{ .Release.Name }}-configmap\\ndata:\\n  environment: {{ .Values.environment | default \"dev\" | quote }}\\n  region: {{ .Values.region | upper | quote }}\\nЕсли значение среды не указано, функция шаблона будет использовать его по умолчанию. Проверив поле региона, вы увидите, что в шаблоне нет значения по умолчанию. Однако у поля есть другая функция, называемая upper, для преобразования предоставленного значения в верхний регистр.\\nТребуется еще одна важная и полезная функция — required. Она позволяет вам установить значение, необходимое для рендеринга шаблона. Например, предположим, что вам нужно имя для вашей ConfigMap со следующим шаблоном:\\n...\\nmetadata:\\n  name: {{ required \"Name is required\" .Values.configName }}\\n...\\nЕсли запись пуста, рендеринг шаблона завершится ошибкой. Требуется имя. Функции шаблонов очень полезны при создании чартов Helm. Они могут улучшить создание шаблонов, уменьшить дублирование кода и могут использоваться для проверки значений перед развертыванием ваших приложений в Kubernetes.\\n8. Обновление развертываний (Deployments) при изменении ConfigMaps или секретов.\\nОбычно к контейнерам монтируются ConfigMaps или секреты. Хотя развертывания и образы контейнеров меняются с новыми выпусками, ConfigMaps или секреты меняются нечасто. Следующая аннотация позволяет развертывать новые развертывания (Deployments) при изменении ConfigMap:\\nkind: Deployment\\nspec:\\n  template:\\n    metadata:\\n      annotations:\\n        checksum/config: {{ include (print $.Template.BasePath \"/configmap.yaml\") . | sha256sum }}\\n...\\nЛюбое изменение в ConfigMap вычислит новую сумму sha256sum и создаст новые версии развертывания (Deployments). Это гарантирует, что контейнеры в развертываниях (Deployments) будут перезапущены с использованием нового ConfigMap.\\n9. Отказ от удаления ресурсов с помощью политик ресурсов.\\nВ типичной настройке после установки Helm чартов, Helm создает несколько ресурсов в кластере. Затем вы можете обновить его, изменив значения и добавив или удалив ресурсы. Когда приложение больше не понадобится, его можно удалить, что приведет к удалению всех ресурсов из кластера. Однако некоторые ресурсы следует сохранить в кластере даже после выполнения удаления Helm. Предположим, вы развернули базу данных с PersistentVolumeClaim и хотите сохранить тома, даже если вы удаляете выпуск базы данных. Для таких ресурсов вам необходимо использовать аннотации политики ресурсов следующим образом:\\nkind: Secret\\nmetadata:\\n  annotations:\\n    \"helm.sh/resource-policy\": keep\\n...\\nКоманды Helm, такие как удаление, обновление или откат, приведут к удалению указанного выше секрета. Но, используя политику ресурсов, как показано, Helm пропустит удаление секрета и позволит его осиротить. Поэтому аннотацию следует использовать с большой осторожностью и только для ресурсов, необходимых после удаления релизов Helm.\\n10. Полезные команды для отладки Helm-чартов.\\nФайлы шаблонов Helm содержат множество различных функций и несколько источников значений для создания ресурсов Kubernetes. Важная обязанность пользователя — знать, что развернуто в кластере. Следовательно, вам нужно научиться отлаживать шаблоны и проверять чарты. Для отладки можно использовать четыре основные команды:\\n\\nhelm lint: инструмент линтер проводит серию тестов, чтобы убедиться, что ваш чарт сформирован правильно.\\nhelm install --dry-run --debug: эта функция отображает шаблоны и показывает полученные манифесты ресурсов. Вы также можете проверить все ресурсы перед развертыванием и убедиться, что значения установлены, а функции шаблонов работают должным образом.\\nhelm get manifest: эта команда извлекает манифесты ресурсов, установленных в кластере. Если выпуск (release) не работает должным образом, это должна быть первая команда, которую вы используете, чтобы узнать, что работает в кластере.\\nhelm get values: эта команда используется для получения значений выпуска (release), установленных в кластере. Если у вас есть какие-либо сомнения относительно вычисленных значений или значений по умолчанию, это обязательно должно быть в вашем наборе инструментов.\\n\\n11. Использование функций поиска, чтобы избежать регенерации секрета.\\nФункции Helm используются для генерации случайных данных, таких как пароли, ключи и сертификаты. Случайная генерация создает новые произвольные значения и обновляет ресурсы в кластере при каждом развертывании и обновлении. Например, он может заменять пароль вашей базы данных в кластере при каждом обновлении версии. Это приводит к тому, что клиенты не могут подключиться к базе данных после смены пароля.\\nЧтобы решить эту проблему, рекомендуется случайным образом генерировать значения и заменять те, которые уже находятся в кластере. Например:\\n{{- $rootPasswordValue := (randAlpha 16) | b64enc | quote }}\\n{{- $secret := (lookup \"v1\" \"Secret\" .Release.Namespace \"db-keys\") }}\\n{{- if $secret }}\\n{{- $rootPasswordValue = index $secret.data \"root-password\" }}\\n{{- end -}}\\napiVersion: v1\\nkind: Secret\\nmetadata:\\n  name: db-keys\\n  namespace: {{ .Release.Namespace }}\\ntype: Opaque\\ndata:\\n  root-password: {{ $rootPasswordValue}}\\nПриведенный выше шаблон сначала создает 16-символьное значение randAlpha, затем проверяет кластер на секрет и соответствующее ему поле. Если он найден, он переопределяет и повторно использует rootPasswordValue в качестве пароля root.\\n12. Переход на Helm 3 для более простых и безопасных приложений Kubernetes.\\nПоследний выпуск Helm — Helm 3, предлагает множество новых функций, которые делают его более легким и оптимизированным инструментом. Helm v3 рекомендуется из-за его повышенной безопасности и простоты. Это включает в себя:\\n\\nУдаление Tiller: Tiller был серверным компонентом Helm, но был удален из v3 из-за наличия исчерпывающих разрешений, необходимых для внесения изменений в кластер в более ранних версиях. Это также создало угрозу безопасности, поскольку любой, кто получит доступ к Tiller, будет иметь чрезмерные разрешения для вашего кластера.\\nУлучшенная стратегия обновления чартов: Helm v2 полагается на двусторонний стратегический патч слияния (two-way strategic merge patch, как правильно перевести?). Он сравнивает новую версию с версией в хранилище ConfigMap и применяет изменения. И наоборот, Helm v3 сравнивает старый манифест, состояние в кластере и новый выпуск (release). Таким образом, внесенные вами вручную изменения не будут потеряны при обновлении выпусков Helm. Это упрощает процесс обновления и повышает надежность приложений.\\n\\nСуществует плагин helm-2to3, который вы можете установить с помощью следующей команды:\\n$ helm3 plugin install https://github.com/helm/helm-2to3\\nЭто небольшой, но полезный плагин с командами очистки, преобразования и перемещения, которые помогут вам перенести и очистить конфигурацию v2 и создать выпуски для v3.\\n13. Поддерживание идемпотентности ваших конвейеров непрерывной доставки\\nРесурсы Kubernetes декларативны в том смысле, что их спецификация и статус хранятся в кластере. Точно так же Helm требуется для создания декларативных шаблонов и выпусков (release). Следовательно, вам необходимо разработать систему управления непрерывной доставкой и выпуском, которая будет идемпотентной при использовании Helm. Идемпотентная операция — это операция, которую можно применять много раз, не изменяя результат после первого запуска.\\nНеобходимо соблюдать два основных правила:\\n\\nВсегда используйте команду helm upgrade --install. Он устанавливает диаграммы, если они еще не установлены. Если они уже установлены, он обновляет их.\\nИспользуйте флаг --atomic для отката изменений в случае неудачной операции во время обновления Helm. Это гарантирует, что выпуски Helm не застрянут в состоянии сбоя.\\n\\nРезюме\\nHelm — незаменимый инструмент для развертывания приложений в кластерах Kubernetes. Но только следуя передовому опыту, вы действительно ощутите преимущества Helm.\\nЛучшие практики, описанные в этой статье, помогут вашим командам создавать, использовать и обновлять распределенные приложения производственного уровня. Что касается разработки, ваши диаграммы Helm будет проще поддерживать и защищать. Что касается эксплуатации, вы получите удовольствие от автоматически обновляемых развертываний, сэкономите ресурсы от удаления и научитесь тестировать и отлаживать.\\nОфициальное руководство по темам Helm — еще один хороший ресурс для проверки команд Helm и понимания их философии дизайна. С этими ресурсами, а также с лучшими практиками и примерами, изложенными в этом блоге, вы наверняка будете вооружены и готовы создавать и управлять приложениями Helm производственного уровня, работающими в Kubernetes.\\nКоллеги, пишите, если что-то переведено неправильно.']"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracting text for all articles\n",
    "posts_text = []\n",
    "for link in posts_link:\n",
    "    soup = BeautifulSoup(requests.get(link).text)\n",
    "    time.sleep(0.3)\n",
    "    text = soup.find(id='post-content-body').text\n",
    "    posts_text.append(text)\n",
    "posts_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dimensional-universal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                       date  \\\n0  2021-07-22T10:07:01.000Z   \n1  2021-07-22T10:00:03.000Z   \n2  2021-07-22T09:54:05.000Z   \n\n                                               title  \\\n0  Многопользовательская сетевая игра Ticket to Ride   \n1  Искусство отладки FPGA: как сократить срок тес...   \n2  Платформы анализа данных: что они умеют и как ...   \n\n                                                link  \\\n0    https://habr.com/ru/company/hsespb/blog/569070/   \n1                   https://habr.com/ru/post/568876/   \n2  https://habr.com/ru/company/factory5/blog/569066/   \n\n                                                text  \n0  Привет, Хабр! Мы — Тимофей Василевский, Сергей...  \n1  Примеры FPGA-проектов на базе Nvidia Jetson, X...  \n2  Рынок ИТ- продуктов переполнен предложениями п...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>title</th>\n      <th>link</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-07-22T10:07:01.000Z</td>\n      <td>Многопользовательская сетевая игра Ticket to Ride</td>\n      <td>https://habr.com/ru/company/hsespb/blog/569070/</td>\n      <td>Привет, Хабр! Мы — Тимофей Василевский, Сергей...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021-07-22T10:00:03.000Z</td>\n      <td>Искусство отладки FPGA: как сократить срок тес...</td>\n      <td>https://habr.com/ru/post/568876/</td>\n      <td>Примеры FPGA-проектов на базе Nvidia Jetson, X...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2021-07-22T09:54:05.000Z</td>\n      <td>Платформы анализа данных: что они умеют и как ...</td>\n      <td>https://habr.com/ru/company/factory5/blog/569066/</td>\n      <td>Рынок ИТ- продуктов переполнен предложениями п...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a dataframe with the following columns date, title, link, text\n",
    "#while checking if article's header or text contain the keywords defined earlier\n",
    "habr_posts = pd.DataFrame()\n",
    "for date, title, link, text in zip(posts_date, posts_title, posts_link, posts_text):\n",
    "    if any(keyword.lower() in text.lower() for keyword in KEYWORDS) or any(keyword.lower() in title.lower() for keyword in KEYWORDS):\n",
    "        row = {'date': date, 'title': title, 'link': link, 'text': text}\n",
    "        habr_posts = pd.concat([habr_posts, pd.DataFrame([row])])\n",
    "habr_posts.reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-increase",
   "metadata": {},
   "source": [
    "## Let's check if a random email has been hacked using Avast Hack Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fluid-petite",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL = ['xxxxx@x.ru', 'yyyyy@y.com']\n",
    "#link to a hidden API which Avast uses to check email for hacks\n",
    "hidden_api_url = 'https://identityprotection.avast.com/v1/web/query/site-breaches/unauthorized-data'\n",
    "#mandatory headers\n",
    "HEADERS = {\n",
    "    'Vaar-Version': '0',\n",
    "    'Vaar-Header-App-Product-Name': 'hackcheck-web-avast',\n",
    "    'Vaar-Header-App-Build-Version': '1.0.0',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "exact-cemetery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"breaches\":{\"88\":{\"breachId\":88,\"site\":\"ir.netease.com\",\"recordsCount\":256475863,\"description\":\"In October 2015, the Chinese internet and gaming company NetEase suffered a data breach that leaked hundreds of millions of user credentials. The dump contained users\\' email addresses along with their plain-text passwords.\",\"publishDate\":\"2016-11-07T00:00:00Z\",\"statistics\":{\"usernames\":0,\"passwords\":255433278,\"emails\":256475863}},\"16587\":{\"breachId\":16587,\"site\":\"dubsmash.com\",\"recordsCount\":161162574,\"description\":\"In December 2018, Dubsmash\\'s database was allegedly breached. The stolen data contains usernames, passwords, email addresses and additional personal information. This breach is being privately shared on the internet.\",\"publishDate\":\"2019-03-07T00:00:00Z\",\"statistics\":{\"usernames\":159256428,\"passwords\":161154927,\"emails\":161146252}},\"17075\":{\"breachId\":17075,\"site\":\"toondo.com\",\"recordsCount\":6034161,\"description\":\"In July 2019, the online comic creation site ToonDoo was allegedly breached. The stolen data contains usernames, IPs, passwords, salts, email addresses and additional personal information. This breach is being publicly shared on the internet.\",\"publishDate\":\"2019-11-14T00:00:00Z\",\"statistics\":{\"usernames\":6034158,\"passwords\":6034156,\"emails\":6024388}},\"4563\":{\"breachId\":4563,\"site\":\"houdao.com\",\"recordsCount\":28478905,\"description\":\"In April 2011, gaming forum Houdao\\'s user database was allegedly breached. The stolen data contained more than 28 million user records including but not limited to email addresses and plaintext passwords. \",\"publishDate\":\"2017-06-28T00:00:00Z\",\"statistics\":{\"usernames\":0,\"passwords\":28477998,\"emails\":28478905}},\"2\":{\"breachId\":2,\"site\":\"linkedin.com\",\"recordsCount\":158591429,\"description\":\"In 2012, online professional networking platform LinkedIn fell victim to a breach of its members\\' passwords. Four years later, 117 million email and password combinations from that breach appeared for sale on a dark web marketplace. \\\\n\\\\nThe leaked passwords had only been protected by unsalted SHA-1 cryptographic hashing, which prompted LinkedIn to enforce salted hashing after the breach. Russian national Yevgeniy Nikulin was accused of the breach and was extradited from the Czech Republic to the United States as of March 2018.\",\"publishDate\":\"2016-10-21T00:00:00Z\",\"statistics\":{\"usernames\":0,\"passwords\":111975337,\"emails\":158591429}},\"17863\":{\"breachId\":17863,\"site\":\"havenly.com\",\"recordsCount\":1362159,\"description\":\"In June 2020, the online interior design service Havenly was allegedly breached. The stolen data contains passwords, email addresses and additional personal information. This breach is being privately shared on the internet.\",\"publishDate\":\"2020-08-06T00:00:00Z\",\"statistics\":{\"usernames\":0,\"passwords\":1209583,\"emails\":1361967}},\"12\":{\"breachId\":12,\"site\":\"vk.com\",\"recordsCount\":110121799,\"description\":\"Popular Russian social networking platform VKontakte was breached in late 2012. Over 100 million clear-text passwords were compromised in the breach. Breached credential sets included victims\\' e-mail addresses, passwords, dates of birth, phone numbers and location details. The credential set was advertised on a dark web marketplace as of June 2016 for a price of one bitcoin. \",\"publishDate\":\"2016-10-29T00:00:00Z\",\"statistics\":{\"usernames\":0,\"passwords\":107425364,\"emails\":104247327}},\"3\":{\"breachId\":3,\"site\":\"adobe.com\",\"recordsCount\":152046506,\"description\":\"In October of 2013, criminals penetrated Adobe\\'s corporate network and the stole source code for several of its software products. The affected products included Adobe\\'s ColdFusion web application platform as well as the Acrobat suite of products. Adobe asserts that criminals also accessed nearly three-million customer credit card records and stole login data for an undisclosed number of Adobe user accounts.\",\"publishDate\":\"2016-10-21T00:00:00Z\",\"statistics\":{\"usernames\":0,\"passwords\":129430596,\"emails\":152046506}}},\"data\":{\"havenly.com\":{\"yyyyy@y.com\":[{\"breachId\":17863,\"usernameBreached\":true,\"passwordBreached\":true}]},\"adobe.com\":{\"yyyyy@y.com\":[{\"breachId\":3,\"usernameBreached\":true,\"passwordBreached\":true}]},\"ir.netease.com\":{\"yyyyy@y.com\":[{\"breachId\":88,\"usernameBreached\":true,\"passwordBreached\":true}]},\"houdao.com\":{\"yyyyy@y.com\":[{\"breachId\":4563,\"usernameBreached\":true,\"passwordBreached\":true}]},\"linkedin.com\":{\"yyyyy@y.com\":[{\"breachId\":2,\"usernameBreached\":true,\"passwordBreached\":true}]},\"dubsmash.com\":{\"yyyyy@y.com\":[{\"breachId\":16587,\"usernameBreached\":true,\"passwordBreached\":true}]},\"vk.com\":{\"xxxxx@x.ru\":[{\"breachId\":12,\"usernameBreached\":true,\"passwordBreached\":true}]},\"toondo.com\":{\"yyyyy@y.com\":[{\"breachId\":17075,\"usernameBreached\":true,\"passwordBreached\":true}]}},\"summary\":{\"xxxxx@x.ru\":{\"breaches\":[12]},\"yyyyy@y.com\":{\"breaches\":[88,16587,17075,4563,2,17863,3]}}}'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracting information regarding hacks in json format\n",
    "email_check = requests.post(hidden_api_url, json={'emailAddresses': EMAIL}, headers=HEADERS)\n",
    "email_check.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "broken-thermal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['breaches', 'data', 'summary'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transforming json into python dictionary\n",
    "email_check_dict = json.loads(email_check.text)\n",
    "email_check_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "convertible-beaver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>date</th>\n",
       "      <th>breach_source</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yyyyy@y.com</td>\n",
       "      <td>2016-11-07</td>\n",
       "      <td>havenly.com</td>\n",
       "      <td>In October 2015, the Chinese internet and gami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yyyyy@y.com</td>\n",
       "      <td>2019-03-07</td>\n",
       "      <td>adobe.com</td>\n",
       "      <td>In December 2018, Dubsmash's database was alle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yyyyy@y.com</td>\n",
       "      <td>2019-11-14</td>\n",
       "      <td>ir.netease.com</td>\n",
       "      <td>In July 2019, the online comic creation site T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yyyyy@y.com</td>\n",
       "      <td>2017-06-28</td>\n",
       "      <td>houdao.com</td>\n",
       "      <td>In April 2011, gaming forum Houdao's user data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yyyyy@y.com</td>\n",
       "      <td>2016-10-21</td>\n",
       "      <td>linkedin.com</td>\n",
       "      <td>In 2012, online professional networking platfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yyyyy@y.com</td>\n",
       "      <td>2020-08-06</td>\n",
       "      <td>dubsmash.com</td>\n",
       "      <td>In June 2020, the online interior design servi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xxxxx@x.ru</td>\n",
       "      <td>2016-10-29</td>\n",
       "      <td>vk.com</td>\n",
       "      <td>Popular Russian social networking platform VKo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>yyyyy@y.com</td>\n",
       "      <td>2016-10-21</td>\n",
       "      <td>toondo.com</td>\n",
       "      <td>In October of 2013, criminals penetrated Adobe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         email        date   breach_source  \\\n",
       "0  yyyyy@y.com  2016-11-07     havenly.com   \n",
       "1  yyyyy@y.com  2019-03-07       adobe.com   \n",
       "2  yyyyy@y.com  2019-11-14  ir.netease.com   \n",
       "3  yyyyy@y.com  2017-06-28      houdao.com   \n",
       "4  yyyyy@y.com  2016-10-21    linkedin.com   \n",
       "5  yyyyy@y.com  2020-08-06    dubsmash.com   \n",
       "6   xxxxx@x.ru  2016-10-29          vk.com   \n",
       "7  yyyyy@y.com  2016-10-21      toondo.com   \n",
       "\n",
       "                                         description  \n",
       "0  In October 2015, the Chinese internet and gami...  \n",
       "1  In December 2018, Dubsmash's database was alle...  \n",
       "2  In July 2019, the online comic creation site T...  \n",
       "3  In April 2011, gaming forum Houdao's user data...  \n",
       "4  In 2012, online professional networking platfo...  \n",
       "5  In June 2020, the online interior design servi...  \n",
       "6  Popular Russian social networking platform VKo...  \n",
       "7  In October of 2013, criminals penetrated Adobe...  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_check_pd = pd.DataFrame()\n",
    "#creating dataframe with the following columns email, date, breach_source, and description\n",
    "for breach_id, breach_source in zip(email_check_dict['breaches'], email_check_dict['data']):\n",
    "    row = {\n",
    "        'email': list(email_check_dict['data'][breach_source].keys())[0],\n",
    "        'date': pd.to_datetime(email_check_dict['breaches'][breach_id]['publishDate'], format='%Y-%m-%d').strftime('%Y-%m-%d'),\n",
    "        'breach_source': breach_source,\n",
    "        'description': email_check_dict['breaches'][breach_id]['description']\n",
    "    }\n",
    "    email_check_pd = pd.concat([email_check_pd, pd.DataFrame([row])])\n",
    "email_check_pd.reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-southwest",
   "metadata": {},
   "source": [
    "## Let's write a script that will get 50 last posts from a given group in Vkontakte.ru, a popular social network within Russian speaking countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fiscal-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP = 'netology'\n",
    "#you would need a vk access token in order to work with website's API\n",
    "with open('vk_access_token.txt', 'r') as token:\n",
    "    TOKEN = token.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "static-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a method to get group's posts\n",
    "GROUP_REQUEST = 'https://api.vk.com/method/wall.get'\n",
    "VERSION = '5.58'\n",
    "SLEEP = 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "editorial-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining mandatory parameters\n",
    "params = {\n",
    "    'owner_id': -30159897,\n",
    "    'domain': f'https://vk.com/{GROUP}',\n",
    "    'access_token': TOKEN,\n",
    "    'v': VERSION,\n",
    "    'count': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "conservative-atlanta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting posts themselves\n",
    "vk_posts = requests.get(GROUP_REQUEST, params)\n",
    "vk_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "reliable-torture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-17 13:25:03</td>\n",
       "      <td>Весна — лучшее время для перемен. Чтобы узнать...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-28 09:23:00</td>\n",
       "      <td>Всё, что окружает нас в повседневной жизни так...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-27 12:57:00</td>\n",
       "      <td>В этой афише мероприятий онлайн-эфир о развити...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-27 09:08:00</td>\n",
       "      <td>Разместить ссылку на свой сайт — это тоже свое...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-26 13:57:00</td>\n",
       "      <td>Каждый день дизайнеры меняют IT и digital, фор...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-03-26 07:44:00</td>\n",
       "      <td>Точно решили, кем хотите стать в диджитале, но...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-03-25 15:17:00</td>\n",
       "      <td>Мудборд — неотъемлемая часть подготовки любого...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-03-25 08:10:00</td>\n",
       "      <td>Почему Java лучший выбор для новичка, какие на...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-03-24 13:57:00</td>\n",
       "      <td>Что для вас важнее всего в работе? \\nКому-то д...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-03-24 07:27:00</td>\n",
       "      <td>🚀 Запустили бесплатный курс «Основы Figma» \\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-03-23 14:16:00</td>\n",
       "      <td>Если вы знаете английский язык — у вас могут б...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-03-23 08:16:00</td>\n",
       "      <td>В новом выпуске НетоNews обсудим изменение поз...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021-03-22 14:46:00</td>\n",
       "      <td>Что общего между лентой новостей в Инстаграм и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2021-03-22 07:36:00</td>\n",
       "      <td>🚀 30 марта стартует бесплатный онлайн-курс «Пс...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021-03-21 15:41:00</td>\n",
       "      <td>🎁 Запустили страницу с подарками и скидками от...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021-03-21 08:10:00</td>\n",
       "      <td>Решиться на перемены — всегда непросто. Решить...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2021-03-20 13:22:00</td>\n",
       "      <td>В этой афише мероприятий конференции для дизай...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2021-03-20 12:22:00</td>\n",
       "      <td>*партнёрский пост* \\n \\n29 марта на сайте m-i-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021-03-20 10:46:00</td>\n",
       "      <td>Задача геймдизайнера не только нарисовать игру...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-03-19 13:36:00</td>\n",
       "      <td>Произведения искусства, которыми мы пользуемся...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2021-03-19 08:02:00</td>\n",
       "      <td>Я новичок, у меня совершенно нет опыта! 😰 \\n \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021-03-18 15:11:00</td>\n",
       "      <td>Мы сталкиваемся с аналитикой гораздо чаще, чем...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2021-03-18 08:00:09</td>\n",
       "      <td>Вместе в природой просыпается накопившаяся за ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2021-03-17 07:46:00</td>\n",
       "      <td>🚀 29 марта стартует бесплатный онлайн-марафон ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2021-03-16 14:46:00</td>\n",
       "      <td>Рано или поздно у любого копирайтера, SMM-мене...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2021-03-16 07:48:00</td>\n",
       "      <td>В новом выпуске НетоNews поговорим о частичной...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2021-03-15 15:09:00</td>\n",
       "      <td>Как на удалёнке выстроить хорошие к коллегами ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2021-03-15 07:55:28</td>\n",
       "      <td>🔥 27 марта пройдёт бесплатный митап «Аналитика...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2021-03-14 09:49:00</td>\n",
       "      <td>Говорят, лидерами не рождаются, а становятся 😎...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2021-03-13 09:53:00</td>\n",
       "      <td>Выстраивать отношения с коллегами, решать зада...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2021-03-12 14:47:00</td>\n",
       "      <td>В эфире #пятничнаябеседка. Здесь мы обсуждаем,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2021-03-12 08:11:00</td>\n",
       "      <td>Настраиваем рекламные кабинеты, пишем сценарий...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2021-03-11 14:53:00</td>\n",
       "      <td>Даже самый уверенный в себе человек испытывает...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2021-03-11 08:40:00</td>\n",
       "      <td>Интернет — пространство безграничных возможнос...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2021-03-10 14:40:00</td>\n",
       "      <td>Проверим, насколько хорошо вы знаете нас 😉 \\nЗ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2021-03-10 07:53:00</td>\n",
       "      <td>🔥 15 марта стартует бесплатный курс «Как стать...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2021-03-09 14:53:00</td>\n",
       "      <td>Поиск изображений для своего блога, сайта или ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2021-03-08 14:13:00</td>\n",
       "      <td>В марте на рынке вакансий настоящий бум — вспл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2021-03-07 10:49:00</td>\n",
       "      <td>Сегодня много говорят о софт-скиллс — гибких н...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2021-03-06 13:47:00</td>\n",
       "      <td>Интересные онлайн-мероприятия в марте: вебинар...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2021-03-06 10:14:00</td>\n",
       "      <td>Чтобы запустить свой интернет-магазин, нужно б...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2021-03-05 13:47:00</td>\n",
       "      <td>Мечты у всех разные, но есть то, что нас объед...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2021-03-05 07:31:00</td>\n",
       "      <td>Говорят, IT-специалисты не ищут работодателей,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2021-03-04 14:57:00</td>\n",
       "      <td>Хороший дизайн — это не только красивая картин...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2021-03-04 08:13:00</td>\n",
       "      <td>Раньше мы не знали, с чего начинать сочинение,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2021-03-03 14:31:00</td>\n",
       "      <td>Сегодня вам пригодится математика и немного ло...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2021-03-03 07:11:00</td>\n",
       "      <td>🔥 10 и 11 марта пройдёт бесплатный онлайн-инте...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2021-03-02 15:04:00</td>\n",
       "      <td>Чтобы создать что-то особенное — нужно перебра...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2021-03-02 08:16:00</td>\n",
       "      <td>«Да это же фотошоп!» — как часто у вас возника...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2021-03-01 15:16:00</td>\n",
       "      <td>Запустить собственный интернет-магазин или нач...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date                                               text\n",
       "0  2021-03-17 13:25:03  Весна — лучшее время для перемен. Чтобы узнать...\n",
       "1  2021-03-28 09:23:00  Всё, что окружает нас в повседневной жизни так...\n",
       "2  2021-03-27 12:57:00  В этой афише мероприятий онлайн-эфир о развити...\n",
       "3  2021-03-27 09:08:00  Разместить ссылку на свой сайт — это тоже свое...\n",
       "4  2021-03-26 13:57:00  Каждый день дизайнеры меняют IT и digital, фор...\n",
       "5  2021-03-26 07:44:00  Точно решили, кем хотите стать в диджитале, но...\n",
       "6  2021-03-25 15:17:00  Мудборд — неотъемлемая часть подготовки любого...\n",
       "7  2021-03-25 08:10:00  Почему Java лучший выбор для новичка, какие на...\n",
       "8  2021-03-24 13:57:00  Что для вас важнее всего в работе? \\nКому-то д...\n",
       "9  2021-03-24 07:27:00  🚀 Запустили бесплатный курс «Основы Figma» \\n ...\n",
       "10 2021-03-23 14:16:00  Если вы знаете английский язык — у вас могут б...\n",
       "11 2021-03-23 08:16:00  В новом выпуске НетоNews обсудим изменение поз...\n",
       "12 2021-03-22 14:46:00  Что общего между лентой новостей в Инстаграм и...\n",
       "13 2021-03-22 07:36:00  🚀 30 марта стартует бесплатный онлайн-курс «Пс...\n",
       "14 2021-03-21 15:41:00  🎁 Запустили страницу с подарками и скидками от...\n",
       "15 2021-03-21 08:10:00  Решиться на перемены — всегда непросто. Решить...\n",
       "16 2021-03-20 13:22:00  В этой афише мероприятий конференции для дизай...\n",
       "17 2021-03-20 12:22:00  *партнёрский пост* \\n \\n29 марта на сайте m-i-...\n",
       "18 2021-03-20 10:46:00  Задача геймдизайнера не только нарисовать игру...\n",
       "19 2021-03-19 13:36:00  Произведения искусства, которыми мы пользуемся...\n",
       "20 2021-03-19 08:02:00  Я новичок, у меня совершенно нет опыта! 😰 \\n \\...\n",
       "21 2021-03-18 15:11:00  Мы сталкиваемся с аналитикой гораздо чаще, чем...\n",
       "22 2021-03-18 08:00:09  Вместе в природой просыпается накопившаяся за ...\n",
       "23 2021-03-17 07:46:00  🚀 29 марта стартует бесплатный онлайн-марафон ...\n",
       "24 2021-03-16 14:46:00  Рано или поздно у любого копирайтера, SMM-мене...\n",
       "25 2021-03-16 07:48:00  В новом выпуске НетоNews поговорим о частичной...\n",
       "26 2021-03-15 15:09:00  Как на удалёнке выстроить хорошие к коллегами ...\n",
       "27 2021-03-15 07:55:28  🔥 27 марта пройдёт бесплатный митап «Аналитика...\n",
       "28 2021-03-14 09:49:00  Говорят, лидерами не рождаются, а становятся 😎...\n",
       "29 2021-03-13 09:53:00  Выстраивать отношения с коллегами, решать зада...\n",
       "30 2021-03-12 14:47:00  В эфире #пятничнаябеседка. Здесь мы обсуждаем,...\n",
       "31 2021-03-12 08:11:00  Настраиваем рекламные кабинеты, пишем сценарий...\n",
       "32 2021-03-11 14:53:00  Даже самый уверенный в себе человек испытывает...\n",
       "33 2021-03-11 08:40:00  Интернет — пространство безграничных возможнос...\n",
       "34 2021-03-10 14:40:00  Проверим, насколько хорошо вы знаете нас 😉 \\nЗ...\n",
       "35 2021-03-10 07:53:00  🔥 15 марта стартует бесплатный курс «Как стать...\n",
       "36 2021-03-09 14:53:00  Поиск изображений для своего блога, сайта или ...\n",
       "37 2021-03-08 14:13:00  В марте на рынке вакансий настоящий бум — вспл...\n",
       "38 2021-03-07 10:49:00  Сегодня много говорят о софт-скиллс — гибких н...\n",
       "39 2021-03-06 13:47:00  Интересные онлайн-мероприятия в марте: вебинар...\n",
       "40 2021-03-06 10:14:00  Чтобы запустить свой интернет-магазин, нужно б...\n",
       "41 2021-03-05 13:47:00  Мечты у всех разные, но есть то, что нас объед...\n",
       "42 2021-03-05 07:31:00  Говорят, IT-специалисты не ищут работодателей,...\n",
       "43 2021-03-04 14:57:00  Хороший дизайн — это не только красивая картин...\n",
       "44 2021-03-04 08:13:00  Раньше мы не знали, с чего начинать сочинение,...\n",
       "45 2021-03-03 14:31:00  Сегодня вам пригодится математика и немного ло...\n",
       "46 2021-03-03 07:11:00  🔥 10 и 11 марта пройдёт бесплатный онлайн-инте...\n",
       "47 2021-03-02 15:04:00  Чтобы создать что-то особенное — нужно перебра...\n",
       "48 2021-03-02 08:16:00  «Да это же фотошоп!» — как часто у вас возника...\n",
       "49 2021-03-01 15:16:00  Запустить собственный интернет-магазин или нач..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vk_netology_posts = pd.DataFrame()\n",
    "#creating a dataframe with the following columns date and text\n",
    "for post in vk_posts.json()['response']['items']:\n",
    "    row = {\n",
    "        'date': pd.to_datetime(post['date'], unit='s'),\n",
    "        'text': post['text']\n",
    "    }\n",
    "    vk_netology_posts = pd.concat([vk_netology_posts, pd.DataFrame([row])])\n",
    "vk_netology_posts.reset_index().drop(columns='index')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}